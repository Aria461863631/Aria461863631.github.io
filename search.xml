<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[ITOC-ch3] THE CHURCH—TURING THESIS]]></title>
    <url>%2F2018%2F05%2F19%2FITOC%2F3%2F</url>
    <content type="text"><![CDATA[[ITOC-ch3] THE CHURCH—TURING THESIS 3.1 TURING MACHINES unrestricted access to unlimited memory The Turing machine model uses an infinite tape as its unlimited memory. It has a tape head that can read and write symbols and move both left and right on the tape. The outputs accept and reject are obtained by entering designated accepting and rejecting states. If it doesn’t enter an accepting or a rejecting state, it will go on forever, never halting. DEF configuration a setting of current state, the current tape contents, and the current head location. Machine M Recognizer input a w, accept, reject or loop may fail to accept an input by entering the $ q_{reject} $ state and rejecting, or by looping EX: if $ w \in L $, than M must accept w. if $ w \notin L $, than M may loop when input w. Decider halts on all inputs EX: if $ w \in L $, than M must accept w. if $ w \notin L $, than M must reject w. Language L The collection of strings that M accepts Turing-recognizable language there exist some Turing machine recognizes it. Turing-decidable language there exist some decider decides it. ?Undecidable language exist recognizer accept all strings $ w \in L $ No decider exist, always loop when input specific string $ w \notin L $ example: 4.2 ?H can’t halt when its input D simulate H itself 3.2 VARIANTS OF TURING MACHINES MULTITAPE TURING MACHINES $ δ : Q × Γ^k → Q × Γ^k × { \{L, R, S\} }^k $ equivalent with ordinary Turing machine uses # to separate different tapes uses dot to keep track the heads NONDETERMINISTIC TURING MACHINES $ δ : Q × Γ→P(Q × Γ × \{L, R\}) $ equivalent with ordinary Turing machine have D try all possible branches of N ’s nondeterministic computation tape 1: input; tape2: copy of N ’s tape on some branch; tape 3: node address of N where D is now in ENUMERATORS a Turing machine with an attached printer The language enumerated by E is the collection of all the strings that it eventually prints out. equivalent with ordinary Turing machine construct M recognize language A enumerated by E construct E enumerate language A recognized by M 3.3 THE DEFINITION OF ALGORITHM Informally speaking, an algorithm is a collection of instructions to perform some task. HILBERT’S PROBLEMS Algorithm DEF λ-calculus notation system by Alonzo Church Turing machines by Alan Turing Church–Turing thesis Example: Hilbert’s tenth problem Algorithm: Test whether a polynomial has an integral root TM: constructing such algorithm equals constructing a TM to decide the language D $ D = \{p \ | \ p \ is \ a \ polynomial \ with \ an \ integral \ root\}. $ decider D can’t exist, problem algorithmically unsolvable. TERMINOLOGY FOR DESCRIBING TURING MACHINES 3 levels describing Algorithm or TM formal description spells out in full the Turing machine’s states, transition function, and so on implementation description describe the way TM moves its head and the way it stores data on its tape high-level description input encoded object as strings break the algorithm into stages, each involves individual steps of the Turing machine’s computation]]></content>
      <categories>
        <category>ITOC</category>
      </categories>
      <tags>
        <tag>ITOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[ITOC-ch2] CONTEXT-FREE LANGUAGES]]></title>
    <url>%2F2018%2F05%2F19%2FITOC%2F2%2F</url>
    <content type="text"><![CDATA[[ITOC-ch2] CONTEXT-FREE LANGUAGES In this chapter we present context-free grammars to describe certain features that have a recursive structure. help us organize and understand relationships of terms. Application: compilers and interpreters Most compilers and interpreters contain a parser that extracts the meaning of a program before generating the compiled code or performing the interpreted execution. 2.1 CONTEXT-FREE GRAMMAR Context Free Grammar consists of a collection of substitution rules Variable symbols on the left hand side Terminal other symbols can’t appear in the left hand side in CFG ( Ex: aB --&gt; XX ) Derivation The sequence of substitutions to obtain a string u derives v, written $ u ⇒^{ * } v $, if u = v or if a sequence $u 1 , u 2 , . . . , u k $ exists for k ≥ 0 and $ u ⇒ u 1 ⇒ u 2 ⇒ . . . ⇒ u k ⇒ v. $ Context Free Language strings generated by CFG $ \{ w ∈ Σ^{ ∗ } | S ⇒^{ * } w \}. $ ?Parsing the compiler extracts the ?meaning of the code Parser Tree Representation of the meaning of the code leftmost derivation Derivation that at every step the leftmost remaining variable is the one replaced AMBIGUITY DEF A string w is derived ambiguously in context-free grammar G if it has two or more different leftmost derivations. Grammar G is ambiguous if it generates some string ambiguously. CHOMSKY NORMAL FORM Simplified form for CFG, can generate all CFL. 2.2 PUSHDOWN AUTOMATA nondeterministic PDA (with extra stack ) DEF (transition functions return sets) Computation under PDA A pushdown automaton $ M = (Q, Σ, Γ, δ, q_0 , F ) $ while input $ \in $ $ Σ_ε $, states $ \in Q $, stack strings $ \in Γ^{ ∗ } $ Example EQUIVALENCE WITH CONTEXT-FREE GRAMMARS 2.21 If a language is context free, then some pushdown automaton recognizes it. 2.27 If a pushdown automaton recognizes some language, then it is context free. 2.21 Construct PDA P to accept all strings generated by CFG G Description of P stack push: marker $ than Start variable while ( stack.top != ‘$’ ) (stack.top == Variable A) nondeterministically select one of the rules A --&gt; w and substitute A by w (stack.top == Terminal a) read next symbol and compare it to a pop ‘$’, Accept 2.27 construct CFG G that generates all the strings PDA P accepts (go from its Start state to an Accept state) Description of G Variables of G are $ \{A_{pq} \ | \ p, q ∈ Q \ \} $. The start variable is $ A_{q0} ,q_{accept} $. Rules For each $ p ∈ Q $, put the rule $ A_{pp} → ε $ in G. For each $ p, q, r, s ∈ Q $, $ u ∈ Γ $ , and $ a, b ∈ Σ_ε $ , if $ δ(p, a, ε) $ contains $ (r, u) $ and $ δ(s, b, u) $ contains $ (q, ε)$, put the rule $ A_{pq} → aA_{rs} b $ in G. For each $ p, q, r ∈ Q $, put the rule $ A_{pq} → A_{pr} A_{rq} $ in G. Idea variable $ A_{pq} $ generates all the strings that can take P from p with an empty stack to q with an empty stack. 递归生成串？最后从最开始的变量 $ A_{q0} ,q_{accept} $ 出发生成的串就是PDA所有ac的串？ Proof prove that this construction works by demonstrating that $ A_{pq} $ generates x if and only if (iff) x can bring P from p with empty stack to q with empty stack. Two direction. Induction 2.3 NON-CONTEXT-FREE LANGUAGES THE PUMPING LEMMA FOR CONTEXT-FREE LANGUAGES DEF If A is a context-free language, then there is a number p (the pumping length) where, if s is any string in A of length at least p, then s may be divided into five pieces $ s = uvxyz $ satisfying the conditions Proof Let A be a CFL and let G be a CFG that generates it. We assign the pumping length p to be $ b^{|V| + 1 } $ b is maximum number of symbols in the right-hand side of a rule ( maximum number of children per node ) |V| is the number of variables in G $ b^{|V| + 1 } $ is the length of the string when its parse tree is|V| + 1 high τ is a parse tree that has the smallest number of nodes. We show that any strings in A of length at least p may be broken into the five pieces uvxyz, satisfying our three conditions. When τ is at least |V| + 1 high, due to pigeon hole principle, some variable R must appears more than once on that path. And we can divide s as follows Contradiction: if both y and v are ε than τ won’t be the smallest tree generate s The subtree where R generates vxy is at most |V| + 1 high, therefore has length at most $ b^{|V| + 1 } = p $ Example $ C = {a_i b_j c_k | 0 ≤ i ≤ j ≤ k \ } $ 2.4 DETERMINISTIC CONTEXT-FREE LANGUAGES]]></content>
      <categories>
        <category>ITOC</category>
      </categories>
      <tags>
        <tag>ITOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[ITOC-ch1] REGULAR LANGUAGES]]></title>
    <url>%2F2018%2F05%2F10%2FITOC%2Fch1%2F</url>
    <content type="text"><![CDATA[[ITOC-ch1] REGULAR LANGUAGES 1.1 FINITE AUTOMATA Deterministic Finite Automata DEF Every state of a DFA always has exactly one exiting transition arrow for each symbol in the alphabet. Computation under DFA DEF Let M = (Q, Σ, δ, q0 , F ) be a finite automaton and let w = $ w_1 w_2 · · · w_n $ be a string where each $ w_i $ is a member of the alphabet Σ. Then M accepts w if a sequence of states $ r_0 , r_1 , . . . , r_n $ in Q exists with three conditions doing computation == state transition 1.2 NONDETERMINISM Nondeterministic Finite Automata DEF Difference a state may have ε, zero, one, or many exiting arrows for each alphabet symbol. Computation under NFA ( $ \in $ ) EQUIVALENCE OF NFAS AND DFAS Every nondeterministic finite automaton has an equivalent deterministic finite automaton. Proof convert the NFA into an equivalent DFA that simulates the NFA We say that M recognizes language A if $ A = \{w| M \ accepts \ w \} $ Regular language DEF DFA == NFA == RL Regular Operations CLOSURE UNDER THE REGULAR OPERATIONS Union Concatenation Star 1.3 REGULAR EXPRESSIONS DEF $ ε $ : empty string; $ ∅ $ : empty set ( even with out empty string ) $ ∅ ^ * = {ε} $ $ 1 ^ ∅ = ∅ $ $ 1 ^* ε = 1 ^* * $ EQUIVALENCE WITH FINITE AUTOMATA FA == RL == RE Proof [1] If a language is described by a regular expression, then it is regular. (Proof by recursive definition from base case 1,2,3 ) [2] If a language is regular, then it is described by a regular expression. Using GNFA the transition arrows may have any regular expressions as labels 1.4 NONREGULAR LANGUAGES ( cannot compute infinite possibilities with finite states ) THE PUMPING LEMMA FOR REGULAR LANGUAGES DEF If A is a regular language, then there is a number p (the pumping length) where if s is any string in A of length at least p, then s may be divided into three pieces, $ s = xyz $, satisfying the following conditions: Proof Let M = DFA that recognizes A. We assign the pumping length p to be the number of states of M. We show that any strings in A of length at least p may be broken into the three pieces xyz, satisfying our three conditions. Due to pigeon hole principle, among the first p + 1 elements in the sequence, two must be the same state Therefore we can divide the string as the following figure while q9 is the state appears twice in the first p+1 length Proof language B is not regular Contradiction Assume p exist ( B is regular ) find a string s in B that has length p or greater considering all ways of dividing s into x, y, and z and for each such division, finding a value i where $ xy^i z \notin B $ Example B $= \{ 0^n 1^n | n ≥ 0 \} $ Assume p Choose s to be the string $ 0^p 1^p $. divide s ( 3 cases ) and show $ xy^i z \notin B $]]></content>
      <categories>
        <category>ITOC</category>
      </categories>
      <tags>
        <tag>ITOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch4] Metalinguistic Abstraction]]></title>
    <url>%2F2018%2F04%2F27%2FSICP%2Fsicp4%2F</url>
    <content type="text"><![CDATA[[SICP-ch4] Metalinguistic Abstraction establishing new languages and implement these languages by constructing evaluators. (极简版) evaluators ( interpreter ) a procedure that, when applied to an expression of the language, performs the actions required to evaluate that expression. 4.1 The Metacircular Evaluator metacircular An evaluator that is written in the same language that it evaluates Recall environment model To evaluate a compound procedure, evaluate the subexpressions and then apply the value To apply a compound procedure, evaluate the body of the procedure in a new environment. Data Abstraction make the evaluator independent of the representation of the language. 4.1.1 The Core of the Evaluator eval and apply (define (eval exp env) ;primitive (cond ((self-evaluating? exp) exp) ((variable? exp) (lookup-variable-value exp env)) ;special form ((quoted? exp) (text-of-quotation exp)) ((assignment? exp) (eval-assignment exp env)) ((definition? exp) (eval-definition exp env)) ((if? exp) (eval-if exp env)) ((lambda? exp) (make-procedure (lambda-parameters exp) (lambda-body exp) env)) ((begin? exp) (eval-sequence (begin-actions exp) env)) ((cond? exp) (eval (cond-&gt;if exp) env)) ;combinations ((application? exp) ;apply (apply (eval (operator exp) env) (list-of-values (operands exp) env))) (else (error "Unknown expression type: EVAL" exp)))) (define (apply procedure arguments) ;primitive (cond ((primitive-procedure? procedure) (apply-primitive-procedure procedure arguments)) ;compound ((compound-procedure? procedure) ;eval (eval-sequence (procedure-body procedure) (extend-environment (procedure-parameters procedure) arguments (procedure-environment procedure)))) (else (error "Unknown procedure type: APPLY" procedure)))) Conditionals eval-if Sequences eval-sequence Assignments eval-assignment Definitions eval-definition 4.1.2 Representing Expressions building constructor, selector and predeicate of our language The only self-evaluating items are numbers and strings Variables are represented by symbols Quotations have the form (quote ⟨text-of-quotation⟩) Assignments have the form (set! ⟨var⟩ ⟨value⟩) Definitions have the form (define ⟨var⟩ ⟨value⟩) lambda expressions are lists that begin with the symbol lambda Conditionals begin with if and have a predicate, a consequent, and an (optional) alternative. begin packages a sequence of expressions into a single expression. An application is any compound expression with operator in car and operands in cdr Derived expressions cond 4.1.3 Evaluator Data Structures representation of true and false procedures environments （a sequence of frams and each frame is a table of bindings） 4.1.4 Running the Evaluator as a Program model the application of primitive procedures. set up a global environment provide a driver loop that models the read-eval-print loop (define (driver-loop) (prompt-for-input input-prompt) ;input (let ((input (read))) ;evaluate output (let ((output (eval input the-global-environment))) (announce-output output-prompt) (user-print output))) (driver-loop)) 4.1.5 Data as Programs One operational view of the meaning of a program is that a program is a description of an abstract (perhaps infinitely large) machine. Evaluator as a bridge between the data objects and the language From the perspective of the user, (* x x) is an expression in the programming language From the perspective of the evaluator, (* x x) is simply a list to be manipulated. 4.1.6 Internal Definitions evaluator execute definitions in sequence, resulting different scope for internal definition Ways providing simultaneous scope scan all definitions and create, and then set to their values by assignment. 4.1.7 Separating Syntactic Analysis from Execution separate eval into analysis and execution (define (eval exp env) ((analyze exp) env)) syntactic of analysis procedure returns an execution procedure that ignores its environment argument (define (analyze-self-evaluating exp) (lambda (env) exp)) analyze-quoted, analyze-variable, analyze-assignment, analyze-definition, analyze-if analyze-lambda, analyze-sequence, analyze-application 4.2 Variations on a Scheme — Lazy Evaluation 4.2.1 Normal Order and Applicative Order applicative-order evaluate arguments than apply strict procedure normal-order enter body procedure than evaluate arguments non-strict procedure 4.2.2 An Interpreter with Lazy Evaluation implement a normal-order language thunk delayed arguments contain the information for applying when needed forcing : evaluating expression in thunk Modifying the evaluator primitive: strict compound: non-strict change eval into actual-value (define (actual-value exp env) (force-it (eval exp env))) Representing thunks 4.2.3 Streams as Lazy Lists With lazy evaluation, streams and lists can be identical, so there is no need for special forms or for separate list and stream operations. Represent car, cdr and cons as non-strict procedure List lazier than previous stream The car of the list, as well as the cdr , is delayed. won’t need explicit delay anymore ( ch-3 ) 4.3 Variations on a Scheme — Nondeterministic Computing nondeterministic computing support automatic search expressions have more than one possible value Our approach generate the sequence of all possible pairs and filter 4.3.1 Amb and Search amb (list (amb 1 2 3) (amb 'a 'b)) can have six possible values: (1 a) (1 b) (2 a) (2 b) (3 a) (3 b) require (define (require p) (if (not p) (amb))) evaluate amb causes time to split into branches machine with sufficient number of processors: parallel executions of choices machine with only one processor: dfs 4.3.2 Examples of Nondeterministic Programs Logical puzzles Parsing natural language 4.3.3 Implementing the amb Evaluator Nondeterministic evaluation result in the discovery of a dead end, must backtrack to a previous choice point Base on 4.1.7: analyzing evalutor ( analyze + execute ) Difference entirely in the execution procedures Execution procedures and continuations the execution procedures in the amb evaluator take three arguments Original: environment Additional: success continuation &amp; failure continuation success continuation Arguments: value &amp; failure continuation recieve value and proceed with the computation call faliure continuation when value leads to a dead end failure continuation try another branch of the nondeterministic process In summary, failure continuations are constructed by amb expressions—to provide a mechanism to make alternative choices if the current choice leads to a dead end; the top-level driver—to provide a mechanism to report failure when the choices are exhausted; assignments—to intercept failures and undo assignments during backtracking. Failures are initiated only when encountered a dead end. This occurs if user program executes (amb) ( no choice any more ) user types try-again at the top-level driver Failure continuations are also called during processing of a failure in order to propagate failure back to the choice point or to the top level: When failure continuation created by an assignment finish undoing, it calls the failure continuation it intercepted, When failure continuation for an amb runs out of choices, it calls the failure continuation that was originally given to the amb Structure of the evaluator amb evaluator (define (ambeval exp env succeed fail) ((analyze exp) env succeed fail)) general form of an execution procedure (lambda (env succeed fail) ;; succeed is (lambda (value fail) . . . ) ;; fail is (lambda () . . . ) . . . ) example: amb neo analyze ( return execution procedure ) (define (analyze-amb exp) (let ((cprocs (map analyze (amb-choices exp)))) (lambda (env succeed fail) (define (try-next choices) (if (null? choices) (fail) ((car choices) env ;success continuation succeed ;failure continuation (lambda () (try-next (cdr choices)))))) (try-next cprocs)))) Driver loop either calls try-again in response to the user typing try-again at the driver loop or else starts a new evaluation by calling ambeval. 4.4 Logic Programming computer science deals with imperative (how to) knowledge, whereas mathematics deals with declarative (what is) knowledge. Bias programming is about constructing algorithms for computing unidirectional functions (computations with well-defined inputs and outputs) Example of logical programming (define (append x y) (if (null? x) y (cons (car x) (append (cdr x) y)))) append procedure can be regard as two rules translate into lisp y = y append '() ( cons u v ) append y = ( cons u z ) IF v appedn y = z In a logic programming language, the programmer writes an append “procedure” by stating the two rules ( What is append ) about append given above. “How to” knowledge is provided automatically by the interpreter query language our logic programming language primitive: pattern ?x compound: and, or, not means of abstraction: rules 4.4.1 Deductive Information Retrieval Simple queries ;;; Query input: (job ?x (computer programmer)) ;;; Query results: (job (Hacker Alyssa P) (computer programmer)) (job (Fect Cy D) (computer programmer)) Compound queries (and (job ?person (computer programmer)) (address ?person ?where)) Rules ;⟨ conclusion ⟩ is a pattern and ⟨ body ⟩ is any query. ; ?first satisfy conclusion than apply body (rule ⟨ conclusion ⟩ ⟨ body ⟩ ) Logic as programs ( implement our append in logical language ) (rule (append-to-form () ?y ?y)) (rule (append-to-form (?u . ?v) ?y (?u . ?z)) (append-to-form ?v ?y ?z)) 4.4.2 How the Query System Works Implement query evaluator must perform some kind of search in order to match queries against facts and rules in the data base. Approaches nondeterministic program Search with the aid of streams Pattern matching evaluate primitive and compound Pattern Matcher Input a pattern a datum a frame that specifies bindings for various pattern variables. Pattern Matcher Output returns the given frame augmented by any bindings that may have been determined by the match. Compound queries Unification evaluate rules a generalization of pattern matching which both the “pattern” and the “datum” may contain variables. Applying rules similar to applying a procedure in the eval / apply 4.4.3 Is Logic Programming Mathematical Logic? No Query language provides a control structure that interprets the logical statements procedurally. Differences Take advantage in efficiency ;while less supervisor than programmers (and (job ?x (computer programmer)) (supervisor ?x ?y)) (and (supervisor ?x ?y) (job ?x (computer programmer))) Infinite loops whether the system will find the simple answer (married Minnie Mickey) before it goes into the loop depends on implementation details concerning the order in which the system checks the items in the data base. ;assert rules (assert! (rule (married ?x ?y) (married ?y ?x))) ;query (married Mickey ?who) Problems with not not ;input empty frame where x unbound, \not filters out frames, return empty stream (and (not (job ?x (computer programmer))) (supervisor ?x ?y)) not of logic programming languages reflects the so-called closed world assumption that all relevant information has been included in the data base.（见脚注) 4.4.4 Implementing the Query System 4.4.4.1 The Driver Loop and Instantiation 4.4.4.2 The Evaluator 4.4.4.3 Finding Assertions by Pattern Matching 4.4.4.4 Rules and Unification 4.4.4.5 Maintaining the Data Base 4.4.4.6 Stream Operations 4.4.4.7 Query Syntax Procedures 4.4.4.8 Frames and Bindings もう…続けない…なわけないだろ！]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch3] Modularity, Objects, and State]]></title>
    <url>%2F2018%2F04%2F18%2FSICP%2Fsicp3%2F</url>
    <content type="text"><![CDATA[ch-3 Modularity, Objects, and State 3.1 Assignment and Local State System decomposed into computational objects with time-varying state language must provide an assignment operator to enable us to change the value 3.1.1 Local State Variables model the situation of withdrawing money from a bank account (define (make-withdraw balance) (lambda (amount) (if (&gt;= balance amount) (begin (set! balance (- balance amount)) balance) "Insufficient funds"))) 3.1.2 The Benefits of Introducing Assignment viewing systems as collections of objects with local state is a powerful technique for maintaining a modular design (define rand (let ((x random-init)) (lambda () (set! x (rand-update x)) x))) (define (cesaro-test) (= (gcd (rand) (rand)) 1)) assignment encapsulates the state of the random-number generator within the rand procedure, so that the details of random-number generation remain independent of the rest of the program therefore easy to isolate the Monte Carlo idea 3.1.3 The Costs of Introducing Assignment the substitution model failed functional programming Programming without assignments procedure with the same arguments will produce the same result can be viewed as computing mathematical functions imperative programming using explicit assignment to update the values Pitfalls the order of the assignments several processes execute concurrently Sameness and change referentially transparent A language that supports the concept that “equals can be substituted for equals” violated when we include set! cannot determine sameness without observing the effects of change 3.2 The Environment Model of Evaluation A variable can no longer be considered to be a name for a value, but designate a “place” in which values can be stored Environment a sequence of frames each frame is a table of bindings, associate name and value 3.2.1 The Rules for Evaluation Define procedure a procedure is always a pair – consisting of some code and a pointer to an environment define creates definitions by adding bindings to frames Apply procedure create a new environment – containing a frame that binds the parameters to the arguments’ value 3.2.2 Applying Simple Procedures each call to square creates a new environment containing a binding for x 3.2.3 Frames as the Repository of Local State how procedures and assignment can be used to represent objects with local state 3.2.4 Internal Definitions make local procedure definitions a useful technique for modularizing programs local names do not interfere external names local procedures can access the arguments of the enclosing procedure 3.3 Modeling with Mutable Data Mutators Data abstraction modify compound data objects 3.3.1 Mutable List Structure set-car! and set-cdr! modify car &amp; cdr pointer Sharing and identity use the predicate eq? to detect sharing in list structures 3.3.2 Representing Queues ( insert-queue! &lt;queue&gt; &lt;item&gt; ) ( delete-queue! &lt;queue&gt; &lt;item ) ​ 3.3.3 Representing Tables assoc returns the record that has the given key as its car lookup then checks to see that the resulting record returned by assoc is not false, and returns the value ( the cdr ) of the record insertfirst see if there is already a record, if not, consing key and value than insert in table 3.3.4 A Simulator for Digital Circuits event-driven simulation actions (“events”) trigger further events that happen at a later time, which in turn trigger more events wires carry digital signals function boxes connect wires carrying input signals to other output wires (define (inverter input output) (define (invert-input) (let ((new-value (logical-not (get-signal input)))) (after-delay inverter-delay (lambda () (set-signal! output new-value))))) (add-action! input invert-input) 'ok) (define (after-delay delay action) (add-to-agenda! (+ delay (current-time the-agenda)) action the-agenda)) agenda contains a schedule of things to do propagateexecuting each procedure on the agenda in sequence 3.3.5 Propagation of Constraints 3.4 Concurrency: Time Is of the Essence by introducing assignment, we are forced to admit time into our computational models Objects in the world are acting concurrently —all at once. 3.4.1 The Nature of Time in Concurrent Systems 3.4.2 Mechanisms for Controlling Concurrency using serializer (define (make-account balance) (let ((protected (make-serializer))) (define (dispatch m) (cond ((eq? m 'withdraw) (protected withdraw)) ((eq? m 'deposit) (protected deposit)) ((eq? m 'balance) balance) (else (error "Unknown request: MAKE-ACCOUNT" m)))) dispatch)) With this implementation, two processes cannot be withdrawing from or depositing into a single account concurrently. Complexity concurrent programming can be treacherously difficult when there are multiple shared resources Implementing serializers synchronization mechanism called a mutex ( acquired &amp; release ) (define (make-mutex) (let ((cell (list false))) (define (the-mutex m) (cond ((eq? m 'acquire) (if (test-and-set! cell) (the-mutex 'acquire))); retry ((eq? m 'release) (clear! cell)))) the-mutex)) ;actual implementation depends on how our system runs concurrent processes (define (test-and-set! cell) (if (car cell) true (begin (set-car! cell true) false))) Deadlock Each process is stalled forever, waiting for the other Concurrency, time, and communication The basic phenomenon here is that synchronizing different processes, establishing shared state, or imposing an order on events requires communication among the processes. 3.5 Streams an alternative approach to modeling state Computational Object real-world computer real-world objects with local state computational objects with local variables time variation in the real world time variation in the computer implement the time variation of the states assignments to the local variables streams model change in terms of sequences that represent the time histories 3.5.1 Streams Are Delayed Lists Streams allows one to formulate programs as sequence manipulations, while ataining the efficiency of incremental computation. basic idea construct a stream only partially when accessed, automatically construct just enough more of itself to produce the required part preserving the illusion that the entire stream exists Delayed Evaluation cons-stream (cons-stream ⟨ a ⟩ ⟨ b ⟩ == (cons ⟨ a ⟩ (delay ⟨ b ⟩ )) ) selectors (define (stream-car stream) (car stream)) (define (stream-cdr stream) (force (cdr stream))) Implementing delay and force delay must package an expression so that it can be evaluated late (delay ⟨ exp ⟩ ) == (lambda () ⟨ exp ⟩ ) force simply calls the procedure (of no arguments) (define (force delayed-object) (delayed-object)) Optimization for forcing the same delayed object: Memoize (delay ⟨ exp ⟩ ) == (memo-proc (lambda () ⟨ exp ⟩ )) 3.5.2 Infinite Streams (define (integers-starting-from n) (cons-stream n (integers-starting-from (+ n 1)))) (define integers (integers-starting-from 1)) Defining streams implicitly (define fibs (cons-stream 0 (cons-stream 1 (add-streams (stream-cdr fibs) fibs)))) 3.5.3 Exploiting the Stream Paradigm (define (pi-summands n) (cons-stream (/ 1.0 n) (stream-map - (pi-summands (+ n 2))))) (define pi-stream (scale-stream (partial-sums (pi-summands 1)) 4)) sequence accelerator Even better, we can accelerate the accelerated sequence, and recursively accelerate that, and so on. Infinite streams of pairs interleave Streams as signals signal-processing systems that contain feedback loops (define (integral integrand initial-value dt) (define int (cons-stream initial-value (add-streams (scale-stream integrand dt) int))) int) 3.5.4 Streams and Delayed Evaluation Unfortunately, stream models of systems with loops may require uses of delay beyond the “hidden” delay supplied by cons-stream . ;does not work, because in the first line of solve the call to integral requires that the input dy be defined, which does not happen until the second line of solve (define (solve f y0 dt) (define y (integral dy y0 dt)) (define dy (stream-map f y)) y) ;Correct Definition (define (solve f y0 dt) (define y (integral (delay dy) y0 dt)) (define dy (stream-map f y)) y) Normal-order evaluation Delay and Force provides great programming flexibility but also can make our programs more complex normal-order evaluation language ( Section 4.2 ) all arguments to procedures are automatically delayed arguments are forced only when they are actually needed Mutability and Delayed evaluation do not mix well Unfortunately, including delays in procedure calls wreaks-havoc(破坏) with our ability to design programs that depend on the order of events such as programs that use assignment, mutate data, or perform input or output. 3.5.5 Modularity of Functional Programs and Modularity of Objects benefits of Assignment increase the modularity by encapsulating states Stream can provide an equivalent modularity without assignment. A functional-programming view of time Modeling with objects Functional programming language Assignment and Mutable objects Stream that represents the time history of states matches the way interacting with the world well-defined mathematical functions whose behavior does not change raise problems of constraining the order and syncronizing raise problems when we wish to design interactive systems We can model the world as a collection of separate, time-bound, interacting objects with state, or we can model the world as a single, timeless, stateless unity. Each view has powerful advantages, but neither view alone is completely satisfactory. A grand unification has yet to emerge]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[NN&DL-ch3] Improving the way neural networks learn]]></title>
    <url>%2F2018%2F04%2F15%2FNeuralNet%2F3%2F</url>
    <content type="text"><![CDATA[ch-3 Improving the way neural networks learn The techniques we’ll develop in this chapter include a better choice of cost function, the cross-entropy cost function; four so-called “regularization” methods ( L1 and L2 regularization, dropout, and artificial expansion of the training data ), which make our networks better at generalizing beyond the training data; a better method for initializing the weights in the network; a set of heuristics to help choose good hyper-parameters ; The cross-entropy cost function Problem of quadratic function Artificial neuron has a lot of difficulty learning when it’s badly wrong w = b = 2.0, η = 0.15 Quadratic cost function $ C = \frac { (y-a) ^ 2 }{ 2 } $ when the neuron’s output is close to 1 ( might be far away from the desired output ) the curve gets very flat, so $ σ′(z) $ gets very small. so $ ∂C/∂w $ and $ ∂C/∂b $ get very small. Introducing cross-entropy where $ a = σ(z) $ and $ z= \Sigma_j w_jx_j + b $ It’s a cost function : non-negative close to zero when a close to y avoids the problem of learning slowing down rates not controlled by $ σ’(z) $ Using the quadratic cost when we have linear neurons in the output layer all the neurons in the final layer are linear neurons outputs are simply $ a_j^L = y_j^L $, sigmoid not applied quadratic cost will not slowdown learning Softmax define a new type of output layer apply the so-called softmax function to z output is a set of positive numbers which sum up to 1, can be thought of as a probability distribution The learning slowdown problem log-likelihood cost function probability of $ a_y^L $ close to 1, cost close to 0 $ δ_j ^L = \partial C / \partial z_j^L $ Overfitting cost on the training data continue decrease while classification accuracy on the test data stops cost on the test data starts increase while classification accuracy on training data rises up to 100% It’s almost as though our network is merely memorizing the training set, without understanding digits well enough to generalize to the test set. Detect overfitting keeping track of accuracy on the test data as our network trains. using validation data: we may find hyper-parameters which fit particular peculiarities of the test_data if using test_data for detection. early stopping: stop training when accuracy no longer improving hold out method: validation_data is kept apart or “held out” from the training_data Avoid overfitting one of the best ways: increase the size of the training data regularization Regularization L2 regularization ( weight decay ) add an extra term on cost function Intuitively, the effect of regularization is to make it so the network prefers to learn small weights λ : compromising between finding small weights and minimizing the original cost function $ -\frac{ηλ}{n}*w $ weights shrink by an amount proportional to w Why does regularization help reduce overfitting? the 9th order model is really just learning the effects of local noise The smallness of the weights means that the network won’t change too much if we change a few random inputs here and there having a large bias doesn’t make a neuron sensitive to its inputs in the same way as having large weights We don’t have an entirely satisfactory systematic understanding of what’s going on, merely incomplete heuristics and rules of thumb. L1 regularization $ -\frac{ηλ}{n} * sgn(w) $weights shrink by a constant amount toward 0 when $|w| $ is large, shrink less; when $ |w| $ is small, shrink more Dropout modify the network itself rather than modifying the cost function start by randomly deleting half the hidden neurons forward-propagate the input and backpropagate the result, update network choosing a new random subset, repeat; Finally, run the full network by halving all weights. Heuristic Averaging the effects of a large number of different networks a neuron cannot rely on particular other neurons, robust to losing any individual connection Artificially expanding the training data The general principle is to expand the training data by applying operations that reflect real-world variation. An aside on big data and what it means to compare classification accuracies more training data can sometimes compensate for differences in the machine learning algorithm used Weight initialization Up to now choose w &amp; b using independent Gaussian random variables mean 0 and standard deviation 1 while z sum up over a total of 501 normalized Gaussian random variables z will have a very broad Gaussian distribution An easy way initialize more sharply peak mean 0 and standard deviation $ 1 / \sqrt {n_{in}} $ seems only speeds up learning, doesn’t change the final performance Connection with Regularization L2 regularization sometimes automatically gives us something similar to the new approach to weight initialization Handwriting recognition revisited: the code How to choose a neural network’s hyper-parameters? Problem when choose η=10.0 and λ=1000.0 classification accuracies are no better than chance Broad strategy examine network to achieve results better than chance Speed up experimentation stripping network down to the simplest increasing the frequency of monitoring When having a signal gradually decrease the frequency of monitoring experiment with a more complex architecture, adjust η and λ again Learning Rate First estimate the order of threshold for η so that training cost won’t oscillating or increasing control the step size in gradient descent, no need to monitor by accuracy Number of training Epochs Early stopping terminate when classification on validation data stops improving Learning rate schedule μ=0 there’s a lot of friction, the velocity can’t build upuse a large learning rate when weights are badly wrong later reduce as we make more fine-tuned adjustments The regularization parameter, λ starting initially λ=0.0 to determine η increase or decrease by factors of 10, get a fine-tune return and re-optimize η again Mini-batch size it’s possible to use matrix techniques to compute the gradient update for all examples in a mini-batch simultaneously using the larger mini-batch would speed things up Other techniques ####Variations on stochastic gradient descent Hessian technique incorporates not just the gradient, but also information about how the gradient is changing $ H $ is a matrix known as the Hessian matrix, whose $ jkth $ entry is $ ∂^2C/∂w_j∂w_k $ the sheer size of the Hessian matrix make it difficult to compute Momentum-based gradient descent introduces a notion of “velocity” and “friction” replace the gradient descent update rule $ w→w′=w−η∇C$ by the “force” ∇C is now modifying v, and the velocity is controlling the rate of change of w. 1−μ as the amount of friction in the system. μ=1, there is no friction; μ=0 there’s a lot of friction, the velocity can’t build up; Other models of artificial neuron tanh neuron ranges from -1 to 1, not 0 to 1 allows both positive and negative activations the activations in hidden layers would be equally balanced Rectified linear unit never cause saturate, no corresponding learning slowdown]]></content>
      <categories>
        <category>NeuralNet</category>
      </categories>
      <tags>
        <tag>NeuralNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[NN&DL-ch2] How the backpropagation algorithm works]]></title>
    <url>%2F2018%2F04%2F15%2FNeuralNet%2F2%2F</url>
    <content type="text"><![CDATA[ch-2 How the backpropagation algorithm works discuss how to compute the gradient ( ∇C\nabla C∇C ) of the cost function Heart of backpropagation an expression for the partial derivative ∂ C /∂ w of the cost function C with respect to any weight w ( or bias b ) in the network tells us how quickly the cost changes when we change the weights and biases Warm up: a fast matrix-based approach to computing the output from a neural network Notation Vectorized form apply the weight matrix to the activations, then add the bias vector, and finally apply the σ function a^l=σ(w^la^{l−1}+b^l) =σ(z^l) The two assumptions we need about the cost function the cost function can be written as an average $ C=\frac{1}{n}∑_xC_x$ over cost functions CxC_xC​x​​ for individual training examples, x. the cost can be written as a function of the outputs from the neural network: costC=C(aL)cost C = C ( a^L )costC=C(a​L​​) The four fundamental equations behind backpropagation **DEF error ** $ δ^l_j $ Error in the output layer δ^l Error δ^l in terms of the error in the next layer, δ^l+1 Rate of change of the cost with respect to any bias Rate of change of the cost with respect to any weight Proof of the four fundamental equations using chain rule The backpropagation algorithm correctness: because the cost is a function of outputs To understand how the cost varies with earlier weights and biases we need to repeatedly apply the chain rule, , working backward through the layers to obtain usable expressions Backpropagation: the big picture a clever way of keeping track of small perturbations to the weights (and biases) as they propagate through the network, reach the output, and then affect the cost]]></content>
      <categories>
        <category>NeuralNet</category>
      </categories>
      <tags>
        <tag>NeuralNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[NN&DL-ch1] Using neural nets to recognize handwritten digits]]></title>
    <url>%2F2018%2F04%2F13%2FNeuralNet%2Fnndlch1%2F</url>
    <content type="text"><![CDATA[ch-1 Using neural nets to recognize handwritten digits Perceptrons a type of artificial neuron a device that makes decisions by weighing up evidence perceptron in higher layer make a decision at a more complex and more abstract level many-layer network of perceptrons engage in sophisticated decision making weight real numbers expressing the importance of the respective inputs to the output bias a measure of how easy it is to get the perceptron to fire Use perceptrons to compute any logical function NAND gate is universal for computation Sigmoid neurons Learning DEF: changing the weights and biases over and over to produce better and better output Problems of Perceptron step function a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0 to 1 sigmoid neuron Input take on values between 0 an 1 Output also between 0 and 1 which is $ σ( wx + b ) $ $ σ(z) ≡ \frac { 1 }{ 1 + e ^ {-z } } $ where $ z = wx + b $ $ σ’(z) = σ(z) (1- σ(z) ) $ The smoothness of σ means that small changes Δw_j in the weights and Δb in the bias will produce a small change Δoutput in the output from the neuron. $ \Delta output \approx \Sigma _j \frac {\partial output}{ \partial w_j} \Delta w_j + \frac {\partial output}{ \partial b} \Delta b $ The architecture of neural networks input layer, output layer, hidden layer Feed Forward Neural Networks no loops in the network Recurrent neural networks feedback loops are possible have neurons which fire for some limited duration of time, before becoming quiescent A simple network to classify handwritten digits To recognize individual digits we will use a three-layer neural network Input: 28 by 28 pixel = 784 Output: 10 neurons represent for 0 to 9 Hidden layer: n = 15 as an example Output layer: why use 10 numbers rather than 4 bits? there’s no easy way to relate that most significant bit to simple shapes ?A way to think about Hidden layer weighting up evidence and produce an abstraction Learning with gradient descent cost function DEF: $ C( w,b ) ≡ \frac{1}{2n} ∑ ∥y(x) − a ∥^2 $ y(x)=(0,0,0,0,0,0,1,0,0,0)Ty(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^Ty(x)=(0,0,0,0,0,0,1,0,0,0)​T​​ : Desired output a : Output from network x : input, n: number of inputs, w : weight, b : bias Aim of our training algorithm minimize the cost C (w, b) as a function of the weights and biases Why not try to maximize that number? the number of images correctly classified is not a smooth function of the weights and biases Gradient Descent Algorithm repeatedly compute the gradient ∇C , and then to move in the opposite direction $ ΔC ≈ \frac { ∂ C }{∂ w_k} Δ w_k + \frac{ ∂ C } {∂ b_l} Δ b_l ≈ ∇C ⋅ Δv.$ choose Δv so as to make ΔC negative $Δv = −η∇C $ apply to each component $ w_k → w_k’ = w_k − η \frac{∂ C}{∂w_k }$ $ b_l → b_l’ = b_l − η \frac{∂ C}{∂b_l }$ Problem In practical implementations, η is often varied so that it remains a good approximation, but the algorithm is too slow Stochastic gradient descent used to speed up learning estimate the gradient $ ∇C $ by computing $ ∇C_x $ for a small sample of randomly chosen training inputs $ w_k → w_k’ = w_k − \frac{η}{m} \Sigma_j\frac{∂ C}{∂w_k }$ $ b_l → b_l’ = b_l − \frac{η}{m}\Sigma_j \frac{∂ C}{∂b_l }$ Toward deep learning The weights and biases in the network were discovered automatically. And that means we don’t immediately have an explanation of how the network does what it does A heuristic we could use is to decompose the problem into sub-problems early layers answering very simple and specific questions about the input image later layers building up a hierarchy of ever more complex and abstract concepts]]></content>
      <categories>
        <category>NeuralNet</category>
      </categories>
      <tags>
        <tag>NeuralNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch8] Shell Lab]]></title>
    <url>%2F2018%2F03%2F21%2FCSAPP%2Fshell%2F</url>
    <content type="text"><![CDATA[[CS:APP-ch8] Shell Lab http://csapp.cs.cmu.edu/3e/labs.html http://condor.depaul.edu/glancast/374class/hw/shlab-readme.html https://github.com/zhoudiqiu/Shell-lab/blob/master/lab5/tsh.c Target implement eval, builtin_cmd, do_bgfg, waitfg sigchld_handler, sigint_handler, sigtstp_handler 0. Preparation tshref: 正确的参考shell，要求自己实现后的shell和它的结果一样 make test01用trace01.txt来验证结果，可以参考make rtest01的结果 gdb makefile去掉参数O2,加上-g 1. void eval ( char*cmdline ) 原型：p755 If command is a built-in command, the shell program handles it immediately. Otherwise, the shell creates a child process to load and execute the program for command. Hint In eval, the parent must use sigprocmaskto block SIGCHLD signals before it forks the child, then unblock these signals, again using sigprocmask after it adds the child to the job list by calling addjob. Since children inherit the blocked vectors of their parents, the child must be sure to then unblock SIGCHLD signals before it execs the new program. The parent needs to block the SIGCHLD signals in this way in order to avoid the race condition where the child is reaped by sigchld_handler (and thus removed from the job list) before the parent calls addjob. After the fork, but before the execve, the child process should call setpgid(0, 0), which puts the child in a new process group whose group ID is identical to the child’s PID. This ensures that there will be only one process, your shell, in the foreground process group. When you type ctrl-c, the shell should catch the resulting SIGINT and then forward it to the appropriate foreground job (or more precisely, the process group that contains the foreground job). /* * eval - Evaluate the command line that the user has just typed in * * If the user has requested a built-in command (quit, jobs, bg or fg) * then execute it immediately. Otherwise, fork a child process and * run the job in the context of the child. If the job is running in * the foreground, wait for it to terminate and then return. Note: * each child process must have a unique process group ID so that our * background children don't receive SIGINT (SIGTSTP) from the kernel * when we type ctrl-c (ctrl-z) at the keyboard. */ void eval ( char *cmdline ) { pid_t pid; sigset_t mask; char *argv[ MAXARGS ]; //Parse the command line and build the argv array int bg = parseline ( cmdline, argv ); if ( !builtin_cmd ( argv ) ) { // blocking SIGCHLD, race sigemptyset ( &amp;mask ); sigaddset ( &amp;mask, SIGCHLD ); sigprocmask ( SIG_BLOCK, &amp;mask, NULL ); // fork new process if ( ( pid = fork () ) &lt; 0 ) unix_error ( "fork error" ); // child process else if ( pid == 0 ) { // unblock SIGCHLD in child process sigprocmask ( SIG_UNBLOCK, &amp;mask, NULL ); // ensures that there will be only one process setpgid ( 0, 0 ); // execute command if ( execvp ( argv[ 0 ], argv ) &lt; 0 ) { printf ( "%s: Command not found\n", argv[ 0 ] ); exit ( 1 ); } // parent } else { addjob ( jobs, pid, bg ? BG : FG, cmdline ); // get SIGCHLD back after add job sigprocmask ( SIG_UNBLOCK, &amp;mask, NULL ); if ( !bg ) { // reap when job terminated ( send signal SIGCHILD ) waitfg ( pid ); } else { //[1] (6325) ./myspin 1 &amp; printf ( "[%d] (%d) %s", pid2jid ( pid ), pid, cmdline ); } } } return; } 2.int builtin_cmd( char **argv) Four commands are to be built-in in the shell quit: exit the shell. jobs: List the running and stopped background jobs. /* * builtin_cmd - If the user has typed a built-in command then execute * it immediately. */ int builtin_cmd ( char **argv ) { if ( strcmp ( argv[ 0 ], "quit" ) == 0 ) { exit ( 0 ); } else if ( strcmp ( argv[ 0 ], "jobs" ) == 0 ) { listjobs ( jobs ); return 1; } else if ( !strcmp ( argv[ 0 ], "fg" ) || !strcmp ( argv[ 0 ], "bg" ) ) { do_bgfg ( argv ); return 1; } return 0; /* not a builtin command */ } 3. void waitfg( pid_t pid ) shell should wait for foreground process This function should wait by sleeping for 1 second repeatedly until the specified process is no longer the foreground process (state = FG) Hints In waitfg, use a busy loop around the sleep function. /* * waitfg - Block until process pid is no longer the foreground process */ void waitfg ( pid_t pid ) { struct job_t *job = getjobpid ( jobs, pid ); if ( pid == 0 ) return; if ( job != NULL ) { // sleep while ( pid == fgpid ( jobs ) ) { } } return; } 4. void do_bgfg( char **argv ) bg &lt;job&gt;: Change a stopped background job to a running background job. fg &lt;job&gt;: Change a stopped or running background job to a running in the foreground. The bg &lt;job&gt; command restarts &lt;job&gt; by sending it a SIGCONTsignal, and then runs it in the background. The &lt;job&gt; argument can be either a PID or a JID. The fg &lt;job&gt;command restarts &lt;job&gt; by sending it a SIGCONTsignal, and then runs it in the foreground. The &lt;job&gt; argument can be either a PID or a JID. /* * do_bgfg - Execute the builtin bg and fg commands */ void do_bgfg ( char **argv ) { struct job_t *job; char *tmp; int jid; pid_t pid; tmp = argv[ 1 ]; // unvalid id if ( tmp == NULL ) { printf ( "%s command requires PID or %%jobid argument\n", argv[ 0 ] ); return; } // jid if ( tmp[ 0 ] == '%' ) { // string to integer jid = atoi ( &amp;tmp[ 1 ] ); job = getjobjid ( jobs, jid ); // unvalid jid if ( job == NULL ) { //%2: No such job printf ( "%s: No such job\n", tmp ); return; } else { pid = job-&gt;pid; } } // pid else if ( isdigit ( tmp[ 0 ] ) ) { pid = atoi ( tmp ); job = getjobpid ( jobs, pid ); // unvalid pid if ( job == NULL ) { //(2): No such process printf ( "(%d): No such process\n", pid ); return; } } else { printf ( "%s: argument must be a PID or %%jobid\n", argv[ 0 ] ); return; } // awakened by the receipt of a SIGCONT signal. kill ( -pid, SIGCONT ); if ( !strcmp ( "fg", argv[ 0 ] ) ) { // foreground, shell wait for it job-&gt;state = FG; waitfg ( job-&gt;pid ); } else { job-&gt;state = BG; // print printf ( "[%d] (%d) %s", job-&gt;jid, job-&gt;pid, job-&gt;cmdline ); } return; } 5.void sigchld_handler( int sig ) This is the shell’s handler for theSIGCHLD signal. Hints In sigchld_handler, use exactly one call to waitpid /* * sigchld_handler - The kernel sends a SIGCHLD to the shell whenever * a child job terminates (becomes a zombie), or stops because it * received a SIGSTOP or SIGTSTP signal. The handler reaps all * available zombie children, but doesn't wait for any other * currently running children to terminate. */ void sigchld_handler ( int sig ) { pid_t pid; int status; // wait for all child to end while ( ( pid = waitpid ( -1, &amp;status, WNOHANG | WUNTRACED ) ) &gt; 0 ) { // child currently stopped if ( WIFSTOPPED ( status ) ) { // change state struct job_t *job = getjobpid ( jobs, pid ); job-&gt;state = ST; int jid = pid2jid ( pid ); // Job [2] (14171) stopped by signal 20 printf ( "Job [%d] (%d) stopped by signal %d\n", jid, pid, WSTOPSIG ( status ) ); } // child terminated because of a signal else if ( WIFSIGNALED ( status ) ) { int jid = pid2jid ( pid ); // INT // Job [1] (13253) terminated by signal 2 printf ( "Job [%d] (%d) terminated by signal %d\n", jid, pid, WTERMSIG ( status ) ); deletejob ( jobs, pid ); } // child terminated normally else if ( WIFEXITED ( status ) ) { deletejob ( jobs, pid ); } } return; } 6. void sigint_handler ( int sig ) Hints When you implement your signal handlers, be sure to send SIGINT and SIGTSTP signals to the entire foreground process group, using -pid instead of pid in the argument to the kill function. The sdriver.pl program tests for this error. /* * sigint_handler - The kernel sends a SIGINT to the shell whenver the * user types ctrl-c at the keyboard. Catch it and send it along * to the foreground job. */ void sigint_handler ( int sig ) { pid_t pid = fgpid ( jobs ); if ( pid != 0 ) { kill ( -pid, sig ); } return; } 7.void sigtstp_handler ( int sig ) /* * sigtstp_handler - The kernel sends a SIGTSTP to the shell whenever * the user types ctrl-z at the keyboard. Catch it and suspend the * foreground job by sending it a SIGTSTP. */ void sigtstp_handler ( int sig ) { pid_t pid = fgpid ( jobs ); if ( pid != 0 ) { kill ( -pid, sig ); } return; }]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch2] Building Abstractions with Data]]></title>
    <url>%2F2018%2F03%2F10%2FSICP%2Fsicp2%2F</url>
    <content type="text"><![CDATA[SICP-ch2 Building Abstractions with Data building abstractions by forming compound data Intro 2.1 data abstraction erect abstraction barriers 2.2 data == procedures, sequences, closure, conventional interface 2.3 symbols as elementary data 2.4 generic operations, data directed programming, additivity 2.5 implement a package 2.1 Introduction to Data Abstraction Data abstraction isolating ( the parts of a program that deal with how data objects are represented ) from ( the parts of a program that deal with how data objects are used ) Construct abstract data use constructors, selectors as interface 2.1.1 Example: Arithmetic Operations for Rational Numbers assume the constructor and selectors are available numer , denom , make-rat use them to express rational operation (define (add-rat x y) (make-rat (+ (* (numer x) (denom y)) (* (numer y) (denom x))) (* (denom x) (denom y)))) implement representation (define (make-rat n d) (cons n d)) (define (numer x) (car x)) (define (denom x) (cdr x)) 2.1.2 Abstraction Barriers 2.1.3 What Is Meant by Data? DEF: some collection of selectors and constructors together with specified conditions for the representation Example Pairs z = ( x, y ) selector: car, cdr constructor: cons condition: if z = ( x, y ) =&gt; ( car z ) = x &amp;&amp; ( cdr z ) = y (define (cons x y) (define (dispatch m) (cond ((= m 0) x) ((= m 1) y) (else (error "Argument not 0 or 1: CONS" m)))) ;message passing: data representation returns procedure dispatch) (define (car z) (z 0)) (define (cdr z) (z 1)) Exercise 2.6 ;2.6 ;church numerals DEF ( define zero ( lambda ( f ) ( lambda ( x ) x ) ) ) ( define ( add-1 n ) ( lambda ( f )( lambda ( x ) ( f ( ( n f ) x ) )))) ;从0开始，每多一个f就+1 ( define ( church-to-int ch ) ( ( ch ( lambda ( n ) ( + n 1 ) ) ) 0 ) ) ( define one ( lambda ( f ) ( lambda ( x ) ( f x ) ) ) ) ( define two ( lambda ( f ) ( lambda ( x ) ( f ( f x ) ) ) ) ) ;a,b: function of zero, one, two... ;b以x为参数经过b次apply f ;a以x经过b次apply后的结果为参数(( b f ) x )，在此基础上再apply f a次 ( define ( add a b ) ( lambda ( f ) ( lambda ( x ) ( ( a f ) ( ( b f ) x ) ) ) ) ) ;调用 ( define ( square x ) ( * x x )) ; f = square x = 2 ( ( two square ) 2 ) 2.2 Hierarchical Data and the Closure Property closure property of cons ( an operation ) The ability to create pairs whose elements are pairs Closure is the key to create hierarchical structures 2.2.1 Representing Sequences The entire sequence is constructed by nested cons operations (cons 1 (cons 2 (cons 3 (cons 4 nil)))) Provides a primitive (list ⟨ a 1 ⟩ ⟨ a 2 ⟩ . . . ⟨ a n ⟩ ) Operations ref, length, append, last-pair, reverse Mapping over list Establish an abstraction barrier that isolates the implementation of procedures that transform lists from the details of how the elements of the list are extracted and combined. (define (map proc items) (if (null? items) nil (cons (proc (car items)) (map proc (cdr items))))) 2.2.2 Hierarchical Structures Tree sequences whose elements are sequences Recursion is a natural tool for dealing with tree structures 2.2.3 Sequences as Conventional Interfaces Conventional Interface permits us to combine processing modules Signal Processing A signal-processing engineer would find it natural to conceptualize these processes in terms of signals flowing through a cascade of stages, each of which implements part of the program plan. Organizing programs so as to reflect the signal-flow structure concentrate on the “signals” represent these signals as lists use list operations to implement the processing helps us make program designs that are modular connecting the components in flexible ways Nested Mappings (define (flatmap proc seq) (accumulate append nil (map proc seq))) 2.3 Symbolic Data Issue arise words and sentences may be regarded either as semantic entities ( meaning ) or as syntactic entities ( characters ) Using quotation mark (list 'a 'b) (a b) manipulating symbols eq?, memq 2.4 Multiple Representations for Abstract Data cope with data that may be represented in different ways by different parts of a program Data Abstraction separate the task of designing a program that uses rational numbers from the task of implementing rational numbers an application of the “principle of least commitment” Abstraction Barrier formed by the selectors and constructors permits us to defer to the last possible moment the choice of a concrete representation thus retain maximum flexibility 2.4.1 Representations for Complex Numbers constructors (make-from-real-imag (real-part z) (imag-part z)) (make-from-mag-ang (magnitude z) (angle z)) selectors rectangular representation &amp; polar representation real-part,imag-part,magnitude,angle implement arithmetic on complex numbers add-complex, sub-complex, mul-complex, div-complex implement 2 representations of complex number 分别用直角和极坐标来实现4个selector和2个constructor 2.4.2 Tagged data If both representations are included in a single system, we will need some way to distinguish data in polar form from data in rectangular form. A straightforward way to accomplish this distinction is to include a type tag —the symbol rectangular or polar —as part of each complex number. (define (make-from-real-imag-rectangular x y) (attach-tag 'rectangular (cons x y))) Make sure names do not conflict: Append the suffix -rectangular (define (real-part-rectangular z) (car z)) Generic selector Dispatch: checks the tag and calls the appropriate procedure of that type. because the selectors are generic, operation ( add, mul ) are unchanged (define (real-part z) (cond ((rectangular? z) (real-part-rectangular (contents z))) ((polar? z) (real-part-polar (contents z))) (else (error "Unknown type: REAL-PART" z)))) Generic Constructor (define (make-from-real-imag x y) (make-from-real-imag-rectangular x y)) General mechanism for interfacing the separate representations stripping off and attaching tags as data objects are passed from level to level 2.4.3 Data-Directed Programming and Additivity Two weaknesses of dispatching above generic interface procedures must know about all the different representations must guarantee that no two procedures in the entire system have the same name Additive design the individual packages separately and combine them to produce a generic system Data-directed programming dealing with a two-dimensional table the possible operations on one axis and the possible types on the other axisss Manipulate table (put ⟨ op ⟩ ⟨ type ⟩ ⟨ item ⟩ ) (get ⟨ op ⟩ ⟨ type ⟩ ) Defines a package Interfaces these by adding entries to the table (define (install-rectangular-package) ;; internal procedures (define (real-part z) (car z)) (define (make-from-real-imag x y) (cons x y)) ;; interface to the rest of the system (define (tag x) (attach-tag 'rectangular x)) (put 'real-part '(rectangular) real-part) (put 'make-from-real-imag 'rectangular (lambda (x y) (tag (make-from-real-imag x y)))) 'done) Generic Selectors (define (apply-generic op . args) (let ((type-tags (map type-tag args))) (let ((proc (get op type-tags))) (if proc ;apply arguments to procedure (apply proc (map contents args)) (error "No method for these types: APPLY-GENERIC" (list op type-tags)))))) (define (real-part z) (apply-generic 'real-part z)) Generic Constructors (define (make-from-real-imag x y) ((get 'make-from-real-imag 'rectangular) x y)) Interface ( generic selectors ) Previous Data-directed a set of procedures single procedure explicit dispatch looks up name conflicts internal change if a new representation is added doesn’t change Centralized selectors Decentralized Message passing Data object receives the requested operation name as a “message.” Instead of using “intelligent operations” that dispatch on data types work with “intelligent data objects” that dispatch on operation names. Represent data object as a procedure takes as input the required operation name performs the operation indicated Constructor (define (make-from-real-imag x y) (define (dispatch op) (cond ((eq? op 'real-part) x) (else (error "Unknown op: MAKE-FROM-REAL-IMAG" op)))) ;return procedure dispatch) Selector (define (apply-generic op arg) (arg op)) 2.5 Systems with Generic Operations use data-directed techniques to construct a package of arithmetic operations 2.5.1 Generic Arithmetic Operations generic arithmetic procedures (define (add x y) (apply-generic 'add x y)) install packages (define (install-complex-package) ;; imported procedures from rectangular and polar packages (define (make-from-real-imag x y) ((get 'make-from-real-imag 'rectangular) x y)) ;; internal procedures (define (add-complex z1 z2) (make-from-real-imag (+ (real-part z1) (real-part z2)) (+ (imag-part z1) (imag-part z2)))) ;; interface to rest of the system (define (tag z) (attach-tag 'complex z)) (put 'make-from-real-imag 'complex (lambda (x y) (tag (make-from-real-imag x y)))) 'done) constructor (define (make-complex-from-real-imag x y) ((get 'make-from-real-imag 'complex) x y)) two-level tag system 2.5.3 Example: Symbolic Algebra implement polynomial package arithmetic on polynomials representation of polynomials arithmetic on term list representation of term list Data Directed Recursion using generic operation ( ADD ) inside each package Hierarchy of types in symbolic data recursive data abstraction neither of these types is above the other naturally difficult to control coercion 2.5.2 Combining Data of Different Types Define cross-type operations One way: design a different procedure for each possible combination of types ;; to be included in the complex package (define (add-complex-to-schemenum z x) (make-from-real-imag (+ (real-part z) x) (imag-part z))) (put 'add '(complex scheme-number) (lambda (z x) (tag (add-complex-to-schemenum z x)))) Cumbersome more code is needed individual packages need to take account of other packages ( not additive ) Coercion objects of one type may be viewed as being of another type coercion procedure (define (scheme-number-&gt;complex n) (make-complex-from-real-imag (contents n) 0)) install in coercion table (put-coercion 'scheme-number 'complex scheme-number-&gt;complex) apply-generic check the coercion table to see if objects of the first type can be coerced to the second type or if there is a way to coerce the second argument to the type of the first argument (define (apply-generic op . args) (let ((type-tags (map type-tag args))) (let ((proc (get op type-tags))) (if proc (apply proc (map contents args)) (if (= (length args) 2) (let ((type1 (car type-tags)) (type2 (cadr type-tags)) (a1 (car args)) (a2 (cadr args))) (let ((t1-&gt;t2 (get-coercion type1 type2)) (t2-&gt;t1 (get-coercion type2 type1))) (cond (t1-&gt;t2 (apply-generic op (t1-&gt;t2 a1) a2)) (t2-&gt;t1 (apply-generic op a1 (t2-&gt;t1 a2))) (else (error "No method for these types" (list op type-tags)))))) (error "No method for these types" (list op type-tags))))))) Advantage need to write only one procedure for each pair of types rather than a different procedure for each collection of types and each generic operation. Not General Enough can’t converting both objects to a third type. Hierarchies of types apply-generic raise the object to its super type until we either find a level at which the desired operation can be performed or hit the top Inadequates multiple-supertypes means that there is no unique way to “raise” a type in the hierarchy]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch1] Building Abstractions with Procedures]]></title>
    <url>%2F2018%2F02%2F28%2FSICP%2Fsicp1%2F</url>
    <content type="text"><![CDATA[SICP ch-1 Building Abstractions with Procedures We are about to study the idea of computational process 1.1 The Elements of Programming Language combine simple ideas to form complex ideas by three mechanisms primitive expression means of combination means of abstraction 1.1.1 Expressions you type an expression, the interpreter responds by displaying the result of its evaluating that expression ( 486 ) Combinations Expressions formed by delimiting a list of expressions within parentheses ( + 137 349 ) operator, operands 1.1.2 Naming and the Environment Naming language provides the means of using names ( variable ) to refer to computational objects ( value ) Associating values with symbols Environment The memory that interpreter must maintain to keeps track of the name-object pairs. ( ch-3 ) Determine the meaning of the symbols in expressions. 1.1.3 Evaluating Combinations Interpreter is itself following a procedure Evaluate the subexpressions of the combination Apply the procedure that is the value of the leftmost subexpression (the operator) to the arguments that are the values of the other subexpressions (the operands). Thus, the evaluation rule is recursive in nature. Special Forms Exceptions to the general evaluation rule. ( apply operator to operands ) Each special form has its own evaluation rule. define, cond, if, and, or, 1.1.4 Compound Procedures Procedure definitions (define (&lt;name&gt; &lt;formal parameters&gt;) &lt;body&gt;) 1.1.5 The Substitution Model for Procedure Application Evaluate a combination evaluates the elements of the combination applies the procedure to the arguments Substitution Model To apply a compound procedure to arguments, evaluate the body of he procedure with each formal parameter replaced by the corresponding argument. Typical interpreters do not evaluate procedure by substitute values, but using a local environment for the formal parameters ( ch-3 ) This model breaks down when we address procedures with “mutable data” ( ch-3 set! ) Normal Order fully expand and then reduce (x) Applicative Order evaluate the arguments and then apply 1.1.6 Conditional Expressions and Predicates Predicate an expression whose value is interpreted as either true or false. (cond (&lt;p1&gt; &lt;e1&gt;) (&lt;p2&gt; &lt;e2&gt;) ... (&lt;pn&gt; &lt;en&gt;)) (if &lt;predicate&gt; &lt;consequent&gt; &lt;alternative&gt;) (and &lt;e1&gt; ... &lt;en&gt;) (or &lt;e1&gt; ... &lt;en&gt;) (not &lt;e&gt;) 1.1.7 Example: Square Roots by Newton’s Method 1.1.8 Procedures are Black-Box Abstractions bound variables In procedure definition, it doesn’t matter what name the formal parameter has. free variables the scope of that name 1.2 Procedures and the Processes They Generate Procedure A procedure is a pattern for the local evolution of a computational process. It specifies how each stage of the process is built upon the previous stage. 1.2.1 Linear Recursion and Iteration Recursive procedure syntactic fact that the procedure definition refers to the procedure itself Recursive process The process that characterized by a chain of deferred operations Requires interpreter keep track of operations to be performed later on. Iterative process one whose state can be summarized by a fixed number of state variables, together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an (optional) end test. Tail Recursive An implementation with the property of: “An iterative process will be executed in constant space, even if it is described by a recursive procedure.” 1.3 Formulating Abstractions with Higher-Order Procedures Procedures are abstractions that describe compound operations Higher-order procedures procedures that manipulate procedures 1.3.1 Procedures as Arguments 1.3.2 Constructing Procedures Using lambda In general, lambda is used to create procedures in the same way as define , except that no name is specified for the procedure (lambda (&lt;formal parameters&gt;) &lt;body&gt;) using lambda creating local variables (let ((&lt;var1&gt; &lt;exp1&gt;) (&lt;var2&gt; &lt;exp2&gt;) ... (&lt;varn&gt; &lt;expn&gt;)) &lt;body&gt;) 1.3.3 Procedures as General Methods finding roots finding fixed point 1.3.4 Procedures as Returned Values return values are themselves procedures Abstractions and first-class procedures programming languages impose restrictions on the ways in which computational elements can be manipulated First-class status elements with fewest restrictions They may be named by variables. They may be passed as arguments to procedures. They may be returned as the results of procedures. They may be included in data structures. Lisp awards procedures full first-class status, which poses challenges for efficient implementation, but the resulting gain in expressive power is enormous.]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch1] A Tour of Computer Systems]]></title>
    <url>%2F2018%2F01%2F01%2FCSAPP%2Fch-1%2F</url>
    <content type="text"><![CDATA[CS:APP ch-1 A Tour of Computer Sytems A computer system consists of hardware and systems software that work together to run application programs. Tracing the life time of hello program. #include &lt;stdio.h&gt; int main() { printf("hello world!\n"); return 0; } 1.1 Information Is Bits + Context The hello program begins life as a source program ( a sequence of bits, organized in bytes ) Text files files consist exclusively ASCII characters. Binary files all other files All information in a system is represented as a bunch of bits. The only thing that distinguishes different data objects is the context in which we view them. 1.2 Programs Are Translated by Other Programs into Different Forms In order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk file. 1.3 It Pays to Understand How Compilation Systems Work Reasons why need to understand compilation systems Optimizing program performance Understanding link-time error Avoiding security holes 1.4 Processors Read and Interpret Instructions Stored in Memory To run the executable object file, type ./hello to an applcation program known as a shell Hardware Organization of a System Buses: transfer words I/O Devices: system’s connection to the external world, connected to I/O bus by either a controller or an adapter Main Memory: temporary storage device that holds both a program and the data it manipulates while the processor is executing the program Processor: the engine that inter-prets (or executes ) instructions stored in main memory Running hello Reading command from keyboard Load file from disk to memory Write output string from memory to display 1.5 Caches Matter The machine instruction in the hello program are originally stored on disk When loaded, opied to main memory When processor runs, copied into the processor ( register + caches ) deal with processor-memory gap 1.6 Storage Devices Form a Hierarchy storage at one level serves as a cache for the next lower level 1.7 The Operating System Manages the Hardware OS a layer of software interposed between the application program and the hardware two primitive purpose: (1)protect hardware (2)provide applications with simple and uniform mechanisms for manipulating hardwares achieve both goals via abstractions Process The operating system’s abstraction for a running program context switching Thread VIrtual Memory Each process has the same uniform view of memory ( virtual address space ) Files A sequence of bytes, nothing more and nothing less Every I/O devices is modeled as a file 1.8 Systems Communicate with Other Systems Using Networks 1.9 Important Themes Amdahl’s law: observation of the effectiveness Concurrency and Parallelism concurrency a system with multiple, simultaneous activities parallelism use of concurrency to make a system run faster Parrallelism can be exploited at multiple( three ) levels of abstraction in a computer system. ( hight --&gt; low ) Thread-level Concurrency: multicore &amp; hyperthreading Instruction-level Parallelism: pipeling Single-Instruction, Multiple-Data ( SIMD ) Parallelism The Importance of Abstractions in Computer Systems Processor side instruction set architecture provides an abstraction of actual processor Operating System side Files as an abstraction of I/O Virtual Memory as an abstraction of Program Memory Processes as an abstraction of a Running Program Virtual Machine as an abstraction of the Entire Computer]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch9] Virtual Memory]]></title>
    <url>%2F2017%2F12%2F29%2FCSAPP%2Fch-9%2F</url>
    <content type="text"><![CDATA[CS:APP ch-9 Virtual Memory Virtual Memory modern systems provide an abstraction of main memory three important capabilities It uses main memory efficiently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory, and transferring data back and forth between disk and memory as needed. It simplifies memory management by providing each process with a uniform address space. It protects the address space of each process from corruption by other processes. 9.1 Physical and Virtual Addressing Main memory organized as an array of M contiguous byte-size cells, each byte has a unique physical address. Physical Addressing: use physical address ( PA ) to access memory Virtual Addressing: use virtual address ( VA ) and translate to physical address Addressing Translation: converting a virtual address to a physical one 9.2 Address Spaces Virtual Address Space CPU generates virtual addresses from an address space of $ N = 2^n $ addresses called the virtual address space {0 , 1, 2 ,…,N − 1} Physical Address Space A system also has a physical address space that corresponds to the M bytes of physical memory in the system {0 , 1, 2 ,…,M − 1} 9.3 VM as a Tool for Caching A virtual memory is organized as an array of N contiguous byte-sized cells stored on disk. Each byte has a unique virtual address that serves as an index into the array. VM systems partition the virtual memory into fixed-size blocks call Virtual Pages ( VP ) as transfer units between disk and main memory. Three sets of VP: Unallocated, Cached, Uncached DRAM cache organization large virtual pages ( cache block )( 4 KB to 2 MB ) fully associative sophisticated replacement algorithms use write-back Page Tables A data structure stored in physical memory that maps virtual pages to physical pages. Page Hit Check valid bit, uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word. Page Faults The page fault exception invokes a page fault exception handler in the kernel selects a victim page, in this case VP 4 stored in PP 3. modifies the page table entry for VP 4 to reflect the fact that VP 4 is no longer cached in main memory. copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns. restarts the faulting instruction and page hit The strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging 9.4 VM as a Tool for Memory Management Operating systems provide a separate page table, and thus a separate virtual address space, for each process. Linking: Allow each process to use the same basic format for its memory image ( code segment always start at 0x400000 ) Loading: (1) Allocate Virtual Pages (2) Mark as invalid ( not cached ) (3) Point table entries to the location of in object file (4) Data are paged in th first time referenced Sharing: Multiple processes share a single copy of some code Memory Allocation ​ ​ 9.5 VM as a Tool for Memory Protection three permission bits to each PTE SUP must run in kernel mode READ / WRITE 9.6 Address Translation a mapping between an N-element virtual address space and M-element physical address space Steps that the CPU hardware performs when there is a page hit / fault Speeding up Address Translation with a TLB a small cache of PTEs in the MMU called a translation look aside buffer (TLB) Multi-Level Page Tables つ…つづく！]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch8] Exceptional Control Flow]]></title>
    <url>%2F2017%2F12%2F28%2FCSAPP%2Fch8%2F</url>
    <content type="text"><![CDATA[CS:APP ch-8 Exceptional Control Flow control flow sequence of control transfers ( from the address of instruction to the next ) Exceptional Control Flow ( ECF ) Abrupt changes in control flow Occurs all levels Hardware: Transfers to exceptional handlers Operating System: Context switch Application: Signal Individual Program: Nonlocal jump 8.1 Exceptions DEF Abrupt change in control flow in response to some change in the processor’s state event change in state Exception Handling Hardware processor detect an event determine corresponding exception number k trigger exception by calling the handler through entry k of the exception table Software ( handler ) do some processing return control to the interrupted control flow Difference with procedure call return either current or next instruction push additional processor state to restart ( on kernel stack ) handlers run in kernel mode Classes of Exceptions Interrupt: I/O devices Trap: System call ( fork, read, execve, exit ) Fault: error conditions that might be able to correct or just abort ( page fault, segmentation fault, divide zero ) Abort: unrecoverable ( machine check ) 8.2 Processes DEF: An instance of program in execution program run in context of process Logical Control Flow provide an illusion that program has exclusive use of the processor The sequence of PC values is know as logical control flow Each process executes a portion of its flow and then is preempted (temporarily suspended) while other processes take their turns. Concurrent Flow A logic flow whose execution overlaps in time with another flow Concurrency The general phenomenon of multiple flows executing concurrently Parallel Flow subset of concurrent flow that run on different cores Private Address Space provide an illusion that program has exclusive use of the system address’s space User and kernel mode: mode bit context the state that kernel needs to restart the preempted process ( general-purpose registers, floating point registers, program counter, user’s stack, status register, kernel’s stack, page table, process table, file table ) scheduling the kernel decide to preempt the current process and restart a previously preempted process Context Switch Saves context of the current process Restores the saved context of previous process Passes control to newly restored process Example when execute read system call ( disk ) Kernel switch process until disk sends an interrupt signal 8.4 Process Control Obtaining Process ID pid_t getpid ( void ); Create and Terminating process Three states Running Stopped Terminated void exit ( int status ); //return 0 to child, PID of chile to parent pid_t fork ( void ); Call Once Return Twice Concurrent execution Duplicate but separate address space Shared files Reaping Child Process The process kept around in a terminated state until it is reaped by its parent 父进程进行回收的函数，也是一个等待子进程执行结束的函数就是waitpid。这在 APUE（advanced programming in unix enviroment）中很早就提过这个函数 // return PID of terminated child and reap it pid_t waitpid(pid_t pid, int *statud, int options) Putting Process to Sleep unsigned int sleep(unsigned int secs); int pause(void); // 一直休眠，直到收到一个信号 Loading and Running Programs int execve(const char *filename, const char *argv[], const char *envp[]); 8.5 Signals The transfer of a signal to a destination process occurs in two distinct steps Sending a signal The kernel has detected a system event such as a divide-by-zero error or the termination of a child process. A process has invoked the kill function to explicitly request the kernel to send a signal to the destination process. A process can send a signal to itself. Receiving a signal The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler. Pending signal A signal that has been sent but not yet received At any point in time, there can be at most one pending signal of a particular type. Received at most once: The kernel sets bit k in pending whenever a signal of type k is delivered and clears bit k in pending whenever a signal of type k is received. 8.6 Nonlocal Jumps]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch6] The Memory Hierarchy]]></title>
    <url>%2F2017%2F12%2F27%2FCSAPP%2Fch-6%2F</url>
    <content type="text"><![CDATA[CS:APP ch-6 The Memory Hierarchy 6.1 Storage Technologies SRAM cache, transistor, stable, fast, expensive DRAM main memory, capacitor, sensitive( periodically refresh every bit ) d * w cells --&gt; d supercells == r rows * c columns ( two dimension array ) data pins ( w bits ) &amp; addr pins ( row addr i, column addr j ) Memory modules: Each supercell stores 11 byte. Each address represent by 8 supercells Enhanced DRAM Nonvolatile Memory ( ROM ) retain information even when they’re powered off Flash memory ( erasable programmable ROM ) firmware: programs stored in ROM Accessing Main Memory I/O bridge: translate signals Disk platter --&gt; surface --&gt; track --&gt; sectors Logical Disk Blocks: hide complexity from OS disk controller: maintains mapping between logical block number and physical disk sectors I/O devices: USB, graphics card, host bus adapter I/O bus: connect disks to CPU and main memory, independent under CPU Accessing Disk Memory mapped I/O: a block of addresses in the address space is reserved for communicating with I/O devices. Each of these address is known as an I/O port. Each device associated with one or more ports. CPU read disk initiate a read indicate logical block number that should be read indicate main memory address to store disk sector do other work Disk controller receives the read command translates logical block number to sector address read content transfer content directly to main memory ( DMA ) send a interrupt signal to CPU CPU returns control to the point where interrupted Solid State Disk A page can be written only after the entire block it belongs to has erased A block wears out after roughly 100, 000 repeated writes Advantage: semiconductor, faster, use less power, more rugged Disadvantage: potential to wear out, expensive 6.2 Locality Temporal Locality: reference again multiple times Spatial Locality: reference nearby locations stride-k reference pattern: visiting every kth element of a contiguous vector k increase, spatial decrease 6.3 The Memory Hierarchy Caching in memory hierarchy Cache hits Cache misses replacing / evicting the block, victim block, replacement policy Kind of Cache misses cold miss: cache is empty conflict miss: map to the same block capacity miss: size of working set exceed Cache management partition storage into blocks, transfer blocks between levels, deal with hits and misses compiler – register file hardware logic – L1, L2, L3 caches OS + Address Translation hardware – main memory AFS client process – disk 6.4 Cache Memories Main memory: $ M = 2^m $ cache: $ C = S * E * B $ $ S = 2^s $ cache sets, $ E $ = cache lines, $ B = 2^b $ = bytes per block Address: set index = $ log_2S $ – Sets tag = $ m - ( s + b ) $ – Lines block offset = $ log_2B $ – blocks Process that cache determine hit or miss Set Selection Line matching: search each line, find tag bits in cache match tag bits in address Word Extraction: block offset provide first byte of desired word Line Replacement: random / least frequently used / least recently used Direct-Mapped Caches one line per set ( E = 1 ) Set Associative Caches a E-way set $ 1 &lt; E &lt; C/B $ Fully Associative Caches S = 1 $ E = C/B $ Issues with writes write-through ( immediately write to next lower level when hit ) no-write-allocate ( write directly to next lower level when miss ) write-back ( defers the update until it is evicted when hit ) write-allocate ( load the block into cache and update it when miss ) Anatomy of real cache hierarchy Instruction cache ( i-cache ) Data cache ( d-cache ) both ( unified cache ) 6.5 Writing Cache-Friendly Code 6.6 Putting It Together: The Impact of Caches on Program Performance smaller size – better temporal locality smaller stride – better spatial locality The memory mountain]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch4] Processor Architecture]]></title>
    <url>%2F2017%2F12%2F25%2FCSAPP%2Fch4%2F</url>
    <content type="text"><![CDATA[CS:APP-ch4 Processor Architecture ISA --&gt; SEQ --&gt; PIPE 4.1 The Y86-64 Instruction Set Architecture (ISA) Programmer-Visible State Program registers Condition codes Program Counter(PC) Memory Status code Y86-64 Instructions type: code function register specifier bytes: rA rB additional 8-byte constant word: immediate data, displacement, destination Aside RISC &amp; CISC Y86-64 include both CISC: condition codes, variable length instructions, stack to store return address RISC: use load/store architecture and regular instruction encoding, pass argument through registers 4.2 Logical Design and the Hardware Control Language HCL 4.3 Sequential Y86-64 Implementations Organizing processing into stages Fetch: icode, ifun, valC = 8-byte constant, valP = next PC Decode: register valA = rA valB = rB Execute: valE = valA OP valB, CC Memory: valM = read/write from memory Write back: two result to register file PC Update: PC = valP SEQ Hardware Structure SEQ Timing SEQ: combinational logic two memory devices clocked register ( PC, CC reg ) random access memory ( register file, instruction memory, data memory ) Combinational logic does not require sequencing or control Instruction memory read only therefore also not required Required explicit control over sequencing Program Counter: loaded with new instruction address every clock cycle Condition Code register: loaded when integer operation register file: two ports, allow two program registers be updated on every cycle data memory: written only when rmmovq, pushq, call is executed PRINCIPLE never need to read back the state updated by an instruction in order to complete this instruction states are loaded during the start of next cycle 4.4 General Principle of Pipelining Throughput: number of instructions served per unit time Latency: Total time required to perform a single instruction from beginning to end Limitations Nonuniform partitioning ( 每个part的执行时间不同造成delay ) Diminishing Returns of Deep Pipelining (分太多了) Feedback (下一条指令要等上一条执行完) 4.5 Pipelined Y86-64 Implementations SEQ+ PC update stage comes at the beginning PIPE- insert registers Rearranging and Relabeling Signals Next PC Prediction Pipeline Hazard stalling forwarding load/use data hazard control hazard exception]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CLRS-ch34] NP-Completeness]]></title>
    <url>%2F2017%2F12%2F24%2FCLRS%2F34-np%2F</url>
    <content type="text"><![CDATA[CLRS-ch34 NP-Completeness Intro class P solvable in polynomial time class NP problems are verifiable in polynomial time class NPC it is in NP and is as hard as any problem in NP decision problems reductions the first NP-complete problem 34.1 Polynomial time Abstract Problems a binary relation on a set I of instances and a set S of solutions Encodings In order to solve abstract problems, represent problem instances in a way program understands a mapping e from S to the set of binary strings Abstract Pro–&gt; Encode --&gt; Concrete Pro --&gt;Algorithm Solve polynomial related Polynomial time solvable exist an algorithm to solve a concrete problem in O(nk)O(n^k)O(n​k​​) extend the definition from concrete to abstract by using encoding as bridge A formal language framework alphabet $\Sigma $ a finite set of symbols { 0,1 } language L over Σ is any set of strings made up of symbols from Σ { 10, 11, 101 … } $ \Sigma ^* $ the language of all strings over Σ view instances for any decision problems Q as a language L over Σ = { 0 ,1 } Accepted The language L accepted by an algorithm A $ L = \{ x \in \{0,1\}^* : A(x) = 1 \} $ Reject : A(x) = 0 may runs forever if doesn’t accept Decided The language L decided by an algorithm A if every binary string in L is accepted by A (output 1) and every binary string not in L is rejected ( output 0) $ x \in L $ --&gt; A(x) = 1, $ x\notin L $ --&gt; A(x) = 0 Complexity class P $ P = \{ L \subseteq \{ 0,1 \}^{*} $ : exists an algorithm A that decides L in polynomial time $ \}$ 34.2 Polynomial-time verification algorithms verify membership from a certificate in languages Verified The language L verified by an algorithm A $ L = \{ x \in \{ 0, 1 \}^* : $ there exist $ y \in \{ 0,1 \}^* $ such that A( x,y ) = 1 $ \} $ Complexity class NP $ NP = \{ L \subseteq \{ 0, 1 \}^* : L = \{ x \in \{ 0,1\}^* $ : there exists a certificate y with |y| = $O(|x|^c) $ such that A(x,y) = 1 $ \} \} $ $P \subseteq NP $ Complexity class co-NP set of languages L such that $ \bar L \in NP $ Four possibilities for relationship among complexity class 34.3 NP-completeness and reducibility Reducibility if Q – &gt; reduce --&gt; Q’ , Q is “no harder to solve” than Q’ $ Q \le_p Q’ $ ( no more than a polynomial factor harder ) NP-completeness $ L \in NP $ $ L’ \le_p L $ for every $ L’ \in NP $. (hardest) Theorem 34.4 If any NP-complete problem is polynomial time solvable, then P = NP If any problem in NP is not polynomial-time solvable, then all NP-complete problems are not polynomial-time solvable. Circuit satisfiability Given a boolean combinational circuit composed of AND, OR, and NOT gates, is it satisfiable? 34.5 belongs to NP 34.6 NP-hard basic idea : represent the computation of A(algorithm verify L) as a sequence of configurations. Each configuration is mapped to the next configuration by a boolean combinational circuit M. The output is a distinguished bit in the working storage. The reduction algorithm F constructs a single combinational circuit that computes all configurations produced by a given initial configuration. 34.5 NP-complete problems]]></content>
      <categories>
        <category>CLRS</category>
      </categories>
      <tags>
        <tag>CLRS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题十二 基础DP]]></title>
    <url>%2F2017%2F12%2F20%2FACM%2Fkuangbin-12%2F</url>
    <content type="text"><![CDATA[kuangbin 专题十二 基础DP LIS dp[ i ]代表前i个数的最长子序列大小 dp[ i ] =max { dp[ j ] + 1 } ( val[ j ] &lt; val[ i ] ) $ O ( n^2 ) $ int dp[ N ], val[ N ]; int LIS ( int n ) { int mx = 0; for ( int i = 0; i &lt; n; ++i ) { dp[ i ] = 1; for ( int j = 0; j &lt; i; ++j ) { if ( val[ j ] &lt; val[ i ] &amp;&amp; dp[ i ] &lt; dp[ j ] + 1 ) dp[ i ] = dp[ j ] + 1; } if ( mx &lt; dp[ i ] ) mx = dp[ i ]; } return mx; } $ O ( nlgn ) $ http://www.geeksforgeeks.org/longest-monotonically-increasing-subsequence-size-n-log-n/ http://blog.csdn.net/shuangde800/article/details/7474903 int val[ N ];//输入的值 int len;//ed数组的长度（LIS长度） int ed[ N ];//每个长度的序列的末尾 //下界，返回 &gt;= 所查找对象的第一个位置 int binary_search ( int i ) { int left, right, mid; left = 0, right = len; while ( left &lt; right ) { mid = left + ( right - left ) / 2; if ( ed[ mid ] &gt;= val[ i ] ) right = mid; else left = mid + 1; } return left; } void LIS ( int n ) { ed[ 1 ] = val[ 1 ]; len = 1; for ( int i = 2; i &lt;= n; ++i ) { //更新最长的末尾 if ( val[ i ] &gt; ed[ len ] ) ed[ ++len ] = val[ i ]; //产生了新的序列，改变旧的长度的末尾 else { // 如果用STL： pos=lower_bound(ed,ed+len,val[i])-ed; int pos = binary_search ( i ); ed[ pos ] = val[ i ]; } printf ( "%d\n", len ); } } LCS dp[ i ][ j ] 代表两个字符串前i / j 个下的最长大小 dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1 ( val[ i ] == val[ j ] ) dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ) ( != ) //CLRS 15.4 int len1, len2; char s1[ N ], s2[ N ]; int dp[ N ][ N ]; inline int max ( int a, int b ) { return a &gt; b ? a : b; } void LCS () { for ( int i = 1; i &lt;= len1; i++ ) { for ( int j = 1; j &lt;= len2; j++ ) { if ( s1[ i ] == s2[ j ] ) dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1; else dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ); } } } void Print ( int i, int j ) { //当最长的子序列搜索完，但其中一串仍有剩余时，输出 if ( i == 0 || j == 0 ) { return; } //找到公共字符 if ( s1[ i ] == s2[ j ] ) { Print ( i - 1, j - 1 ); printf ( "%c", s1[ i ] ); } else if ( dp[ i - 1 ][ j ] &gt; dp[ i ][ j - 1 ] ) { Print ( i - 1, j ); } else { Print ( i, j - 1 ); } } 完全背包 dp[ i ]代表背包重i的时候最大的价值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ) int v[ N ], w[ N ]; // value，weight int dp[ N ]; //n个物品，最多装wei的东西 int knapsack ( int n, int wei ) { memset ( dp, INF, sizeof ( dp ) ); dp[ 0 ] = 0; for ( int i = 1; i &lt;= wei; ++i ) { for ( int j = 0; j &lt; n; ++j ) { if ( i &gt;= w[ j ] ) dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); } } return dp[ wei ]; } 题目列表 A - Max Sum Plus Plus HDU - 1024 n个数，要求分成m组，使m组的和加起来得到最大值。 dp[i][j]表示前j个数分成i组的最大值。 dp[i][j]=max(dp[i][j-1]+a[j],max(dp[i-1][k])+a[j]) B - Ignatius and the Princess IV HDU - 1029 给n(奇数)个数，定义特殊的数为在序列中出现次数不少于(n+1)/2次的数，找出这个特殊的数 一边输入一边记录个数就好了 C - Monkey and Banana HDU - 1069 给定箱子种类数量n，及对应长宽高，每个箱子数量无限，求其能叠起来的最大高度是多少(上面箱子的长宽严格小于下面箱子) 按照长排序，在求宽关于高的LIS D - Doing Homework HDU - 1074 有n门功课需要完成，每一门功课都有时间期限以及你完成所需要的时间，如果完成的时间超出时间期限多少单位，就会被减多少学分，问以怎样的功课完成顺序，会使减掉的学分最少，有多个解时，输出功课名排列最小的一个。 15个作业状态压缩来做 枚举每一个状态，枚举每个状态新增的那个节点，再计算最小并记录前一个状态 E - Super Jumping! Jumping! Jumping! HDU - 1087 从起点到达终点，只能前行不能后退，且下一步必须比前面的点的值大，求所有走的点的值总和最大是多少。 dp[i] = max(dp[k] + a[j]); 1&lt;=k&lt;=i-1; 最大递增子串和。 F - Piggy-Bank HDU - 1114 给出存钱罐本身的重量和装钱后的重量，以及存钱罐中钱的面值和重量，求存钱罐装满时，钱的总和最小是多少 完全背包解题，每种钱币都可以装无限个，注意初始化的值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); G - 免费馅饼 HDU - 1176 0—10的点，不同时间在每个点上掉下来物品，只能到达左右两边距离为1和本身所在的位置，求最大物品数 dp[x][t] = max ( dp[x-1][t-1],dp[x][t-1],dp[x+1][t-1]) + v[x][t] H - Tickets HDU - 1260 单张或两张一起买，给出一个一个买票和两个两个买票的时间，求最少 dp[ i ] = min ( dp[ i - 1 ] + s[ i ], dp[ i - 2 ] + d[ i - 1 ] ); I - 最少拦截系统 HDU - 1257 求有多少个递减序列 反过来，转换成求整个系列有多少个LIS，则是所求的组数 J - FatMouse’s Speed HDU - 1160 给n个老鼠的体重和速度，求找出一个最长的序列，此序列体重递增速度递减 按体重递增排序，再求最长递增(此递增表示体重递增速度递减)子序列。 dp[i] = max(dp[j]+1) 0&lt;=j&lt;=i-1 K - Jury Compromise POJ - 1015 必须满足辩方总分D和控方总分P的差的绝对值|D-P|最小。如果有多种选择方案的 |D-P| 值相同，那么选辩控双方总分之和D+P最大的方案即可。 dp(j, k)表示，取j 个候选人，使其辩控差为k 的所有方案中，辩控和最大的那个方案的辩控和。 综上：dp[j][k]=dp[j-1][k-V[i]]+S[i] 正向计算，输出的时候就用正向的输出了,不过每次都要查找下一个位置是否在之前用过了 L - Common Subsequence POJ - 1458 LCS LCS模板 M - Help Jimmy POJ - 1661 老鼠在时刻0从高于所有平台的某处开始下落.当Jimmy落到某个平台上时，游戏者选择让它向左还是向右跑.当Jimmy跑到平台的边缘时，开始继续下落。Jimmy每次下落的高度不能超过MAX dp[i][0] = min(dp[k][0]+l[i]-l[k], dp[k][1]+r[i]-l[k]) + h[i]-h[k]; (左左和左右取最小) dp[i][1] = min(dp[k][0]+r[i]-l[k], dp[k][1]+r[i]-r[k]) + h[i]-h[k];(右左和右右取最小) N - Longest Ordered Subsequence POJ - 2533 LIS LIS模板 O - Treats for the Cows POJ - 3186 n个数在一个双端队列中，每次从队首或队尾出。出的第n个数乘以n，最后加起来，求最大和。 dp[i][j] 代表从i取到j的最大总数 dp[i][j] = max(dp[i+1][j]+a[i] * (n+i-j) , dp[i][j-1]+a[j] * (n+i-j)); P - FatMouse and Cheese HDU - 1078 给定一幅图，每个点有一定权值，现在有一只老鼠在起始点（0,0），他能水平或者垂直移动1~k格之后，停在某点并获得权值，而且每次移动后所在的点，都要比刚离开的那个点的权值更大，求最多能获得多少权值。 DP / Memoized dp[ x ][ y ] = dp[ xx ][ yy ] + val[ x ][ y ] Q - Phalanx HDU - 2859 给了一个字符串矩阵，求以次对角线方向对称的最大对称矩阵。 每次只需求最外面一层对称个数sum，再和右上角对称矩阵大小加一取最小就行，就求出当前小矩阵的最大对称矩阵。最后取个所有对称矩阵大小的最大值就行。 dp[i][j] = min(sum,dp[i-1][j+1]+1); R - Milking Time POJ - 3616 奶牛Bessie在0~N时间段产奶。农夫约翰有M个时间段可以挤奶，时间段f,t内Bessie能挤到的牛奶量e。奶牛产奶后需要休息R小时才能继续下一次产奶，求Bessie最大的挤奶量。 dp[ i ] = max ( dp[ j ] + node[ i ].val, dp[ i ] ) ( node[ j ].ed &lt;= node[ i ].st ) S - Making the Grade POJ - 3666 农夫约翰想修一条尽量平缓的路，路的每一段海拔是A_i，修理后是B_i，花费|A_i – B_i|，求最小花费。 dp[i][j] = min(dp[i – 1][k]) + |A[i] – B[j]| 离散化]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[GSLA ch-6] Eigenvalues and Eigenvectors]]></title>
    <url>%2F2017%2F12%2F14%2FGSLA%2FEigenvalues%20and%20Eigenvectors%2F</url>
    <content type="text"><![CDATA[Eigenvalues and Eigenvectors 6.1 Introduction certain vectors x are in the same direction as Ax basic equation: ​ $ Ax = \lambda x $ how to compute: let $ det(A-\lambda x)= 0 $ ,find the roots == find eigenvalues, find eigenvectors in Null space $ A^nx = \lambda^n x $ span eigenspace Projection: $ \lambda = 1 $ or 0 Reflection: $ \lambda = 1 $ or -1 Rotation: complex eigenvalues only product of eigenvalues == determinant == product of pivots sum of eigenvalues == sum of diagonal entries ( not pivots ) == trace 6.2 Diagonalizing eigenvectors in columns of S, eigenvalues in diagonal of Λ $ Λ = S^{-1}AS $ $ A = SΛS^{-1} $ Independent x from different λ: $ c_1 λ_1 x_1 + c_2 λ_2 x_2 = 0 $ $ c_1 λ_2 x_2 + c_2 λ_2 x_2 = 0 $ subtract $ (λ_1 - λ_2)c_1x_1 = 0 $ diagonalizability: enough eigenvectors (maybe same λ) so that S is invertible uk=Aku0=SΛkS−1u0u_k = A^k u_0 = S \Lambda^k S^{-1}u_0u​k​​=A​k​​u​0​​=SΛ​k​​S​−1​​u​0​​ $ u_0 = c_1x_1 + c_2x_2 + … +c_nx_n $ eigenvector basis multiply λik\lambda_i ^ kλ​i​k​​ add up A,B share same eigenvector matrix S if and only if AB = BA ?Heisenberg uncertainty principle position matrix P, momentum matrix Q, $ QP-PQ = I $ knew P still could not know Q $ |Px||Qx| \ge \frac{1}{2}|x|^2 $ 6.3 Applications to Differential Equations 6.4 Symmetric Matrices real eigenvalues orthonormal eigenvectors Spectral Theorem (principle of axis theorem) $ A = Q\Lambda Q^T $ Normal Matrices $ \bar A^TA = A \bar A^T $ symmetric, skewed-symmetric, orthogonal A has n orthonormal vectors ($ A = Q\Lambda \bar Q ^T $) if and only if A is normal Real Eigenvalues proof: $ Ax = \lambda x $ $ \bar x ^TA = \bar x ^T \bar\lambda $ ( $ A = A^T $ )( conjugate and transpose ) $ \bar x^T A x = \bar x^T \lambda x $ $ \bar x ^ T A x = \bar x ^ T \bar\lambda x $ left side the same therefor $ \lambda == \bar \lambda $ Orthonormal proof: no eigenvalues repeated Allow repeated eigenvalues ( ? Schur’s Theorem ) sum of rank one projection matrices $ A = \lambda_1x_1x_1^T + \lambda_2x_2x_2^T+… $ $ = \lambda_1P_1 +\lambda_2P2+… $ number of positive pivots == number of positive eigenvalues $ A = LDL^T $ 6.5 Positive Definite Matrices All λ &gt; 0 quick way to test All pivots positive n eigenvalues positive $ x^TAx $ is positive except x = 0 $ A == R^TR $ (symmetric) and R has independent columns ($ x^T R^TRX &gt;= 0 $) n upper left determinants R can be chosen: rectangular / $ (L\sqrt D)^T $ / $ Q \sqrt\Lambda Q^T $ $x^TAx $ (2*2) = $ ax^2 + 2bxy + cy^2 &gt; 0 $ ( ellipse $ z = x2/a2 + y2/b2 $ ) Application tilted ellipse $ x^TAx $ lined - up ellipse $ X^T\Lambda X = 1 $ rotation matrix Q axes: eigenvectors half-length: $ 1/\sqrt\lambda $ 6.6 Similar Matrices DEF: A similar to B (A family) $ B = M^{-1}AM $ Property: A and B have same eigenvalues x a eigenvector of A and $ M^{-1}x $ eigenvector of B Jordan Form triple eigenvalues while one eigenvector J with λ in the diagonal and 1 above similar to every matrices with repeated eigenvalues λ and one eigenvector λ repeated only once than J == Λ Jordan Block make A as simple as possible while preserving essential properties 6.7 Singular Value Decomposition SUMMARY $ A = U \Sigma V^T $ $ \Sigma^2 = \Lambda $ (of $ A^TA $ and $ AA^T $ ) $ U = Q $ (of $ AA^T $ in $ R^m $) $ V = Q $ (of $ A^TA $ in $ R^n $) orthonormal basis of row space {$ v_1, v_2, … v_r $} orthonormal basis of null space{ $ v_{r+1}, v_{r+2},…v_n $} orthonormal basis of column space{ $ u_1, u_2,…u_r $} orthonormal basis of left null space { $ u_{r+1}, u_{r+2}, … u_m $} Rotation – Stretch – Rotation $ Av_1 = \sigma_1u_1 $ … so $ AV = U\Sigma $ (m * n) * (n * n) = (m * m)* (m * n) $ V $ and $ U $ are orthogonal matrices $ \Sigma $ = ( old r*r $ \Sigma $ ) + (m-r zero rows) + ( n-r zero columns ) therefore … when A positive definite symmetric $ A = U \Sigma V^T = Q\Lambda Q^{T} $ 7.3 Diagonalization and the Pseudoinverse change bases $ \Lambda { w – w } = S^{-1}{std – w} A_{std} S_{ w – std } $ $ \Sigma { v – u } = U^{-1}{std – u} A{std}V{v – std} $ Polar Decomposition orthogonal and semidefinite rotation and stretching $ A = U\Sigma V^T = ( UV^T ) (V \Sigma V^T) = QH $ Pseudoinverse $ A^+ = V\Sigma^+ U^T $]]></content>
      <tags>
        <tag>Linear Algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题四 最短路]]></title>
    <url>%2F2017%2F12%2F10%2FACM%2Fkuangbin-4%2F</url>
    <content type="text"><![CDATA[kuangbin专题四 最短路 ACM图论存图方式 http://jzqt.github.io/2015/07/21/ACM图论之存图方式/ 单源最短路 Dijkstra ​ $ O(V^2 + E) $ int edg[ N ][ N ]; // weight of each edge int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited // CLRS 24.3 // s:start point, n: total number of nodes void dijkstra ( int s, int n ) { // INITIALIZE-SINGLE-SOURCE memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = edg[ s ][ i ]; dis[ s ] = 0; // Extract Min // u for the node, mi for dis[ u ] for ( int i = 1; i &lt;= n - 1; ++i ) { int u = -1, mi = INF; for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] &amp;&amp; dis[ j ] &lt; mi ) mi = dis[ u = j ]; vis[ u ] = true; // Relax edges adjacent to u for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] ) dis[ j ] = min ( dis[ j ], dis[ u ] + edg[ u ][ j ] ); } } Dijkstra + STL priority_queue ​ $ O((V+E)lgV) $ int head[ N ]; //链式前向星建图 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct Node { int u, d; // id, dis bool operator&lt; ( const Node &amp;rhs ) const { return d &gt; rhs.d; } }; struct Edge { int u, v, w, nex; } edg[ N ]; // CLRS 24.3 // s:start point void dijkstra ( int s ) { priority_queue&lt;Node&gt; Q; dis[ s ] = 0; Q.push ( ( Node ){s, dis[ s ]} ); while ( !Q.empty () ) { // u = Extract_Min( Q ) Node x = Q.top (); Q.pop (); int u = x.u; if ( vis[ u ] ) continue; vis[ u ] = true; // for each vertex v in G.adj[ u ] for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) { int v = edg[ i ].v; int w = edg[ i ].w; // relax if ( dis[ v ] &gt; dis[ u ] + w ) { dis[ v ] = dis[ u ] + w; Q.push ( ( Node ){v, dis[ v ]} ); } } } } SPFA (Shortest Path Faster Algorithm) ​ $ O(kE) $ ( k &lt; 2 ) int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited int cnt[ N ]; //入列次数超过n，有负环 int head[ N ]; //链式前向星 struct edg { int u, v, w, nex; } edg[ N ]; bool SPFA ( int s, int n ) { // INIT memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = INF; dis[ s ] = 0; memset ( cnt, 0, sizeof ( cnt ) ); cnt[ s ] = 1; // BFS方式的spfa queue&lt;int&gt; Q; Q.push ( s ); while ( !Q.empty () ) { int u = Q.front (); Q.pop (); vis[ u ] = false; // 出队要取消标记 for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) { int v = edg[ i ].v; // Relax所有出边 if ( dis[ v ] &gt; dis[ u ] + edg[ i ].w ) { dis[ v ] = dis[ u ] + edg[ i ].w; //没入队的标记并入队 if ( !vis[ v ] ) { vis[ v ] = true; Q.push ( v ); // 判断负环 if ( ++cnt[ v ] &gt; n ) return false; } } } } return true; } 贴一个dfs的，自己没写过，感觉dijkstra+heap最好 int spfa_dfs ( int u ) { vis[ u ] = true; for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) { int v = edg[ i ].v, w = edg[ i ].w; if ( dis[ u ] + w &lt; dis[ v ] ) { dis[ v ] = dis[ u ] + w; if ( !vis[ v ] ) { if ( spfa_dfs ( v ) ) return 1; } else return 1; } } vis[ u ] = false; return 0; } Bellman-Ford ​ $ O(VE) $ int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct edg { int u, v, w; } edg[ N ]; //CLRS 24.1 //对每个点，relax所有的边 bool Bellman_Ford ( int n, int e, int s, double num ) { // Initialize Single Sorce( G,s ) for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = 0; dis[ s ] = num; // for i = 1 to |G.V|-1 for ( int i = 0; i &lt; n - 1; ++i ) { // for each edg ( u,v ) in G.E for ( int j = 0; j &lt; e; ++j ) { int u = edg[ j ].u; int v = edg[ j ].v; // Relax if ( dis[ v ] &gt; dis[ u ] + edg[ j ].w ) { dis[ v ] = dis[ u ] + edg[ u ].w; } } } // 存在负环 for ( int i = 0; i &lt; e; ++i ) if ( dis[ edg[ i ].v ] &gt; ( dis[ edg[ i ].u ] - edg[ i ].w ) ) return false; return true; } 适用场景 如果是稠密图，Dijkstra+heap比SPFA快。稀疏图则SPFA更快。再就是SPFA可以判断负环 对于极端的链状图，SPFA无疑是最合适的了。每个结只进队一次，标准的O(E)。 朴素的dijkstra对于这样的图就力不从心了：每次循环都过一遍结点，在松弛，然后发现每次O(V)的时间只松弛了一个点。 ​ 多源最短路 Floyd void init () { for ( int i = 0; i &lt;= n; ++i ) for ( int j = 0; j &lt;= n; ++j ) { //有向图 w[ i ][ j ] = INF; //记录路径段内的后一点 //也可以改成前一点path[i][j] = i,就要反向输出; path[ i ][ j ] = j; } } // CLRS 25.2 All-Pairs-Shortest-Path void Floyd () { // Dp // 表示所有点只经过集合{ 1..k }内的点时的最短路径 // 只经过1的时候relax一次，在添加上经过2后的relax，最后一直到n for ( int k = 1; k &lt;= n; ++k ) //选取每一个起点 for ( int i = 1; i &lt;= n; ++i ) //选取每一个终点 for ( int j = 1; j &lt;= n; ++j ) // i -- j的路径存在 if ( w[ i ][ k ] != INF &amp;&amp; w[ k ][ j ] != INF ) { int tmp = w[ i ][ k ] + w[ k ][ j ]; // d(ij)^k = min { d(ik)^(k-1) + d(kj)^(k-1) } if ( w[ i ][ j ] &gt; tmp ) { w[ i ][ j ] = tmp; //记录后继路径 //前驱path[ i ][ j ] = path[ k ][ j ]; path[ i ][ j ] = path[ i ][ k ]; } } } 题目列表 A - Til the Cows Come Home POJ-2387 一个农场有n (1000)个点，有t (2000)条道路连接, 从n到1最短 dijkstra 模板 B - Frogger POJ-2253 无向图一条1~2的路径使得该路径上的最大边权最小. (max Route weight is the minimum among all routes) dijkstra变形 double minimax = max ( mi, v[ j ][ u ] ); C - Heavy Transportation POJ - 1797 ​ 有n个城市，n个城市之间有m条公路或桥梁，每个公路或桥都有一个最大载重量，问从城市1到城市n所能运送到货物到最大重量是多少 ( min Route weight is the maximum among all routes ) dijkstra变形 int maxmini = min ( mi, v[ j ][ u ] ); D - Silver Cow Party POJ - 3268 n个农场，m条单向路，n个牛分别在n个农场，第x农场为终点，问每个牛从所在农场前往x农场的往返路程最小值是多少，求出n个牛中最短路上往返路程的最大的那个 从n个点到1再从1回到n个点，通过调转边的方向两次dijkstra E - Currency Exchange POJ - 1860 有n种货币，你含有num面额的其中一种货币。求有没有可能，在多次兑换后你手中的货币大于num。 求最大路径，反向用Bellman-Ford F - Wormholes POJ - 3259 农场之间有很多条路，以及单向的虫洞，每条路走完会花费一定的时间，而虫洞可以回到之前的时间，问农场主是否能回到自己出发时间前的出发点 SPFA判断负环 G - MPI Maelstrom POJ - 1502 从第一个点出发，求到其他点最短路的最大值 dijkstra 注意下三角矩阵邻接表的建图 H - Cow Contest POJ - 3660 n个牛进行比赛，现已知m个关系， 牛u可以胜过牛v。问最后可以确定排名位数的有几个牛. Floyd判断两两牛之间的关系。如果一个牛可以胜过a个牛，b个牛可以胜过它，那么如果a＋b＝n－1，他的排名就可以确定 I - Arbitrage POJ - 2240 给定多种货币之间的兑换关系，问是否可以套利 Bellman-Ford判断正环（要返回自己所以松弛n次) floyd判断回来后是否&gt;1 J - Invitation Cards POJ - 1511 求源点到各点的往返最短路之和 邻接表逆置（建了两个邻接表） 数据多需要优化SPFA/dijkstra+heap K - Candies POJ - 3159 给n个人分糖果，m组数据a，b，c；意思是a比b少的糖果个数绝对不超过c个，也就是d(b)-d(a) &lt; c，求1比n少的糖果数的最大值。 差分约束，和最短路的松弛一样 数据多，dijkstra+heap L - Subway POJ - 2502 小明步行的速度是10km/h，地铁速度是40km/h，给定家和学校的坐标，再给定多条地铁线路站点的坐标，问小明从家到学校所需的最短时间 dijkstra，建图连接所有的点并赋值时间 M - 昂贵的聘礼 POJ - 1062 每个人可能有直接购买或者交换物品换取折扣这两种方式交易（交换物品要从别人手里买） 等级差之间超过m的不能交易 求用最少的钱买到非洲大酋长的承诺 等级限制采用枚举的方法，分别从lv[ 1 ] - m ~~ lv[ 1 ] + m，每次枚举的区间长度为m,一共m次最短路搜索 N - Tram POJ - 1847 电动巴士在每个十字路口有一个默认方向，走向别的方向需要改动扳手。 dijkstra 每个边初始化为INF,要切换的路 = 1,不用切换 = 0 O - Extended Traffic LightOJ - 1074 给定每条街的拥挤度p(x)，街a到街b的时间就是(p(b)-p(a))**3，求第一个点到第k个点的最短路 SPFA判断负环 dfs记录负环里的点 P - The Shortest Path in Nya Graph HDU - 4725 共n个点，n层(每个点单独一层)，相邻的两层之间权值为w 还有m条额外的边，权值为v，求1到n的最短路 建图 ！！给每个点两个辅助点，一个做出度，一个做入度 Q - Marriage Match IV HDU - 3416 网络流，不会qwq R - 0 or 1 HDU - 4370 X12+X13+…X1n=1,X1n+X2n+…Xn-1n=1,其余行列和相同，求ΣCij*Xij 神一样的建图！！节点1的出度为1.节点n的入度为1.节点2-n-1的入度和出度相等. 问题就相当于求一条最短路，从节点1出发，到节点N. 同时节点1的一个最小环+节点n的一个最小环也是可行解，两者取最小 S - Layout POJ - 3169 两点间可能&lt;= x 或者&gt;=x，求1–&gt;n最大 差分约束，和最短路一样，SPFA判断负环则代表不存在可行解]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
</search>
