<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[CS:APP-ch1] A Tour of Computer Systems]]></title>
    <url>%2F2018%2F01%2F01%2FCSAPP%2Fch-1%2F</url>
    <content type="text"><![CDATA[CS:APP ch-1 A Tour of Computer Sytems A computer system consists of hardware and systems software that work together to run application programs. Tracing the life time of hello program. 1 2 3 4 5 6 7 8 #include &lt;stdio.h&gt; int main() &#123; printf("hello world!\n"); return 0; &#125; 1.1 Information Is Bits + Context The hello program begins life as a source program ( a sequence of bits, organized in bytes ) Text files files consist exclusively ASCII characters. Binary files all other files All information in a system is represented as a bunch of bits. The only thing that distinguishes different data objects is the context in which we view them. 1.2 Programs Are Translated by Other Programs into Different Forms In order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk file. 1.3 It Pays to Understand How Compilation Systems Work Reasons why need to understand compilation systems Optimizing program performance Understanding link-time error Avoiding security holes 1.4 Processors Read and Interpret Instructions Stored in Memory To run the executable object file, type ./hello to an applcation program known as a shell Hardware Organization of a System Buses: transfer words I/O Devices: system’s connection to the external world, connected to I/O bus by either a controller or an adapter Main Memory: temporary storage device that holds both a program and the data it manipulates while the processor is executing the program Processor: the engine that inter-prets (or executes ) instructions stored in main memory Running hello Reading command from keyboard Load file from disk to memory Write output string from memory to display 1.5 Caches Matter The machine instruction in the hello program are originally stored on disk When loaded, opied to main memory When processor runs, copied into the processor ( register + caches ) deal with processor-memory gap 1.6 Storage Devices Form a Hierarchy storage at one level serves as a cache for the next lower level 1.7 The Operating System Manages the Hardware OS a layer of software interposed between the application program and the hardware two primitive purpose: (1)protect hardware (2)provide applications with simple and uniform mechanisms for manipulating hardwares achieve both goals via abstractions Process The operating system’s abstraction for a running program context switching Thread VIrtual Memory Each process has the same uniform view of memory ( virtual address space ) Files A sequence of bytes, nothing more and nothing less Every I/O devices is modeled as a file 1.8 Systems Communicate with Other Systems Using Networks 1.9 Important Themes Amdahl’s law: observation of the effectiveness Concurrency and Parallelism concurrency a system with multiple, simultaneous activities parallelism use of concurrency to make a system run faster Parrallelism can be exploited at multiple( three ) levels of abstraction in a computer system. ( hight --&gt; low ) Thread-level Concurrency: multicore &amp; hyperthreading Instruction-level Parallelism: pipeling Single-Instruction, Multiple-Data ( SIMD ) Parallelism The Importance of Abstractions in Computer Systems Processor side instruction set architecture provides an abstraction of actual processor Operating System side Files as an abstraction of I/O Virtual Memory as an abstraction of Program Memory Processes as an abstraction of a Running Program Virtual Machine as an abstraction of the Entire Computer]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch9] Virtual Memory]]></title>
    <url>%2F2017%2F12%2F29%2FCSAPP%2Fch-9%2F</url>
    <content type="text"><![CDATA[CS:APP ch-9 Virtual Memory Virtual Memory modern systems provide an abstraction of main memory three important capabilities It uses main memory efficiently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory, and transferring data back and forth between disk and memory as needed. It simplifies memory management by providing each process with a uniform address space. It protects the address space of each process from corruption by other processes. 9.1 Physical and Virtual Addressing Main memory organized as an array of M contiguous byte-size cells, each byte has a unique physical address. Physical Addressing: use physical address ( PA ) to access memory Virtual Addressing: use virtual address ( VA ) and translate to physical address Addressing Translation: converting a virtual address to a physical one 9.2 Address Spaces Virtual Address Space CPU generates virtual addresses from an address space of $ N = 2^n $ addresses called the virtual address space {0 , 1, 2 ,…,N − 1} Physical Address Space A system also has a physical address space that corresponds to the M bytes of physical memory in the system {0 , 1, 2 ,…,M − 1} 9.3 VM as a Tool for Caching A virtual memory is organized as an array of N contiguous byte-sized cells stored on disk. Each byte has a unique virtual address that serves as an index into the array. VM systems partition the virtual memory into fixed-size blocks call Virtual Pages ( VP ) as transfer units between disk and main memory. Three sets of VP: Unallocated, Cached, Uncached DRAM cache organization large virtual pages ( cache block )( 4 KB to 2 MB ) fully associative sophisticated replacement algorithms use write-back Page Tables A data structure stored in physical memory that maps virtual pages to physical pages. Page Hit Check valid bit, uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word. Page Faults The page fault exception invokes a page fault exception handler in the kernel selects a victim page, in this case VP 4 stored in PP 3. modifies the page table entry for VP 4 to reflect the fact that VP 4 is no longer cached in main memory. copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns. restarts the faulting instruction and page hit The strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging 9.4 VM as a Tool for Memory Management Operating systems provide a separate page table, and thus a separate virtual address space, for each process. Linking: Allow each process to use the same basic format for its memory image ( code segment always start at 0x400000 ) Loading: (1) Allocate Virtual Pages (2) Mark as invalid ( not cached ) (3) Point table entries to the location of in object file (4) Data are paged in th first time referenced Sharing: Multiple processes share a single copy of some code Memory Allocation ​ ​ 9.5 VM as a Tool for Memory Protection three permission bits to each PTE SUP must run in kernel mode READ / WRITE 9.6 Address Translation a mapping between an N-element virtual address space and M-element physical address space Steps that the CPU hardware performs when there is a page hit / fault Speeding up Address Translation with a TLB a small cache of PTEs in the MMU called a translation look aside buffer (TLB) Multi-Level Page Tables つ…つづく！]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch8] Exceptional Control Flow]]></title>
    <url>%2F2017%2F12%2F28%2FCSAPP%2Fch8%2F</url>
    <content type="text"><![CDATA[CS:APP ch-8 Exceptional Control Flow control flow sequence of control transfers ( from the address of instruction to the next ) Exceptional Control Flow ( ECF ) Abrupt changes in control flow Occurs all levels Hardware: Transfers to exceptional handlers Operating System: Context switch Application: Signal Individual Program: Nonlocal jump 8.1 Exceptions DEF Abrupt change in control flow in response to some change in the processor’s state event change in state Exception Handling Hardware processor detect an event determine corresponding exception number k trigger exception by calling the handler through entry k of the exception table Software ( handler ) do some processing return control to the interrupted control flow Difference with procedure call return either current or next instruction push additional processor state to restart ( on kernel stack ) handlers run in kernel mode Classes of Exceptions Interrupt: I/O devices Trap: System call ( fork, read, execve, exit ) Fault: error conditions that might be able to correct or just abort ( page fault, segmentation fault, divide zero ) Abort: unrecoverable ( machine check ) 8.2 Processes DEF: An instance of program in execution program run in context of process Logical Control Flow provide an illusion that program has exclusive use of the processor The sequence of PC values is know as logical control flow Each process executes a portion of its flow and then is preempted (temporarily suspended) while other processes take their turns. Concurrent Flow A logic flow whose execution overlaps in time with another flow Concurrency The general phenomenon of multiple flows executing concurrently Parallel Flow subset of concurrent flow that run on different cores Private Address Space provide an illusion that program has exclusive use of the system address’s space User and kernel mode: mode bit context the state that kernel needs to restart the preempted process ( general-purpose registers, floating point registers, program counter, user’s stack, status register, kernel’s stack, page table, process table, file table ) scheduling the kernel decide to preempt the current process and restart a previously preempted process Context Switch Saves context of the current process Restores the saved context of previous process Passes control to newly restored process Example when execute read system call ( disk ) Kernel switch process until disk sends an interrupt signal 8.4 Process Control Obtaining Process ID 1 pid_t getpid ( void ); Create and Terminating process Three states Running Stopped Terminated 1 void exit ( int status ); 1 2 //return 0 to child, PID of chile to parent pid_t fork ( void ); Call Once Return Twice Concurrent execution Duplicate but separate address space Shared files Reaping Child Process The process kept around in a terminated state until it is reaped by its parent 父进程进行回收的函数，也是一个等待子进程执行结束的函数就是waitpid。这在 APUE（advanced programming in unix enviroment）中很早就提过这个函数 1 2 // return PID of terminated child and reap it pid_t waitpid(pid_t pid, int *statud, int options) Putting Process to Sleep 1 2 unsigned int sleep(unsigned int secs); int pause(void); // 一直休眠，直到收到一个信号 Loading and Running Programs 1 int execve(const char *filename, const char *argv[], const char *envp[]); 8.5 Signals The transfer of a signal to a destination process occurs in two distinct steps Sending a signal The kernel has detected a system event such as a divide-by-zero error or the termination of a child process. A process has invoked the kill function to explicitly request the kernel to send a signal to the destination process. A process can send a signal to itself. Receiving a signal The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler. Pending signal A signal that has been sent but not yet received At any point in time, there can be at most one pending signal of a particular type. Received at most once: The kernel sets bit k in pending whenever a signal of type k is delivered and clears bit k in pending whenever a signal of type k is received. 8.6 Nonlocal Jumps]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch6] The Memory Hierarchy]]></title>
    <url>%2F2017%2F12%2F27%2FCSAPP%2Fch-6%2F</url>
    <content type="text"><![CDATA[CS:APP ch-6 The Memory Hierarchy 6.1 Storage Technologies SRAM cache, transistor, stable, fast, expensive DRAM main memory, capacitor, sensitive( periodically refresh every bit ) d * w cells --&gt; d supercells == r rows * c columns ( two dimension array ) data pins ( w bits ) &amp; addr pins ( row addr i, column addr j ) Memory modules: Each supercell stores 11 byte. Each address represent by 8 supercells Enhanced DRAM Nonvolatile Memory ( ROM ) retain information even when they’re powered off Flash memory ( erasable programmable ROM ) firmware: programs stored in ROM Accessing Main Memory I/O bridge: translate signals Disk platter --&gt; surface --&gt; track --&gt; sectors Logical Disk Blocks: hide complexity from OS disk controller: maintains mapping between logical block number and physical disk sectors I/O devices: USB, graphics card, host bus adapter I/O bus: connect disks to CPU and main memory, independent under CPU Accessing Disk Memory mapped I/O: a block of addresses in the address space is reserved for communicating with I/O devices. Each of these address is known as an I/O port. Each device associated with one or more ports. CPU read disk initiate a read indicate logical block number that should be read indicate main memory address to store disk sector do other work Disk controller receives the read command translates logical block number to sector address read content transfer content directly to main memory ( DMA ) send a interrupt signal to CPU CPU returns control to the point where interrupted Solid State Disk A page can be written only after the entire block it belongs to has erased A block wears out after roughly 100, 000 repeated writes Advantage: semiconductor, faster, use less power, more rugged Disadvantage: potential to wear out, expensive 6.2 Locality Temporal Locality: reference again multiple times Spatial Locality: reference nearby locations stride-k reference pattern: visiting every kth element of a contiguous vector k increase, spatial decrease 6.3 The Memory Hierarchy Caching in memory hierarchy Cache hits Cache misses replacing / evicting the block, victim block, replacement policy Kind of Cache misses cold miss: cache is empty conflict miss: map to the same block capacity miss: size of working set exceed Cache management partition storage into blocks, transfer blocks between levels, deal with hits and misses compiler – register file hardware logic – L1, L2, L3 caches OS + Address Translation hardware – main memory AFS client process – disk 6.4 Cache Memories Main memory: $ M = 2^m $ cache: $ C = S * E * B $ $ S = 2^s $ cache sets, $ E $ = cache lines, $ B = 2^b $ = bytes per block Address: set index = $ log_2S $ – Sets tag = $ m - ( s + b ) $ – Lines block offset = $ log_2B $ – blocks Process that cache determine hit or miss Set Selection Line matching: search each line, find tag bits in cache match tag bits in address Word Extraction: block offset provide first byte of desired word Line Replacement: random / least frequently used / least recently used Direct-Mapped Caches one line per set ( E = 1 ) Set Associative Caches a E-way set $ 1 &lt; E &lt; C/B $ Fully Associative Caches S = 1 $ E = C/B $ Issues with writes write-through ( immediately write to next lower level when hit ) no-write-allocate ( write directly to next lower level when miss ) write-back ( defers the update until it is evicted when hit ) write-allocate ( load the block into cache and update it when miss ) Anatomy of real cache hierarchy Instruction cache ( i-cache ) Data cache ( d-cache ) both ( unified cache ) 6.5 Writing Cache-Friendly Code 6.6 Putting It Together: The Impact of Caches on Program Performance smaller size – better temporal locality smaller stride – better spatial locality The memory mountain]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch4] Processor Architecture]]></title>
    <url>%2F2017%2F12%2F25%2FCSAPP%2Fch4%2F</url>
    <content type="text"><![CDATA[CS:APP-ch4 Processor Architecture ISA --&gt; SEQ --&gt; PIPE 4.1 The Y86-64 Instruction Set Architecture (ISA) Programmer-Visible State Program registers Condition codes Program Counter(PC) Memory Status code Y86-64 Instructions type: code function register specifier bytes: rA rB additional 8-byte constant word: immediate data, displacement, destination Aside RISC &amp; CISC Y86-64 include both CISC: condition codes, variable length instructions, stack to store return address RISC: use load/store architecture and regular instruction encoding, pass argument through registers 4.2 Logical Design and the Hardware Control Language HCL 4.3 Sequential Y86-64 Implementations Organizing processing into stages Fetch: icode, ifun, valC = 8-byte constant, valP = next PC Decode: register valA = rA valB = rB Execute: valE = valA OP valB, CC Memory: valM = read/write from memory Write back: two result to register file PC Update: PC = valP SEQ Hardware Structure SEQ Timing SEQ: combinational logic two memory devices clocked register ( PC, CC reg ) random access memory ( register file, instruction memory, data memory ) Combinational logic does not require sequencing or control Instruction memory read only therefore also not required Required explicit control over sequencing Program Counter: loaded with new instruction address every clock cycle Condition Code register: loaded when integer operation register file: two ports, allow two program registers be updated on every cycle data memory: written only when rmmovq, pushq, call is executed PRINCIPLE never need to read back the state updated by an instruction in order to complete this instruction states are loaded during the start of next cycle 4.4 General Principle of Pipelining Throughput: number of instructions served per unit time Latency: Total time required to perform a single instruction from beginning to end Limitations Nonuniform partitioning ( 每个part的执行时间不同造成delay ) Diminishing Returns of Deep Pipelining (分太多了) Feedback (下一条指令要等上一条执行完) 4.5 Pipelined Y86-64 Implementations SEQ+ PC update stage comes at the beginning PIPE- insert registers Rearranging and Relabeling Signals Next PC Prediction Pipeline Hazard stalling forwarding load/use data hazard control hazard exception]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CLRS-ch34] NP-Completeness]]></title>
    <url>%2F2017%2F12%2F24%2FCLRS%2F34-np%2F</url>
    <content type="text"><![CDATA[CLRS-ch34 NP-Completeness Intro class P solvable in polynomial time class NP problems are verifiable in polynomial time class NPC it is in NP and is as hard as any problem in NP decision problems reductions the first NP-complete problem 34.1 Polynomial time Abstract Problems a binary relation on a set I of instances and a set S of solutions Encodings In order to solve abstract problems, represent problem instances in a way program understands a mapping e from S to the set of binary strings Abstract Pro–&gt; Encode --&gt; Concrete Pro --&gt;Algorithm Solve polynomial related Polynomial time solvable exist an algorithm to solve a concrete problem in O(nk)O(n^k)O(n​k​​) extend the definition from concrete to abstract by using encoding as bridge A formal language framework alphabet $\Sigma $ a finite set of symbols { 0,1 } language L over Σ is any set of strings made up of symbols from Σ { 10, 11, 101 … } $ \Sigma ^* $ the language of all strings over Σ view instances for any decision problems Q as a language L over Σ = { 0 ,1 } Accepted The language L accepted by an algorithm A $ L = \{ x \in \{0,1\}^* : A(x) = 1 \} $ Reject : A(x) = 0 may runs forever if doesn’t accept Decided The language L decided by an algorithm A if every binary string in L is accepted by A (output 1) and every binary string not in L is rejected ( output 0) $ x \in L $ --&gt; A(x) = 1, $ x\notin L $ --&gt; A(x) = 0 Complexity class P $ P = \{ L \subseteq \{ 0,1 \}^{*} $ : exists an algorithm A that decides L in polynomial time $ \}$ 34.2 Polynomial-time verification algorithms verify membership from a certificate in languages Verified The language L verified by an algorithm A $ L = \{ x \in \{ 0, 1 \}^* : $ there exist $ y \in \{ 0,1 \}^* $ such that A( x,y ) = 1 $ \} $ Complexity class NP $ NP = \{ L \subseteq \{ 0, 1 \}^* : L = \{ x \in \{ 0,1\}^* $ : there exists a certificate y with |y| = $O(|x|^c) $ such that A(x,y) = 1 $ \} \} $ $P \subseteq NP $ Complexity class co-NP set of languages L such that $ \bar L \in NP $ Four possibilities for relationship among complexity class 34.3 NP-completeness and reducibility Reducibility if Q – &gt; reduce --&gt; Q’ , Q is “no harder to solve” than Q’ $ Q \le_p Q’ $ ( no more than a polynomial factor harder ) NP-completeness $ L \in NP $ $ L’ \le_p L $ for every $ L’ \in NP $. (hardest) Theorem 34.4 If any NP-complete problem is polynomial time solvable, then P = NP If any problem in NP is not polynomial-time solvable, then all NP-complete problems are not polynomial-time solvable. Circuit satisfiability Given a boolean combinational circuit composed of AND, OR, and NOT gates, is it satisfiable? 34.5 belongs to NP 34.6 NP-hard basic idea : represent the computation of A(algorithm verify L) as a sequence of configurations. Each configuration is mapped to the next configuration by a boolean combinational circuit M. The output is a distinguished bit in the working storage. The reduction algorithm F constructs a single combinational circuit that computes all configurations produced by a given initial configuration. 34.5 NP-complete problems]]></content>
      <categories>
        <category>CLRS</category>
      </categories>
      <tags>
        <tag>CLRS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题十二 基础DP]]></title>
    <url>%2F2017%2F12%2F20%2FACM%2Fkuangbin-12%2F</url>
    <content type="text"><![CDATA[kuangbin 专题十二 基础DP LIS dp[ i ]代表前i个数的最长子序列大小 dp[ i ] =max { dp[ j ] + 1 } ( val[ j ] &lt; val[ i ] ) $ O ( n^2 ) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int dp[ N ], val[ N ]; int LIS ( int n ) &#123; int mx = 0; for ( int i = 0; i &lt; n; ++i ) &#123; dp[ i ] = 1; for ( int j = 0; j &lt; i; ++j ) &#123; if ( val[ j ] &lt; val[ i ] &amp;&amp; dp[ i ] &lt; dp[ j ] + 1 ) dp[ i ] = dp[ j ] + 1; &#125; if ( mx &lt; dp[ i ] ) mx = dp[ i ]; &#125; return mx; &#125; $ O ( nlgn ) $ http://www.geeksforgeeks.org/longest-monotonically-increasing-subsequence-size-n-log-n/ http://blog.csdn.net/shuangde800/article/details/7474903 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 int val[ N ];//输入的值 int len;//ed数组的长度（LIS长度） int ed[ N ];//每个长度的序列的末尾 //下界，返回 &gt;= 所查找对象的第一个位置 int binary_search ( int i ) &#123; int left, right, mid; left = 0, right = len; while ( left &lt; right ) &#123; mid = left + ( right - left ) / 2; if ( ed[ mid ] &gt;= val[ i ] ) right = mid; else left = mid + 1; &#125; return left; &#125; void LIS ( int n ) &#123; ed[ 1 ] = val[ 1 ]; len = 1; for ( int i = 2; i &lt;= n; ++i ) &#123; //更新最长的末尾 if ( val[ i ] &gt; ed[ len ] ) ed[ ++len ] = val[ i ]; //产生了新的序列，改变旧的长度的末尾 else &#123; // 如果用STL： pos=lower_bound(ed,ed+len,val[i])-ed; int pos = binary_search ( i ); ed[ pos ] = val[ i ]; &#125; printf ( "%d\n", len ); &#125; &#125; LCS dp[ i ][ j ] 代表两个字符串前i / j 个下的最长大小 dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1 ( val[ i ] == val[ j ] ) dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ) ( != ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //CLRS 15.4 int len1, len2; char s1[ N ], s2[ N ]; int dp[ N ][ N ]; inline int max ( int a, int b ) &#123; return a &gt; b ? a : b; &#125; void LCS () &#123; for ( int i = 1; i &lt;= len1; i++ ) &#123; for ( int j = 1; j &lt;= len2; j++ ) &#123; if ( s1[ i ] == s2[ j ] ) dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1; else dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ); &#125; &#125; &#125; void Print ( int i, int j ) &#123; //当最长的子序列搜索完，但其中一串仍有剩余时，输出 if ( i == 0 || j == 0 ) &#123; return; &#125; //找到公共字符 if ( s1[ i ] == s2[ j ] ) &#123; Print ( i - 1, j - 1 ); printf ( "%c", s1[ i ] ); &#125; else if ( dp[ i - 1 ][ j ] &gt; dp[ i ][ j - 1 ] ) &#123; Print ( i - 1, j ); &#125; else &#123; Print ( i, j - 1 ); &#125; &#125; 完全背包 dp[ i ]代表背包重i的时候最大的价值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int v[ N ], w[ N ]; // value，weight int dp[ N ]; //n个物品，最多装wei的东西 int knapsack ( int n, int wei ) &#123; memset ( dp, INF, sizeof ( dp ) ); dp[ 0 ] = 0; for ( int i = 1; i &lt;= wei; ++i ) &#123; for ( int j = 0; j &lt; n; ++j ) &#123; if ( i &gt;= w[ j ] ) dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); &#125; &#125; return dp[ wei ]; &#125; 题目列表 A - Max Sum Plus Plus HDU - 1024 n个数，要求分成m组，使m组的和加起来得到最大值。 dp[i][j]表示前j个数分成i组的最大值。 dp[i][j]=max(dp[i][j-1]+a[j],max(dp[i-1][k])+a[j]) B - Ignatius and the Princess IV HDU - 1029 给n(奇数)个数，定义特殊的数为在序列中出现次数不少于(n+1)/2次的数，找出这个特殊的数 一边输入一边记录个数就好了 C - Monkey and Banana HDU - 1069 给定箱子种类数量n，及对应长宽高，每个箱子数量无限，求其能叠起来的最大高度是多少(上面箱子的长宽严格小于下面箱子) 按照长排序，在求宽关于高的LIS D - Doing Homework HDU - 1074 有n门功课需要完成，每一门功课都有时间期限以及你完成所需要的时间，如果完成的时间超出时间期限多少单位，就会被减多少学分，问以怎样的功课完成顺序，会使减掉的学分最少，有多个解时，输出功课名排列最小的一个。 15个作业状态压缩来做 枚举每一个状态，枚举每个状态新增的那个节点，再计算最小并记录前一个状态 E - Super Jumping! Jumping! Jumping! HDU - 1087 从起点到达终点，只能前行不能后退，且下一步必须比前面的点的值大，求所有走的点的值总和最大是多少。 dp[i] = max(dp[k] + a[j]); 1&lt;=k&lt;=i-1; 最大递增子串和。 F - Piggy-Bank HDU - 1114 给出存钱罐本身的重量和装钱后的重量，以及存钱罐中钱的面值和重量，求存钱罐装满时，钱的总和最小是多少 完全背包解题，每种钱币都可以装无限个，注意初始化的值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); G - 免费馅饼 HDU - 1176 0—10的点，不同时间在每个点上掉下来物品，只能到达左右两边距离为1和本身所在的位置，求最大物品数 dp[x][t] = max ( dp[x-1][t-1],dp[x][t-1],dp[x+1][t-1]) + v[x][t] H - Tickets HDU - 1260 单张或两张一起买，给出一个一个买票和两个两个买票的时间，求最少 dp[ i ] = min ( dp[ i - 1 ] + s[ i ], dp[ i - 2 ] + d[ i - 1 ] ); I - 最少拦截系统 HDU - 1257 求有多少个递减序列 反过来，转换成求整个系列有多少个LIS，则是所求的组数 J - FatMouse’s Speed HDU - 1160 给n个老鼠的体重和速度，求找出一个最长的序列，此序列体重递增速度递减 按体重递增排序，再求最长递增(此递增表示体重递增速度递减)子序列。 dp[i] = max(dp[j]+1) 0&lt;=j&lt;=i-1 K - Jury Compromise POJ - 1015 必须满足辩方总分D和控方总分P的差的绝对值|D-P|最小。如果有多种选择方案的 |D-P| 值相同，那么选辩控双方总分之和D+P最大的方案即可。 dp(j, k)表示，取j 个候选人，使其辩控差为k 的所有方案中，辩控和最大的那个方案的辩控和。 综上：dp[j][k]=dp[j-1][k-V[i]]+S[i] 正向计算，输出的时候就用正向的输出了,不过每次都要查找下一个位置是否在之前用过了 L - Common Subsequence POJ - 1458 LCS LCS模板 M - Help Jimmy POJ - 1661 老鼠在时刻0从高于所有平台的某处开始下落.当Jimmy落到某个平台上时，游戏者选择让它向左还是向右跑.当Jimmy跑到平台的边缘时，开始继续下落。Jimmy每次下落的高度不能超过MAX dp[i][0] = min(dp[k][0]+l[i]-l[k], dp[k][1]+r[i]-l[k]) + h[i]-h[k]; (左左和左右取最小) dp[i][1] = min(dp[k][0]+r[i]-l[k], dp[k][1]+r[i]-r[k]) + h[i]-h[k];(右左和右右取最小) N - Longest Ordered Subsequence POJ - 2533 LIS LIS模板 O - Treats for the Cows POJ - 3186 n个数在一个双端队列中，每次从队首或队尾出。出的第n个数乘以n，最后加起来，求最大和。 dp[i][j] 代表从i取到j的最大总数 dp[i][j] = max(dp[i+1][j]+a[i] * (n+i-j) , dp[i][j-1]+a[j] * (n+i-j)); P - FatMouse and Cheese HDU - 1078 给定一幅图，每个点有一定权值，现在有一只老鼠在起始点（0,0），他能水平或者垂直移动1~k格之后，停在某点并获得权值，而且每次移动后所在的点，都要比刚离开的那个点的权值更大，求最多能获得多少权值。 DP / Memoized dp[ x ][ y ] = dp[ xx ][ yy ] + val[ x ][ y ] Q - Phalanx HDU - 2859 给了一个字符串矩阵，求以次对角线方向对称的最大对称矩阵。 每次只需求最外面一层对称个数sum，再和右上角对称矩阵大小加一取最小就行，就求出当前小矩阵的最大对称矩阵。最后取个所有对称矩阵大小的最大值就行。 dp[i][j] = min(sum,dp[i-1][j+1]+1); R - Milking Time POJ - 3616 奶牛Bessie在0~N时间段产奶。农夫约翰有M个时间段可以挤奶，时间段f,t内Bessie能挤到的牛奶量e。奶牛产奶后需要休息R小时才能继续下一次产奶，求Bessie最大的挤奶量。 dp[ i ] = max ( dp[ j ] + node[ i ].val, dp[ i ] ) ( node[ j ].ed &lt;= node[ i ].st ) S - Making the Grade POJ - 3666 农夫约翰想修一条尽量平缓的路，路的每一段海拔是A_i，修理后是B_i，花费|A_i – B_i|，求最小花费。 dp[i][j] = min(dp[i – 1][k]) + |A[i] – B[j]| 离散化]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[GSLA ch-6] Eigenvalues and Eigenvectors]]></title>
    <url>%2F2017%2F12%2F14%2FLA%2FEigenvalues%20and%20Eigenvectors%2F</url>
    <content type="text"><![CDATA[Eigenvalues and Eigenvectors 6.1 Introduction certain vectors x are in the same direction as Ax basic equation: ​ $ Ax = \lambda x $ how to compute: let $ det(A-\lambda x)= 0 $ ,find the roots == find eigenvalues, find eigenvectors in Null space $ A^nx = \lambda^n x $ span eigenspace Projection: $ \lambda = 1 $ or 0 Reflection: $ \lambda = 1 $ or -1 Rotation: complex eigenvalues only product of eigenvalues == determinant == product of pivots sum of eigenvalues == sum of diagonal entries ( not pivots ) == trace 6.2 Diagonalizing eigenvectors in columns of S, eigenvalues in diagonal of Λ $ Λ = S^{-1}AS $ $ A = SΛS^{-1} $ Independent x from different λ: $ c_1 λ_1 x_1 + c_2 λ_2 x_2 = 0 $ $ c_1 λ_2 x_2 + c_2 λ_2 x_2 = 0 $ subtract $ (λ_1 - λ_2)c_1x_1 = 0 $ diagonalizability: enough eigenvectors (maybe same λ) so that S is invertible uk=Aku0=SΛkS−1u0u_k = A^k u_0 = S \Lambda^k S^{-1}u_0u​k​​=A​k​​u​0​​=SΛ​k​​S​−1​​u​0​​ $ u_0 = c_1x_1 + c_2x_2 + … +c_nx_n $ eigenvector basis multiply λik\lambda_i ^ kλ​i​k​​ add up A,B share same eigenvector matrix S if and only if AB = BA ?Heisenberg uncertainty principle position matrix P, momentum matrix Q, $ QP-PQ = I $ knew P still could not know Q $ |Px||Qx| \ge \frac{1}{2}|x|^2 $ 6.3 Applications to Differential Equations 6.4 Symmetric Matrices real eigenvalues orthonormal eigenvectors Spectral Theorem (principle of axis theorem) $ A = Q\Lambda Q^T $ Normal Matrices $ \bar A^TA = A \bar A^T $ symmetric, skewed-symmetric, orthogonal A has n orthonormal vectors ($ A = Q\Lambda \bar Q ^T $) if and only if A is normal Real Eigenvalues proof: $ Ax = \lambda x $ $ \bar x ^TA = \bar x ^T \bar\lambda $ ( $ A = A^T $ )( conjugate and transpose ) $ \bar x^T A x = \bar x^T \lambda x $ $ \bar x ^ T A x = \bar x ^ T \bar\lambda x $ left side the same therefor $ \lambda == \bar \lambda $ Orthonormal proof: no eigenvalues repeated Allow repeated eigenvalues ( ? Schur’s Theorem ) sum of rank one projection matrices $ A = \lambda_1x_1x_1^T + \lambda_2x_2x_2^T+… $ $ = \lambda_1P_1 +\lambda_2P2+… $ number of positive pivots == number of positive eigenvalues $ A = LDL^T $ 6.5 Positive Definite Matrices All λ &gt; 0 quick way to test All pivots positive n eigenvalues positive $ x^TAx $ is positive except x = 0 $ A == R^TR $ (symmetric) and R has independent columns ($ x^T R^TRX &gt;= 0 $) n upper left determinants R can be chosen: rectangular / $ (L\sqrt D)^T $ / $ Q \sqrt\Lambda Q^T $ $x^TAx $ (2*2) = $ ax^2 + 2bxy + cy^2 &gt; 0 $ ( ellipse $ z = x2/a2 + y2/b2 $ ) Application tilted ellipse $ x^TAx $ lined - up ellipse $ X^T\Lambda X = 1 $ rotation matrix Q axes: eigenvectors half-length: $ 1/\sqrt\lambda $ 6.6 Similar Matrices DEF: A similar to B (A family) $ B = M^{-1}AM $ Property: A and B have same eigenvalues x a eigenvector of A and $ M^{-1}x $ eigenvector of B Jordan Form triple eigenvalues while one eigenvector J with λ in the diagonal and 1 above similar to every matrices with repeated eigenvalues λ and one eigenvector λ repeated only once than J == Λ Jordan Block make A as simple as possible while preserving essential properties 6.7 Singular Value Decomposition SUMMARY $ A = U \Sigma V^T $ $ \Sigma^2 = \Lambda $ (of $ A^TA $ and $ AA^T $ ) $ U = Q $ (of $ AA^T $ in $ R^m $) $ V = Q $ (of $ A^TA $ in $ R^n $) orthonormal basis of row space {$ v_1, v_2, … v_r $} orthonormal basis of null space{ $ v_{r+1}, v_{r+2},…v_n $} orthonormal basis of column space{ $ u_1, u_2,…u_r $} orthonormal basis of left null space { $ u_{r+1}, u_{r+2}, … u_m $} Rotation – Stretch – Rotation $ Av_1 = \sigma_1u_1 $ … so $ AV = U\Sigma $ (m * n) * (n * n) = (m * m)* (m * n) $ V $ and $ U $ are orthogonal matrices $ \Sigma $ = ( old r*r $ \Sigma $ ) + (m-r zero rows) + ( n-r zero columns ) therefore … when A positive definite symmetric $ A = U \Sigma V^T = Q\Lambda Q^{T} $ 7.3 Diagonalization and the Pseudoinverse change bases $ \Lambda { w – w } = S^{-1}{std – w} A_{std} S_{ w – std } $ $ \Sigma { v – u } = U^{-1}{std – u} A{std}V{v – std} $ Polar Decomposition orthogonal and semidefinite rotation and stretching $ A = U\Sigma V^T = ( UV^T ) (V \Sigma V^T) = QH $ Pseudoinverse $ A^+ = V\Sigma^+ U^T $]]></content>
      <tags>
        <tag>Linear Algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题四 最短路]]></title>
    <url>%2F2017%2F12%2F10%2FACM%2Fkuangbin-4%2F</url>
    <content type="text"><![CDATA[kuangbin专题四 最短路 ACM图论存图方式 http://jzqt.github.io/2015/07/21/ACM图论之存图方式/ 单源最短路 Dijkstra ​ $ O(V^2 + E) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 int edg[ N ][ N ]; // weight of each edge int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited // CLRS 24.3 // s:start point, n: total number of nodes void dijkstra ( int s, int n ) &#123; // INITIALIZE-SINGLE-SOURCE memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = edg[ s ][ i ]; dis[ s ] = 0; // Extract Min // u for the node, mi for dis[ u ] for ( int i = 1; i &lt;= n - 1; ++i ) &#123; int u = -1, mi = INF; for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] &amp;&amp; dis[ j ] &lt; mi ) mi = dis[ u = j ]; vis[ u ] = true; // Relax edges adjacent to u for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] ) dis[ j ] = min ( dis[ j ], dis[ u ] + edg[ u ][ j ] ); &#125; &#125; Dijkstra + STL priority_queue ​ $ O((V+E)lgV) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 int head[ N ]; //链式前向星建图 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct Node &#123; int u, d; // id, dis bool operator&lt; ( const Node &amp;rhs ) const &#123; return d &gt; rhs.d; &#125; &#125;; struct Edge &#123; int u, v, w, nex; &#125; edg[ N ]; // CLRS 24.3 // s:start point void dijkstra ( int s ) &#123; priority_queue&lt;Node&gt; Q; dis[ s ] = 0; Q.push ( ( Node )&#123;s, dis[ s ]&#125; ); while ( !Q.empty () ) &#123; // u = Extract_Min( Q ) Node x = Q.top (); Q.pop (); int u = x.u; if ( vis[ u ] ) continue; vis[ u ] = true; // for each vertex v in G.adj[ u ] for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v; int w = edg[ i ].w; // relax if ( dis[ v ] &gt; dis[ u ] + w ) &#123; dis[ v ] = dis[ u ] + w; Q.push ( ( Node )&#123;v, dis[ v ]&#125; ); &#125; &#125; &#125; &#125; SPFA (Shortest Path Faster Algorithm) ​ $ O(kE) $ ( k &lt; 2 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited int cnt[ N ]; //入列次数超过n，有负环 int head[ N ]; //链式前向星 struct edg &#123; int u, v, w, nex; &#125; edg[ N ]; bool SPFA ( int s, int n ) &#123; // INIT memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = INF; dis[ s ] = 0; memset ( cnt, 0, sizeof ( cnt ) ); cnt[ s ] = 1; // BFS方式的spfa queue&lt;int&gt; Q; Q.push ( s ); while ( !Q.empty () ) &#123; int u = Q.front (); Q.pop (); vis[ u ] = false; // 出队要取消标记 for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v; // Relax所有出边 if ( dis[ v ] &gt; dis[ u ] + edg[ i ].w ) &#123; dis[ v ] = dis[ u ] + edg[ i ].w; //没入队的标记并入队 if ( !vis[ v ] ) &#123; vis[ v ] = true; Q.push ( v ); // 判断负环 if ( ++cnt[ v ] &gt; n ) return false; &#125; &#125; &#125; &#125; return true; &#125; 贴一个dfs的，自己没写过，感觉dijkstra+heap最好 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int spfa_dfs ( int u ) &#123; vis[ u ] = true; for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v, w = edg[ i ].w; if ( dis[ u ] + w &lt; dis[ v ] ) &#123; dis[ v ] = dis[ u ] + w; if ( !vis[ v ] ) &#123; if ( spfa_dfs ( v ) ) return 1; &#125; else return 1; &#125; &#125; vis[ u ] = false; return 0; &#125; Bellman-Ford ​ $ O(VE) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct edg &#123; int u, v, w; &#125; edg[ N ]; //CLRS 24.1 //对每个点，relax所有的边 bool Bellman_Ford ( int n, int e, int s, double num ) &#123; // Initialize Single Sorce( G,s ) for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = 0; dis[ s ] = num; // for i = 1 to |G.V|-1 for ( int i = 0; i &lt; n - 1; ++i ) &#123; // for each edg ( u,v ) in G.E for ( int j = 0; j &lt; e; ++j ) &#123; int u = edg[ j ].u; int v = edg[ j ].v; // Relax if ( dis[ v ] &gt; dis[ u ] + edg[ j ].w ) &#123; dis[ v ] = dis[ u ] + edg[ u ].w; &#125; &#125; &#125; // 存在负环 for ( int i = 0; i &lt; e; ++i ) if ( dis[ edg[ i ].v ] &gt; ( dis[ edg[ i ].u ] - edg[ i ].w ) ) return false; return true; &#125; 适用场景 如果是稠密图，Dijkstra+heap比SPFA快。稀疏图则SPFA更快。再就是SPFA可以判断负环 对于极端的链状图，SPFA无疑是最合适的了。每个结只进队一次，标准的O(E)。 朴素的dijkstra对于这样的图就力不从心了：每次循环都过一遍结点，在松弛，然后发现每次O(V)的时间只松弛了一个点。 ​ 多源最短路 Floyd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // CLRS 25.2 All-Pairs-Shortest-Path void Floyd () &#123; // Dp // 表示所有点只经过集合&#123; 1..k &#125;内的点时的最短路径 // 只经过1的时候relax一次，在添加上经过2后的relax，最后一直到n for ( int k = 1; k &lt;= n; ++k ) &#123; //选取每一个起点 for ( int i = 1; i &lt;= n; ++i ) //选取每一个终点 for ( int j = 1; j &lt;= n; ++j ) // d(ij)^k = min &#123; d(ik)^(k-1) + d(kj)^(k-1) &#125; dis[ i ][ j ] = dis[ i ][ j ] || ( dis[ i ][ k ] &amp;&amp; dis[ k ][ j ] ); &#125; &#125; 题目列表 A - Til the Cows Come Home POJ-2387 一个农场有n (1000)个点，有t (2000)条道路连接, 从n到1最短 dijkstra 模板 B - Frogger POJ-2253 无向图一条1~2的路径使得该路径上的最大边权最小. (max Route weight is the minimum among all routes) dijkstra变形 double minimax = max ( mi, v[ j ][ u ] ); C - Heavy Transportation POJ - 1797 ​ 有n个城市，n个城市之间有m条公路或桥梁，每个公路或桥都有一个最大载重量，问从城市1到城市n所能运送到货物到最大重量是多少 ( min Route weight is the maximum among all routes ) dijkstra变形 int maxmini = min ( mi, v[ j ][ u ] ); D - Silver Cow Party POJ - 3268 n个农场，m条单向路，n个牛分别在n个农场，第x农场为终点，问每个牛从所在农场前往x农场的往返路程最小值是多少，求出n个牛中最短路上往返路程的最大的那个 从n个点到1再从1回到n个点，通过调转边的方向两次dijkstra E - Currency Exchange POJ - 1860 有n种货币，你含有num面额的其中一种货币。求有没有可能，在多次兑换后你手中的货币大于num。 求最大路径，反向用Bellman-Ford F - Wormholes POJ - 3259 农场之间有很多条路，以及单向的虫洞，每条路走完会花费一定的时间，而虫洞可以回到之前的时间，问农场主是否能回到自己出发时间前的出发点 SPFA判断负环 G - MPI Maelstrom POJ - 1502 从第一个点出发，求到其他点最短路的最大值 dijkstra 注意下三角矩阵邻接表的建图 H - Cow Contest POJ - 3660 n个牛进行比赛，现已知m个关系， 牛u可以胜过牛v。问最后可以确定排名位数的有几个牛. Floyd判断两两牛之间的关系。如果一个牛可以胜过a个牛，b个牛可以胜过它，那么如果a＋b＝n－1，他的排名就可以确定 I - Arbitrage POJ - 2240 给定多种货币之间的兑换关系，问是否可以套利 Bellman-Ford判断正环（要返回自己所以松弛n次) floyd判断回来后是否&gt;1 J - Invitation Cards POJ - 1511 求源点到各点的往返最短路之和 邻接表逆置（建了两个邻接表） 数据多需要优化SPFA/dijkstra+heap K - Candies POJ - 3159 给n个人分糖果，m组数据a，b，c；意思是a比b少的糖果个数绝对不超过c个，也就是d(b)-d(a) &lt; c，求1比n少的糖果数的最大值。 差分约束，和最短路的松弛一样 数据多，dijkstra+heap L - Subway POJ - 2502 小明步行的速度是10km/h，地铁速度是40km/h，给定家和学校的坐标，再给定多条地铁线路站点的坐标，问小明从家到学校所需的最短时间 dijkstra，建图连接所有的点并赋值时间 M - 昂贵的聘礼 POJ - 1062 每个人可能有直接购买或者交换物品换取折扣这两种方式交易（交换物品要从别人手里买） 等级差之间超过m的不能交易 求用最少的钱买到非洲大酋长的承诺 等级限制采用枚举的方法，分别从lv[ 1 ] - m ~~ lv[ 1 ] + m，每次枚举的区间长度为m,一共m次最短路搜索 N - Tram POJ - 1847 电动巴士在每个十字路口有一个默认方向，走向别的方向需要改动扳手。 dijkstra 每个边初始化为INF,要切换的路 = 1,不用切换 = 0 O - Extended Traffic LightOJ - 1074 给定每条街的拥挤度p(x)，街a到街b的时间就是(p(b)-p(a))**3，求第一个点到第k个点的最短路 SPFA判断负环 dfs记录负环里的点 P - The Shortest Path in Nya Graph HDU - 4725 共n个点，n层(每个点单独一层)，相邻的两层之间权值为w 还有m条额外的边，权值为v，求1到n的最短路 建图 ！！给每个点两个辅助点，一个做出度，一个做入度 Q - Marriage Match IV HDU - 3416 网络流，不会qwq R - 0 or 1 HDU - 4370 X12+X13+…X1n=1,X1n+X2n+…Xn-1n=1,其余行列和相同，求ΣCij*Xij 神一样的建图！！节点1的出度为1.节点n的入度为1.节点2-n-1的入度和出度相等. 问题就相当于求一条最短路，从节点1出发，到节点N. 同时节点1的一个最小环+节点n的一个最小环也是可行解，两者取最小 S - Layout POJ - 3169 两点间可能&lt;= x 或者&gt;=x，求1–&gt;n最大 差分约束，和最短路一样，SPFA判断负环则代表不存在可行解]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
</search>
