<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[CS:APP-ch8] Shell Lab]]></title>
    <url>%2F2018%2F03%2F21%2FCSAPP%2Fshell%2F</url>
    <content type="text"><![CDATA[[CS:APP-ch8] Shell Lab http://csapp.cs.cmu.edu/3e/labs.html http://condor.depaul.edu/glancast/374class/hw/shlab-readme.html https://github.com/zhoudiqiu/Shell-lab/blob/master/lab5/tsh.c &lt;br/&gt; Target implement eval, builtin_cmd, do_bgfg, waitfg sigchld_handler, sigint_handler, sigtstp_handler &lt;br/&gt; &lt;!-- more --&gt; 0. Preparation tshref: 正确的参考shell，要求自己实现后的shell和它的结果一样 make test01用trace01.txt来验证结果，可以参考make rtest01的结果 gdb makefile去掉参数O2,加上-g &lt;br/&gt; 1. void eval ( char*cmdline ) 原型：p755 If command is a built-in command, the shell program handles it immediately. Otherwise, the shell creates a child process to load and execute the program for command. Hint In eval, the parent must use sigprocmaskto block SIGCHLD signals before it forks the child, then unblock these signals, again using sigprocmask after it adds the child to the job list by calling addjob. Since children inherit the blocked vectors of their parents, the child must be sure to then unblock SIGCHLD signals before it execs the new program. The parent needs to block the SIGCHLD signals in this way in order to avoid the race condition where the child is reaped by sigchld_handler (and thus removed from the job list) before the parent calls addjob. After the fork, but before the execve, the child process should call setpgid(0, 0), which puts the child in a new process group whose group ID is identical to the child’s PID. This ensures that there will be only one process, your shell, in the foreground process group. When you type ctrl-c, the shell should catch the resulting SIGINT and then forward it to the appropriate foreground job (or more precisely, the process group that contains the foreground job). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 /* * eval - Evaluate the command line that the user has just typed in * * If the user has requested a built-in command (quit, jobs, bg or fg) * then execute it immediately. Otherwise, fork a child process and * run the job in the context of the child. If the job is running in * the foreground, wait for it to terminate and then return. Note: * each child process must have a unique process group ID so that our * background children don't receive SIGINT (SIGTSTP) from the kernel * when we type ctrl-c (ctrl-z) at the keyboard. */ void eval ( char *cmdline ) &#123; pid_t pid; sigset_t mask; char *argv[ MAXARGS ]; //Parse the command line and build the argv array int bg = parseline ( cmdline, argv ); if ( !builtin_cmd ( argv ) ) &#123; // blocking SIGCHLD, race sigemptyset ( &amp;mask ); sigaddset ( &amp;mask, SIGCHLD ); sigprocmask ( SIG_BLOCK, &amp;mask, NULL ); // fork new process if ( ( pid = fork () ) &lt; 0 ) unix_error ( "fork error" ); // child process else if ( pid == 0 ) &#123; // unblock SIGCHLD in child process sigprocmask ( SIG_UNBLOCK, &amp;mask, NULL ); // ensures that there will be only one process setpgid ( 0, 0 ); // execute command if ( execvp ( argv[ 0 ], argv ) &lt; 0 ) &#123; printf ( "%s: Command not found\n", argv[ 0 ] ); exit ( 1 ); &#125; // parent &#125; else &#123; addjob ( jobs, pid, bg ? BG : FG, cmdline ); // get SIGCHLD back after add job sigprocmask ( SIG_UNBLOCK, &amp;mask, NULL ); if ( !bg ) &#123; // reap when job terminated ( send signal SIGCHILD ) waitfg ( pid ); &#125; else &#123; //[1] (6325) ./myspin 1 &amp; printf ( "[%d] (%d) %s", pid2jid ( pid ), pid, cmdline ); &#125; &#125; &#125; return; &#125; &lt;br/&gt; 2.int builtin_cmd( char **argv) Four commands are to be built-in in the shell quit: exit the shell. jobs: List the running and stopped background jobs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /* * builtin_cmd - If the user has typed a built-in command then execute * it immediately. */ int builtin_cmd ( char **argv ) &#123; if ( strcmp ( argv[ 0 ], "quit" ) == 0 ) &#123; exit ( 0 ); &#125; else if ( strcmp ( argv[ 0 ], "jobs" ) == 0 ) &#123; listjobs ( jobs ); return 1; &#125; else if ( !strcmp ( argv[ 0 ], "fg" ) || !strcmp ( argv[ 0 ], "bg" ) ) &#123; do_bgfg ( argv ); return 1; &#125; return 0; /* not a builtin command */ &#125; &lt;br/&gt; 3. void waitfg( pid_t pid ) shell should wait for foreground process This function should wait by sleeping for 1 second repeatedly until the specified process is no longer the foreground process (state = FG) Hints In waitfg, use a busy loop around the sleep function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /* * waitfg - Block until process pid is no longer the foreground process */ void waitfg ( pid_t pid ) &#123; struct job_t *job = getjobpid ( jobs, pid ); if ( pid == 0 ) return; if ( job != NULL ) &#123; // sleep while ( pid == fgpid ( jobs ) ) &#123; &#125; &#125; return; &#125; &lt;br/&gt; 4. void do_bgfg( char **argv ) bg &lt;job&gt;: Change a stopped background job to a running background job. fg &lt;job&gt;: Change a stopped or running background job to a running in the foreground. The bg &lt;job&gt; command restarts &lt;job&gt; by sending it a SIGCONTsignal, and then runs it in the background. The &lt;job&gt; argument can be either a PID or a JID. The fg &lt;job&gt;command restarts &lt;job&gt; by sending it a SIGCONTsignal, and then runs it in the foreground. The &lt;job&gt; argument can be either a PID or a JID. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 /* * do_bgfg - Execute the builtin bg and fg commands */ void do_bgfg ( char **argv ) &#123; struct job_t *job; char *tmp; int jid; pid_t pid; tmp = argv[ 1 ]; // unvalid id if ( tmp == NULL ) &#123; printf ( "%s command requires PID or %%jobid argument\n", argv[ 0 ] ); return; &#125; // jid if ( tmp[ 0 ] == '%' ) &#123; // string to integer jid = atoi ( &amp;tmp[ 1 ] ); job = getjobjid ( jobs, jid ); // unvalid jid if ( job == NULL ) &#123; //%2: No such job printf ( "%s: No such job\n", tmp ); return; &#125; else &#123; pid = job-&gt;pid; &#125; &#125; // pid else if ( isdigit ( tmp[ 0 ] ) ) &#123; pid = atoi ( tmp ); job = getjobpid ( jobs, pid ); // unvalid pid if ( job == NULL ) &#123; //(2): No such process printf ( "(%d): No such process\n", pid ); return; &#125; &#125; else &#123; printf ( "%s: argument must be a PID or %%jobid\n", argv[ 0 ] ); return; &#125; // awakened by the receipt of a SIGCONT signal. kill ( -pid, SIGCONT ); if ( !strcmp ( "fg", argv[ 0 ] ) ) &#123; // foreground, shell wait for it job-&gt;state = FG; waitfg ( job-&gt;pid ); &#125; else &#123; job-&gt;state = BG; // print printf ( "[%d] (%d) %s", job-&gt;jid, job-&gt;pid, job-&gt;cmdline ); &#125; return; &#125; &lt;br/&gt; 5.void sigchld_handler( int sig ) This is the shell's handler for theSIGCHLD signal. Hints In sigchld_handler, use exactly one call to waitpid 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /* * sigchld_handler - The kernel sends a SIGCHLD to the shell whenever * a child job terminates (becomes a zombie), or stops because it * received a SIGSTOP or SIGTSTP signal. The handler reaps all * available zombie children, but doesn't wait for any other * currently running children to terminate. */ void sigchld_handler ( int sig ) &#123; pid_t pid; int status; // wait for all child to end while ( ( pid = waitpid ( -1, &amp;status, WNOHANG | WUNTRACED ) ) &gt; 0 ) &#123; // child currently stopped if ( WIFSTOPPED ( status ) ) &#123; // change state struct job_t *job = getjobpid ( jobs, pid ); job-&gt;state = ST; int jid = pid2jid ( pid ); // Job [2] (14171) stopped by signal 20 printf ( "Job [%d] (%d) stopped by signal %d\n", jid, pid, WSTOPSIG ( status ) ); &#125; // child terminated because of a signal else if ( WIFSIGNALED ( status ) ) &#123; int jid = pid2jid ( pid ); // INT // Job [1] (13253) terminated by signal 2 printf ( "Job [%d] (%d) terminated by signal %d\n", jid, pid, WTERMSIG ( status ) ); deletejob ( jobs, pid ); &#125; // child terminated normally else if ( WIFEXITED ( status ) ) &#123; deletejob ( jobs, pid ); &#125; &#125; return; &#125; &lt;br/&gt; 6. void sigint_handler ( int sig ) Hints When you implement your signal handlers, be sure to send SIGINT and SIGTSTP signals to the entire foreground process group, using -pid instead of pid in the argument to the kill function. The sdriver.pl program tests for this error. 1 2 3 4 5 6 7 8 9 10 11 12 13 /* * sigint_handler - The kernel sends a SIGINT to the shell whenver the * user types ctrl-c at the keyboard. Catch it and send it along * to the foreground job. */ void sigint_handler ( int sig ) &#123; pid_t pid = fgpid ( jobs ); if ( pid != 0 ) &#123; kill ( -pid, sig ); &#125; return; &#125; &lt;br/&gt; 7.void sigtstp_handler ( int sig ) 1 2 3 4 5 6 7 8 9 10 11 12 13 /* * sigtstp_handler - The kernel sends a SIGTSTP to the shell whenever * the user types ctrl-z at the keyboard. Catch it and suspend the * foreground job by sending it a SIGTSTP. */ void sigtstp_handler ( int sig ) &#123; pid_t pid = fgpid ( jobs ); if ( pid != 0 ) &#123; kill ( -pid, sig ); &#125; return; &#125;]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch2] Building Abstractions with Data]]></title>
    <url>%2F2018%2F03%2F10%2FSICP%2Fsicp2%2F</url>
    <content type="text"><![CDATA[SICP-ch2 Building Abstractions with Data building abstractions by forming compound data &lt;br/&gt; Intro 2.1 data abstraction erect abstraction barriers 2.2 data == procedures, sequences, closure, conventional interface 2.3 symbols as elementary data 2.4 generic operations, data directed programming, additivity 2.5 implement a package &lt;br/&gt; &lt;!-- more --&gt; 2.1 Introduction to Data Abstraction Data abstraction isolating ( the parts of a program that deal with how data objects are represented ) from ( the parts of a program that deal with how data objects are used ) &lt;br/&gt; Construct abstract data use constructors, selectors as interface &lt;br/&gt; 2.1.1 Example: Arithmetic Operations for Rational Numbers assume the constructor and selectors are available 1 numer , denom , make-rat use them to express rational operation 1 2 3 4 (define (add-rat x y) (make-rat (+ (* (numer x) (denom y)) (* (numer y) (denom x))) (* (denom x) (denom y)))) implement representation 1 2 3 (define (make-rat n d) (cons n d)) (define (numer x) (car x)) (define (denom x) (cdr x)) &lt;br/&gt; 2.1.2 Abstraction Barriers &lt;br/&gt; 2.1.3 What Is Meant by Data? DEF: some collection of selectors and constructors together with specified conditions for the representation &lt;br/&gt; Example Pairs z = ( x, y ) selector: car, cdr constructor: cons condition: if z = ( x, y ) =&gt; ( car z ) = x &amp;&amp; ( cdr z ) = y 1 2 3 4 5 6 7 8 9 (define (cons x y) (define (dispatch m) (cond ((= m 0) x) ((= m 1) y) (else (error "Argument not 0 or 1: CONS" m)))) ;message passing: data representation returns procedure dispatch) (define (car z) (z 0)) (define (cdr z) (z 1)) &lt;br/&gt; Exercise 2.6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 ;2.6 ;church numerals DEF ( define zero ( lambda ( f ) ( lambda ( x ) x ) ) ) ( define ( add-1 n ) ( lambda ( f )( lambda ( x ) ( f ( ( n f ) x ) )))) ;从0开始，每多一个f就+1 ( define ( church-to-int ch ) ( ( ch ( lambda ( n ) ( + n 1 ) ) ) 0 ) ) ( define one ( lambda ( f ) ( lambda ( x ) ( f x ) ) ) ) ( define two ( lambda ( f ) ( lambda ( x ) ( f ( f x ) ) ) ) ) ;a,b: function of zero, one, two... ;b以x为参数经过b次apply f ;a以x经过b次apply后的结果为参数(( b f ) x )，在此基础上再apply f a次 ( define ( add a b ) ( lambda ( f ) ( lambda ( x ) ( ( a f ) ( ( b f ) x ) ) ) ) ) ;调用 ( define ( square x ) ( * x x )) ; f = square x = 2 ( ( two square ) 2 ) &lt;br/&gt; 2.2 Hierarchical Data and the Closure Property closure property of cons ( an operation ) The ability to create pairs whose elements are pairs Closure is the key to create hierarchical structures &lt;br/&gt; 2.2.1 Representing Sequences The entire sequence is constructed by nested cons operations 1 2 3 4 (cons 1 (cons 2 (cons 3 (cons 4 nil)))) Provides a primitive 1 (list ⟨ a 1 ⟩ ⟨ a 2 ⟩ . . . ⟨ a n ⟩ ) Operations 1 ref, length, append, last-pair, reverse &lt;br/&gt; Mapping over list Establish an abstraction barrier that isolates the implementation of procedures that transform lists from the details of how the elements of the list are extracted and combined. 1 2 3 4 5 (define (map proc items) (if (null? items) nil (cons (proc (car items)) (map proc (cdr items))))) &lt;br/&gt; 2.2.2 Hierarchical Structures Tree sequences whose elements are sequences Recursion is a natural tool for dealing with tree structures &lt;br/&gt; 2.2.3 Sequences as Conventional Interfaces Conventional Interface permits us to combine processing modules &lt;br/&gt; Signal Processing A signal-processing engineer would find it natural to conceptualize these processes in terms of signals flowing through a cascade of stages, each of which implements part of the program plan. &lt;br/&gt; Organizing programs so as to reflect the signal-flow structure concentrate on the “signals” represent these signals as lists use list operations to implement the processing helps us make program designs that are modular connecting the components in flexible ways &lt;br/&gt; Nested Mappings 1 2 (define (flatmap proc seq) (accumulate append nil (map proc seq))) &lt;br/&gt; 2.3 Symbolic Data Issue arise words and sentences may be regarded either as semantic entities ( meaning ) or as syntactic entities ( characters ) Using quotation mark 1 2 (list 'a 'b) (a b) manipulating symbols 1 eq?, memq &lt;br/&gt; 2.4 Multiple Representations for Abstract Data cope with data that may be represented in different ways by different parts of a program Data Abstraction separate the task of designing a program that uses rational numbers from the task of implementing rational numbers an application of the &quot;principle of least commitment&quot; Abstraction Barrier formed by the selectors and constructors permits us to defer to the last possible moment the choice of a concrete representation thus retain maximum flexibility &lt;br/&gt; 2.4.1 Representations for Complex Numbers constructors 1 2 (make-from-real-imag (real-part z) (imag-part z)) (make-from-mag-ang (magnitude z) (angle z)) selectors rectangular representation &amp; polar representation 1 real-part,imag-part,magnitude,angle implement arithmetic on complex numbers 1 add-complex, sub-complex, mul-complex, div-complex implement 2 representations of complex number 1 分别用直角和极坐标来实现4个selector和2个constructor &lt;br/&gt; 2.4.2 Tagged data If both representations are included in a single system, we will need some way to distinguish data in polar form from data in rectangular form. A straightforward way to accomplish this distinction is to include a type tag —the symbol rectangular or polar —as part of each complex number. 1 2 (define (make-from-real-imag-rectangular x y) (attach-tag 'rectangular (cons x y))) &lt;br/&gt; Make sure names do not conflict: Append the suffix -rectangular 1 (define (real-part-rectangular z) (car z)) &lt;br/&gt; Generic selector Dispatch: checks the tag and calls the appropriate procedure of that type. because the selectors are generic, operation ( add, mul ) are unchanged 1 2 3 4 5 6 (define (real-part z) (cond ((rectangular? z) (real-part-rectangular (contents z))) ((polar? z) (real-part-polar (contents z))) (else (error "Unknown type: REAL-PART" z)))) &lt;br/&gt; Generic Constructor 1 2 (define (make-from-real-imag x y) (make-from-real-imag-rectangular x y)) &lt;br/&gt; General mechanism for interfacing the separate representations stripping off and attaching tags as data objects are passed from level to level &lt;br/&gt; 2.4.3 Data-Directed Programming and Additivity Two weaknesses of dispatching above generic interface procedures must know about all the different representations must guarantee that no two procedures in the entire system have the same name &lt;br/&gt; Additive design the individual packages separately and combine them to produce a generic system &lt;br/&gt; Data-directed programming dealing with a two-dimensional table the possible operations on one axis and the possible types on the other axisss &lt;br/&gt; Manipulate table 1 2 (put ⟨ op ⟩ ⟨ type ⟩ ⟨ item ⟩ ) (get ⟨ op ⟩ ⟨ type ⟩ ) &lt;br/&gt; Defines a package Interfaces these by adding entries to the table 1 2 3 4 5 6 7 8 9 10 11 12 (define (install-rectangular-package) ;; internal procedures (define (real-part z) (car z)) (define (make-from-real-imag x y) (cons x y)) ;; interface to the rest of the system (define (tag x) (attach-tag 'rectangular x)) (put 'real-part '(rectangular) real-part) (put 'make-from-real-imag 'rectangular (lambda (x y) (tag (make-from-real-imag x y)))) 'done) &lt;br/&gt; Generic Selectors 1 2 3 4 5 6 7 8 9 (define (apply-generic op . args) (let ((type-tags (map type-tag args))) (let ((proc (get op type-tags))) (if proc ;apply arguments to procedure (apply proc (map contents args)) (error "No method for these types: APPLY-GENERIC" (list op type-tags)))))) 1 (define (real-part z) (apply-generic 'real-part z)) &lt;br/&gt; Generic Constructors 1 2 (define (make-from-real-imag x y) ((get 'make-from-real-imag 'rectangular) x y)) &lt;br/&gt; Interface ( generic selectors ) Previous Data-directed a set of procedures single procedure explicit dispatch looks up name conflicts internal change if a new representation is added doesn't change Centralized selectors Decentralized &lt;br/&gt; Message passing Data object receives the requested operation name as a “message.” &lt;br/&gt; Instead of using “intelligent operations” that dispatch on data types work with “intelligent data objects” that dispatch on operation names. &lt;br/&gt; Represent data object as a procedure takes as input the required operation name performs the operation indicated &lt;br/&gt; Constructor 1 2 3 4 5 6 (define (make-from-real-imag x y) (define (dispatch op) (cond ((eq? op 'real-part) x) (else (error "Unknown op: MAKE-FROM-REAL-IMAG" op)))) ;return procedure dispatch) &lt;br/&gt; Selector 1 (define (apply-generic op arg) (arg op)) &lt;br/&gt; 2.5 Systems with Generic Operations use data-directed techniques to construct a package of arithmetic operations 2.5.1 Generic Arithmetic Operations generic arithmetic procedures 1 (define (add x y) (apply-generic 'add x y)) install packages 1 2 3 4 5 6 7 8 9 10 11 12 13 (define (install-complex-package) ;; imported procedures from rectangular and polar packages (define (make-from-real-imag x y) ((get 'make-from-real-imag 'rectangular) x y)) ;; internal procedures (define (add-complex z1 z2) (make-from-real-imag (+ (real-part z1) (real-part z2)) (+ (imag-part z1) (imag-part z2)))) ;; interface to rest of the system (define (tag z) (attach-tag 'complex z)) (put 'make-from-real-imag 'complex (lambda (x y) (tag (make-from-real-imag x y)))) 'done) constructor 1 2 (define (make-complex-from-real-imag x y) ((get 'make-from-real-imag 'complex) x y)) two-level tag system &lt;br/&gt; 2.5.3 Example: Symbolic Algebra implement polynomial package arithmetic on polynomials representation of polynomials arithmetic on term list representation of term list Data Directed Recursion using generic operation ( ADD ) inside each package Hierarchy of types in symbolic data recursive data abstraction neither of these types is above the other naturally difficult to control coercion &lt;br/&gt; 2.5.2 Combining Data of Different Types Define cross-type operations &lt;br/&gt; One way: design a different procedure for each possible combination of types 1 2 3 4 5 ;; to be included in the complex package (define (add-complex-to-schemenum z x) (make-from-real-imag (+ (real-part z) x) (imag-part z))) (put 'add '(complex scheme-number) (lambda (z x) (tag (add-complex-to-schemenum z x)))) Cumbersome more code is needed individual packages need to take account of other packages ( not additive ) &lt;br/&gt; Coercion objects of one type may be viewed as being of another type coercion procedure 1 2 (define (scheme-number-&gt;complex n) (make-complex-from-real-imag (contents n) 0)) install in coercion table 1 2 3 (put-coercion 'scheme-number 'complex scheme-number-&gt;complex) apply-generic check the coercion table to see if objects of the first type can be coerced to the second type or if there is a way to coerce the second argument to the type of the first argument 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (define (apply-generic op . args) (let ((type-tags (map type-tag args))) (let ((proc (get op type-tags))) (if proc (apply proc (map contents args)) (if (= (length args) 2) (let ((type1 (car type-tags)) (type2 (cadr type-tags)) (a1 (car args)) (a2 (cadr args))) (let ((t1-&gt;t2 (get-coercion type1 type2)) (t2-&gt;t1 (get-coercion type2 type1))) (cond (t1-&gt;t2 (apply-generic op (t1-&gt;t2 a1) a2)) (t2-&gt;t1 (apply-generic op a1 (t2-&gt;t1 a2))) (else (error "No method for these types" (list op type-tags)))))) (error "No method for these types" (list op type-tags))))))) &lt;br/&gt; Advantage need to write only one procedure for each pair of types rather than a different procedure for each collection of types and each generic operation. Not General Enough can't converting both objects to a third type. &lt;br/&gt; Hierarchies of types apply-generic raise the object to its super type until we either find a level at which the desired operation can be performed or hit the top Inadequates multiple-supertypes means that there is no unique way to “raise” a type in the hierarchy]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SICP-ch1] Building Abstractions with Procedures]]></title>
    <url>%2F2018%2F02%2F28%2FSICP%2Fsicp1%2F</url>
    <content type="text"><![CDATA[SICP ch-1 Building Abstractions with Procedures We are about to study the idea of computational process &lt;br/&gt; 1.1 The Elements of Programming Language combine simple ideas to form complex ideas by three mechanisms primitive expression means of combination means of abstraction &lt;br/&gt; &lt;!-- more --&gt; 1.1.1 Expressions you type an expression, the interpreter responds by displaying the result of its evaluating that expression ( 486 ) Combinations Expressions formed by delimiting a list of expressions within parentheses ( + 137 349 ) operator, operands &lt;br/&gt; 1.1.2 Naming and the Environment Naming language provides the means of using names ( variable ) to refer to computational objects ( value ) Associating values with symbols Environment The memory that interpreter must maintain to keeps track of the name-object pairs. ( ch-3 ) Determine the meaning of the symbols in expressions. &lt;br/&gt; 1.1.3 Evaluating Combinations Interpreter is itself following a procedure Evaluate the subexpressions of the combination Apply the procedure that is the value of the leftmost subexpression (the operator) to the arguments that are the values of the other subexpressions (the operands). Thus, the evaluation rule is recursive in nature. &lt;br/&gt; Special Forms Exceptions to the general evaluation rule. ( apply operator to operands ) Each special form has its own evaluation rule. define, cond, if, and, or, &lt;br/&gt; 1.1.4 Compound Procedures Procedure definitions 1 2 (define (&lt;name&gt; &lt;formal parameters&gt;) &lt;body&gt;) &lt;br/&gt; 1.1.5 The Substitution Model for Procedure Application Evaluate a combination evaluates the elements of the combination applies the procedure to the arguments &lt;br/&gt; Substitution Model To apply a compound procedure to arguments, evaluate the body of he procedure with each formal parameter replaced by the corresponding argument. Typical interpreters do not evaluate procedure by substitute values, but using a local environment for the formal parameters ( ch-3 ) This model breaks down when we address procedures with &quot;mutable data&quot; ( ch-3 set! ) &lt;br/&gt; Normal Order fully expand and then reduce (x) Applicative Order evaluate the arguments and then apply &lt;br/&gt; 1.1.6 Conditional Expressions and Predicates Predicate an expression whose value is interpreted as either true or false. 1 2 3 4 5 6 7 8 9 10 11 12 (cond (&lt;p1&gt; &lt;e1&gt;) (&lt;p2&gt; &lt;e2&gt;) ... (&lt;pn&gt; &lt;en&gt;)) (if &lt;predicate&gt; &lt;consequent&gt; &lt;alternative&gt;) (and &lt;e1&gt; ... &lt;en&gt;) (or &lt;e1&gt; ... &lt;en&gt;) (not &lt;e&gt;) &lt;br/&gt; 1.1.7 Example: Square Roots by Newton’s Method 1.1.8 Procedures are Black-Box Abstractions bound variables In procedure definition, it doesn't matter what name the formal parameter has. free variables the scope of that name &lt;br/&gt; 1.2 Procedures and the Processes They Generate Procedure A procedure is a pattern for the local evolution of a computational process. It specifies how each stage of the process is built upon the previous stage. &lt;br/&gt; 1.2.1 Linear Recursion and Iteration Recursive procedure syntactic fact that the procedure definition refers to the procedure itself Recursive process The process that characterized by a chain of deferred operations Requires interpreter keep track of operations to be performed later on. Iterative process one whose state can be summarized by a fixed number of state variables, together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an (optional) end test. Tail Recursive An implementation with the property of: &quot;An iterative process will be executed in constant space, even if it is described by a recursive procedure.&quot; &lt;br/&gt; 1.3 Formulating Abstractions with Higher-Order Procedures Procedures are abstractions that describe compound operations Higher-order procedures procedures that manipulate procedures &lt;br/&gt; 1.3.1 Procedures as Arguments 1.3.2 Constructing Procedures Using lambda In general, lambda is used to create procedures in the same way as define , except that no name is specified for the procedure 1 2 (lambda (&lt;formal parameters&gt;) &lt;body&gt;) using lambda creating local variables 1 2 3 4 5 (let ((&lt;var1&gt; &lt;exp1&gt;) (&lt;var2&gt; &lt;exp2&gt;) ... (&lt;varn&gt; &lt;expn&gt;)) &lt;body&gt;) &lt;br/&gt; 1.3.3 Procedures as General Methods finding roots finding fixed point 1.3.4 Procedures as Returned Values return values are themselves procedures &lt;br/&gt; Abstractions and first-class procedures programming languages impose restrictions on the ways in which computational elements can be manipulated First-class status elements with fewest restrictions They may be named by variables. They may be passed as arguments to procedures. They may be returned as the results of procedures. They may be included in data structures. Lisp awards procedures full first-class status, which poses challenges for efficient implementation, but the resulting gain in expressive power is enormous.]]></content>
      <categories>
        <category>SICP</category>
      </categories>
      <tags>
        <tag>SICP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch1] A Tour of Computer Systems]]></title>
    <url>%2F2018%2F01%2F01%2FCSAPP%2Fch-1%2F</url>
    <content type="text"><![CDATA[CS:APP ch-1 A Tour of Computer Sytems A computer system consists of hardware and systems software that work together to run application programs. Tracing the life time of hello program. 1 2 3 4 5 6 7 8 #include &lt;stdio.h&gt; int main() &#123; printf("hello world!\n"); return 0; &#125; &lt;br/&gt; &lt;!-- more --&gt; 1.1 Information Is Bits + Context The hello program begins life as a source program ( a sequence of bits, organized in bytes ) Text files files consist exclusively ASCII characters. Binary files all other files All information in a system is represented as a bunch of bits. The only thing that distinguishes different data objects is the context in which we view them. &lt;br/&gt; 1.2 Programs Are Translated by Other Programs into Different Forms In order to run hello.c on the system, the individual C statements must be translated by other programs into a sequence of low-level machine-language instructions. These instructions are then packaged in a form called an executable object program and stored as a binary disk file. 1.3 It Pays to Understand How Compilation Systems Work Reasons why need to understand compilation systems Optimizing program performance Understanding link-time error Avoiding security holes &lt;br/&gt; 1.4 Processors Read and Interpret Instructions Stored in Memory To run the executable object file, type ./hello to an applcation program known as a shell Hardware Organization of a System Buses: transfer words I/O Devices: system’s connection to the external world, connected to I/O bus by either a controller or an adapter Main Memory: temporary storage device that holds both a program and the data it manipulates while the processor is executing the program Processor: the engine that inter-prets (or executes ) instructions stored in main memory Running hello Reading command from keyboard Load file from disk to memory Write output string from memory to display 1.5 Caches Matter The machine instruction in the hello program are originally stored on disk When loaded, opied to main memory When processor runs, copied into the processor ( register + caches ) deal with processor-memory gap &lt;br/&gt; 1.6 Storage Devices Form a Hierarchy storage at one level serves as a cache for the next lower level 1.7 The Operating System Manages the Hardware OS a layer of software interposed between the application program and the hardware two primitive purpose: (1)protect hardware (2)provide applications with simple and uniform mechanisms for manipulating hardwares achieve both goals via abstractions Process The operating system's abstraction for a running program context switching Thread VIrtual Memory Each process has the same uniform view of memory ( virtual address space ) Files A sequence of bytes, nothing more and nothing less Every I/O devices is modeled as a file &lt;br/&gt; 1.8 Systems Communicate with Other Systems Using Networks 1.9 Important Themes Amdahl's law: observation of the effectiveness Concurrency and Parallelism concurrency a system with multiple, simultaneous activities parallelism use of concurrency to make a system run faster Parrallelism can be exploited at multiple( three ) levels of abstraction in a computer system. ( hight --&gt; low ) Thread-level Concurrency: multicore &amp; hyperthreading Instruction-level Parallelism: pipeling Single-Instruction, Multiple-Data ( SIMD ) Parallelism The Importance of Abstractions in Computer Systems Processor side instruction set architecture provides an abstraction of actual processor Operating System side Files as an abstraction of I/O Virtual Memory as an abstraction of Program Memory Processes as an abstraction of a Running Program Virtual Machine as an abstraction of the Entire Computer]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch9] Virtual Memory]]></title>
    <url>%2F2017%2F12%2F29%2FCSAPP%2Fch-9%2F</url>
    <content type="text"><![CDATA[CS:APP ch-9 Virtual Memory Virtual Memory modern systems provide an abstraction of main memory three important capabilities It uses main memory efficiently by treating it as a cache for an address space stored on disk, keeping only the active areas in main memory, and transferring data back and forth between disk and memory as needed. It simplifies memory management by providing each process with a uniform address space. It protects the address space of each process from corruption by other processes. &lt;!-- more --&gt; &lt;br/&gt; 9.1 Physical and Virtual Addressing Main memory organized as an array of M contiguous byte-size cells, each byte has a unique physical address. Physical Addressing: use physical address ( PA ) to access memory Virtual Addressing: use virtual address ( VA ) and translate to physical address Addressing Translation: converting a virtual address to a physical one 9.2 Address Spaces Virtual Address Space CPU generates virtual addresses from an address space of $ N = 2^n $ addresses called the virtual address space {0 , 1, 2 ,...,N − 1} Physical Address Space A system also has a physical address space that corresponds to the M bytes of physical memory in the system {0 , 1, 2 ,...,M − 1} &lt;br/&gt; 9.3 VM as a Tool for Caching A virtual memory is organized as an array of N contiguous byte-sized cells stored on disk. Each byte has a unique virtual address that serves as an index into the array. VM systems partition the virtual memory into fixed-size blocks call Virtual Pages ( VP ) as transfer units between disk and main memory. Three sets of VP: Unallocated, Cached, Uncached DRAM cache organization large virtual pages ( cache block )( 4 KB to 2 MB ) fully associative sophisticated replacement algorithms use write-back Page Tables A data structure stored in physical memory that maps virtual pages to physical pages. Page Hit Check valid bit, uses the physical memory address in the PTE (which points to the start of the cached page in PP 1) to construct the physical address of the word. Page Faults The page fault exception invokes a page fault exception handler in the kernel selects a victim page, in this case VP 4 stored in PP 3. modifies the page table entry for VP 4 to reflect the fact that VP 4 is no longer cached in main memory. copies VP 3 from disk to PP 3 in memory, updates PTE 3, and then returns. restarts the faulting instruction and page hit The strategy of waiting until the last moment to swap in a page, when a miss occurs, is known as demand paging 9.4 VM as a Tool for Memory Management Operating systems provide a separate page table, and thus a separate virtual address space, for each process. Linking: Allow each process to use the same basic format for its memory image ( code segment always start at 0x400000 ) Loading: (1) Allocate Virtual Pages (2) Mark as invalid ( not cached ) (3) Point table entries to the location of in object file (4) Data are paged in th first time referenced Sharing: Multiple processes share a single copy of some code Memory Allocation ​ ​ 9.5 VM as a Tool for Memory Protection three permission bits to each PTE SUP must run in kernel mode READ / WRITE &lt;br/&gt; 9.6 Address Translation a mapping between an N-element virtual address space and M-element physical address space Steps that the CPU hardware performs when there is a page hit / fault Speeding up Address Translation with a TLB a small cache of PTEs in the MMU called a translation look aside buffer (TLB) Multi-Level Page Tables つ....つづく！]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch8] Exceptional Control Flow]]></title>
    <url>%2F2017%2F12%2F28%2FCSAPP%2Fch8%2F</url>
    <content type="text"><![CDATA[CS:APP ch-8 Exceptional Control Flow control flow sequence of control transfers ( from the address of instruction to the next ) Exceptional Control Flow ( ECF ) Abrupt changes in control flow Occurs all levels Hardware: Transfers to exceptional handlers Operating System: Context switch Application: Signal Individual Program: Nonlocal jump &lt;!-- more --&gt; &lt;br/&gt; 8.1 Exceptions DEF Abrupt change in control flow in response to some change in the processor's state event change in state Exception Handling Hardware processor detect an event determine corresponding exception number k trigger exception by calling the handler through entry k of the exception table Software ( handler ) do some processing return control to the interrupted control flow Difference with procedure call return either current or next instruction push additional processor state to restart ( on kernel stack ) handlers run in kernel mode Classes of Exceptions Interrupt: I/O devices Trap: System call ( fork, read, execve, exit ) Fault: error conditions that might be able to correct or just abort ( page fault, segmentation fault, divide zero ) Abort: unrecoverable ( machine check ) 8.2 Processes DEF: An instance of program in execution program run in context of process Logical Control Flow provide an illusion that program has exclusive use of the processor The sequence of PC values is know as logical control flow Each process executes a portion of its flow and then is preempted (temporarily suspended) while other processes take their turns. Concurrent Flow A logic flow whose execution overlaps in time with another flow Concurrency The general phenomenon of multiple flows executing concurrently Parallel Flow subset of concurrent flow that run on different cores &lt;br/&gt; Private Address Space provide an illusion that program has exclusive use of the system address's space User and kernel mode: mode bit &lt;br/&gt; context the state that kernel needs to restart the preempted process ( general-purpose registers, floating point registers, program counter, user's stack, status register, kernel's stack, page table, process table, file table ) scheduling the kernel decide to preempt the current process and restart a previously preempted process Context Switch Saves context of the current process Restores the saved context of previous process Passes control to newly restored process &lt;br/&gt; Example when execute read system call ( disk ) Kernel switch process until disk sends an interrupt signal 8.4 Process Control Obtaining Process ID 1 pid_t getpid ( void ); &lt;br/&gt; Create and Terminating process Three states Running Stopped Terminated 1 void exit ( int status ); 1 2 //return 0 to child, PID of chile to parent pid_t fork ( void ); Call Once Return Twice Concurrent execution Duplicate but separate address space Shared files Reaping Child Process The process kept around in a terminated state until it is reaped by its parent 父进程进行回收的函数，也是一个等待子进程执行结束的函数就是waitpid。这在 APUE（advanced programming in unix enviroment）中很早就提过这个函数 1 2 // return PID of terminated child and reap it pid_t waitpid(pid_t pid, int *statud, int options) &lt;br/&gt; Putting Process to Sleep 1 2 unsigned int sleep(unsigned int secs); int pause(void); // 一直休眠，直到收到一个信号 Loading and Running Programs 1 int execve(const char *filename, const char *argv[], const char *envp[]); &lt;br/&gt; 8.5 Signals The transfer of a signal to a destination process occurs in two distinct steps Sending a signal The kernel has detected a system event such as a divide-by-zero error or the termination of a child process. A process has invoked the kill function to explicitly request the kernel to send a signal to the destination process. A process can send a signal to itself. Receiving a signal The process can either ignore the signal, terminate, or catch the signal by executing a user-level function called a signal handler. Pending signal A signal that has been sent but not yet received At any point in time, there can be at most one pending signal of a particular type. Received at most once: The kernel sets bit k in pending whenever a signal of type k is delivered and clears bit k in pending whenever a signal of type k is received. &lt;br/&gt; 8.6 Nonlocal Jumps]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch6] The Memory Hierarchy]]></title>
    <url>%2F2017%2F12%2F27%2FCSAPP%2Fch-6%2F</url>
    <content type="text"><![CDATA[CS:APP ch-6 The Memory Hierarchy 6.1 Storage Technologies SRAM cache, transistor, stable, fast, expensive &lt;br/&gt; DRAM main memory, capacitor, sensitive( periodically refresh every bit ) d * w cells --&gt; d supercells == r rows * c columns ( two dimension array ) data pins ( w bits ) &amp; addr pins ( row addr i, column addr j ) Memory modules: Each supercell stores 11 byte. Each address represent by 8 supercells Enhanced DRAM &lt;!-- more --&gt; &lt;br/&gt; Nonvolatile Memory ( ROM ) retain information even when they're powered off Flash memory ( erasable programmable ROM ) firmware: programs stored in ROM &lt;br/&gt; Accessing Main Memory I/O bridge: translate signals &lt;br/&gt; Disk platter --&gt; surface --&gt; track --&gt; sectors Logical Disk Blocks: hide complexity from OS disk controller: maintains mapping between logical block number and physical disk sectors I/O devices: USB, graphics card, host bus adapter I/O bus: connect disks to CPU and main memory, independent under CPU Accessing Disk Memory mapped I/O: a block of addresses in the address space is reserved for communicating with I/O devices. Each of these address is known as an I/O port. Each device associated with one or more ports. CPU read disk initiate a read indicate logical block number that should be read indicate main memory address to store disk sector do other work Disk controller receives the read command translates logical block number to sector address read content transfer content directly to main memory ( DMA ) send a interrupt signal to CPU CPU returns control to the point where interrupted Solid State Disk A page can be written only after the entire block it belongs to has erased A block wears out after roughly 100, 000 repeated writes Advantage: semiconductor, faster, use less power, more rugged Disadvantage: potential to wear out, expensive &lt;br/&gt; 6.2 Locality Temporal Locality: reference again multiple times Spatial Locality: reference nearby locations stride-k reference pattern: visiting every kth element of a contiguous vector k increase, spatial decrease &lt;br/&gt; 6.3 The Memory Hierarchy Caching in memory hierarchy Cache hits Cache misses replacing / evicting the block, victim block, replacement policy Kind of Cache misses cold miss: cache is empty conflict miss: map to the same block capacity miss: size of working set exceed Cache management partition storage into blocks, transfer blocks between levels, deal with hits and misses compiler -- register file hardware logic -- L1, L2, L3 caches OS + Address Translation hardware -- main memory AFS client process -- disk &lt;br/&gt; 6.4 Cache Memories Main memory: $ M = 2^m $ cache: $ C = S * E * B $ $ S = 2^s $ cache sets, $ E $ = cache lines, $ B = 2^b $ = bytes per block Address: set index = $ log_2S $ -- Sets tag = $ m - ( s + b ) $ -- Lines block offset = $ log_2B $ -- blocks Process that cache determine hit or miss Set Selection Line matching: search each line, find tag bits in cache match tag bits in address Word Extraction: block offset provide first byte of desired word Line Replacement: random / least frequently used / least recently used &lt;br/&gt; Direct-Mapped Caches one line per set ( E = 1 ) Set Associative Caches a E-way set $ 1 &lt; E &lt; C/B $ Fully Associative Caches S = 1 $ E = C/B $ &lt;br/&gt; Issues with writes write-through ( immediately write to next lower level when hit ) no-write-allocate ( write directly to next lower level when miss ) write-back ( defers the update until it is evicted when hit ) write-allocate ( load the block into cache and update it when miss ) &lt;br/&gt; Anatomy of real cache hierarchy Instruction cache ( i-cache ) Data cache ( d-cache ) both ( unified cache ) 6.5 Writing Cache-Friendly Code 6.6 Putting It Together: The Impact of Caches on Program Performance smaller size -- better temporal locality smaller stride -- better spatial locality The memory mountain]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CS:APP-ch4] Processor Architecture]]></title>
    <url>%2F2017%2F12%2F25%2FCSAPP%2Fch4%2F</url>
    <content type="text"><![CDATA[CS:APP-ch4 Processor Architecture ISA --&gt; SEQ --&gt; PIPE 4.1 The Y86-64 Instruction Set Architecture (ISA) Programmer-Visible State Program registers Condition codes Program Counter(PC) Memory Status code &lt;!-- more --&gt; Y86-64 Instructions type: code function register specifier bytes: rA rB additional 8-byte constant word: immediate data, displacement, destination Aside RISC &amp; CISC Y86-64 include both CISC: condition codes, variable length instructions, stack to store return address RISC: use load/store architecture and regular instruction encoding, pass argument through registers 4.2 Logical Design and the Hardware Control Language HCL 4.3 Sequential Y86-64 Implementations Organizing processing into stages Fetch: icode, ifun, valC = 8-byte constant, valP = next PC Decode: register valA = rA valB = rB Execute: valE = valA OP valB, CC Memory: valM = read/write from memory Write back: two result to register file PC Update: PC = valP SEQ Hardware Structure SEQ Timing SEQ: combinational logic two memory devices clocked register ( PC, CC reg ) random access memory ( register file, instruction memory, data memory ) Combinational logic does not require sequencing or control Instruction memory read only therefore also not required Required explicit control over sequencing Program Counter: loaded with new instruction address every clock cycle Condition Code register: loaded when integer operation register file: two ports, allow two program registers be updated on every cycle data memory: written only when rmmovq, pushq, call is executed PRINCIPLE never need to read back the state updated by an instruction in order to complete this instruction states are loaded during the start of next cycle 4.4 General Principle of Pipelining Throughput: number of instructions served per unit time Latency: Total time required to perform a single instruction from beginning to end Limitations Nonuniform partitioning ( 每个part的执行时间不同造成delay ) Diminishing Returns of Deep Pipelining (分太多了) Feedback (下一条指令要等上一条执行完) 4.5 Pipelined Y86-64 Implementations SEQ+ PC update stage comes at the beginning PIPE- insert registers Rearranging and Relabeling Signals Next PC Prediction Pipeline Hazard stalling forwarding load/use data hazard control hazard exception]]></content>
      <categories>
        <category>CSAPP</category>
      </categories>
      <tags>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[CLRS-ch34] NP-Completeness]]></title>
    <url>%2F2017%2F12%2F24%2FCLRS%2F34-np%2F</url>
    <content type="text"><![CDATA[CLRS-ch34 NP-Completeness Intro class P solvable in polynomial time class NP problems are verifiable in polynomial time class NPC it is in NP and is as hard as any problem in NP decision problems reductions the first NP-complete problem &lt;!-- more --&gt; 34.1 Polynomial time Abstract Problems a binary relation on a set I of instances and a set S of solutions Encodings In order to solve abstract problems, represent problem instances in a way program understands a mapping e from S to the set of binary strings Abstract Pro--&gt; Encode --&gt; Concrete Pro --&gt;Algorithm Solve polynomial related Polynomial time solvable exist an algorithm to solve a concrete problem in $O(n^k)$ extend the definition from concrete to abstract by using encoding as bridge A formal language framework alphabet $\Sigma $ a finite set of symbols { 0,1 } language L over Σ is any set of strings made up of symbols from Σ { 10, 11, 101 ... } $ \Sigma ^* $ the language of all strings over Σ view instances for any decision problems Q as a language L over Σ = { 0 ,1 } Accepted The language L accepted by an algorithm A $ L = \{ x \in \{0,1\}^* : A(x) = 1 \} $ Reject : A(x) = 0 may runs forever if doesn't accept Decided The language L decided by an algorithm A if every binary string in L is accepted by A (output 1) and every binary string not in L is rejected ( output 0) $ x \in L $ --&gt; A(x) = 1, $ x\notin L $ --&gt; A(x) = 0 Complexity class P $ P = \{ L \subseteq \{ 0,1 \}^{*} $ : exists an algorithm A that decides L in polynomial time $ \}$ 34.2 Polynomial-time verification algorithms verify membership from a certificate in languages Verified The language L verified by an algorithm A $ L = \{ x \in \{ 0, 1 \}^* : $ there exist $ y \in \{ 0,1 \}^* $ such that A( x,y ) = 1 $ \} $ Complexity class NP $ NP = \{ L \subseteq \{ 0, 1 \}^* : L = \{ x \in \{ 0,1\}^* $ : there exists a certificate y with |y| = $O(|x|^c) $ such that A(x,y) = 1 $ \} \} $ $P \subseteq NP $ Complexity class co-NP set of languages L such that $ \bar L \in NP $ Four possibilities for relationship among complexity class 34.3 NP-completeness and reducibility Reducibility if Q -- &gt; reduce --&gt; Q' , Q is &quot;no harder to solve&quot; than Q' $ Q \le_p Q' $ ( no more than a polynomial factor harder ) NP-completeness $ L \in NP $ $ L' \le_p L $ for every $ L' \in NP $. (hardest) Theorem 34.4 If any NP-complete problem is polynomial time solvable, then P = NP If any problem in NP is not polynomial-time solvable, then all NP-complete problems are not polynomial-time solvable. Circuit satisfiability Given a boolean combinational circuit composed of AND, OR, and NOT gates, is it satisfiable? 34.5 belongs to NP 34.6 NP-hard basic idea : represent the computation of A(algorithm verify L) as a sequence of configurations. Each configuration is mapped to the next configuration by a boolean combinational circuit M. The output is a distinguished bit in the working storage. The reduction algorithm F constructs a single combinational circuit that computes all configurations produced by a given initial configuration. 34.5 NP-complete problems]]></content>
      <categories>
        <category>CLRS</category>
      </categories>
      <tags>
        <tag>CLRS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题十二 基础DP]]></title>
    <url>%2F2017%2F12%2F20%2FACM%2Fkuangbin-12%2F</url>
    <content type="text"><![CDATA[kuangbin 专题十二 基础DP LIS dp[ i ]代表前i个数的最长子序列大小 dp[ i ] =max { dp[ j ] + 1 } ( val[ j ] &lt; val[ i ] ) $ O ( n^2 ) $ &lt;!-- more --&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int dp[ N ], val[ N ]; int LIS ( int n ) &#123; int mx = 0; for ( int i = 0; i &lt; n; ++i ) &#123; dp[ i ] = 1; for ( int j = 0; j &lt; i; ++j ) &#123; if ( val[ j ] &lt; val[ i ] &amp;&amp; dp[ i ] &lt; dp[ j ] + 1 ) dp[ i ] = dp[ j ] + 1; &#125; if ( mx &lt; dp[ i ] ) mx = dp[ i ]; &#125; return mx; &#125; $ O ( nlgn ) $ http://www.geeksforgeeks.org/longest-monotonically-increasing-subsequence-size-n-log-n/ http://blog.csdn.net/shuangde800/article/details/7474903 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 int val[ N ];//输入的值 int len;//ed数组的长度（LIS长度） int ed[ N ];//每个长度的序列的末尾 //下界，返回 &gt;= 所查找对象的第一个位置 int binary_search ( int i ) &#123; int left, right, mid; left = 0, right = len; while ( left &lt; right ) &#123; mid = left + ( right - left ) / 2; if ( ed[ mid ] &gt;= val[ i ] ) right = mid; else left = mid + 1; &#125; return left; &#125; void LIS ( int n ) &#123; ed[ 1 ] = val[ 1 ]; len = 1; for ( int i = 2; i &lt;= n; ++i ) &#123; //更新最长的末尾 if ( val[ i ] &gt; ed[ len ] ) ed[ ++len ] = val[ i ]; //产生了新的序列，改变旧的长度的末尾 else &#123; // 如果用STL： pos=lower_bound(ed,ed+len,val[i])-ed; int pos = binary_search ( i ); ed[ pos ] = val[ i ]; &#125; printf ( "%d\n", len ); &#125; &#125; LCS dp[ i ][ j ] 代表两个字符串前i / j 个下的最长大小 dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1 ( val[ i ] == val[ j ] ) dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ) ( != ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //CLRS 15.4 int len1, len2; char s1[ N ], s2[ N ]; int dp[ N ][ N ]; inline int max ( int a, int b ) &#123; return a &gt; b ? a : b; &#125; void LCS () &#123; for ( int i = 1; i &lt;= len1; i++ ) &#123; for ( int j = 1; j &lt;= len2; j++ ) &#123; if ( s1[ i ] == s2[ j ] ) dp[ i ][ j ] = dp[ i - 1 ][ j - 1 ] + 1; else dp[ i ][ j ] = max ( dp[ i - 1 ][ j ], dp[ i ][ j - 1 ] ); &#125; &#125; &#125; void Print ( int i, int j ) &#123; //当最长的子序列搜索完，但其中一串仍有剩余时，输出 if ( i == 0 || j == 0 ) &#123; return; &#125; //找到公共字符 if ( s1[ i ] == s2[ j ] ) &#123; Print ( i - 1, j - 1 ); printf ( "%c", s1[ i ] ); &#125; else if ( dp[ i - 1 ][ j ] &gt; dp[ i ][ j - 1 ] ) &#123; Print ( i - 1, j ); &#125; else &#123; Print ( i, j - 1 ); &#125; &#125; 完全背包 dp[ i ]代表背包重i的时候最大的价值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int v[ N ], w[ N ]; // value，weight int dp[ N ]; //n个物品，最多装wei的东西 int knapsack ( int n, int wei ) &#123; memset ( dp, INF, sizeof ( dp ) ); dp[ 0 ] = 0; for ( int i = 1; i &lt;= wei; ++i ) &#123; for ( int j = 0; j &lt; n; ++j ) &#123; if ( i &gt;= w[ j ] ) dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); &#125; &#125; return dp[ wei ]; &#125; 题目列表 A - Max Sum Plus Plus HDU - 1024 n个数，要求分成m组，使m组的和加起来得到最大值。 dp[i][j]表示前j个数分成i组的最大值。 dp[i][j]=max(dp[i][j-1]+a[j],max(dp[i-1][k])+a[j]) B - Ignatius and the Princess IV HDU - 1029 给n(奇数)个数，定义特殊的数为在序列中出现次数不少于(n+1)/2次的数，找出这个特殊的数 一边输入一边记录个数就好了 C - Monkey and Banana HDU - 1069 给定箱子种类数量n，及对应长宽高，每个箱子数量无限，求其能叠起来的最大高度是多少(上面箱子的长宽严格小于下面箱子) 按照长排序，在求宽关于高的LIS D - Doing Homework HDU - 1074 有n门功课需要完成，每一门功课都有时间期限以及你完成所需要的时间，如果完成的时间超出时间期限多少单位，就会被减多少学分，问以怎样的功课完成顺序，会使减掉的学分最少，有多个解时，输出功课名排列最小的一个。 15个作业状态压缩来做 枚举每一个状态，枚举每个状态新增的那个节点，再计算最小并记录前一个状态 E - Super Jumping! Jumping! Jumping! HDU - 1087 从起点到达终点，只能前行不能后退，且下一步必须比前面的点的值大，求所有走的点的值总和最大是多少。 dp[i] = max(dp[k] + a[j]); 1&lt;=k&lt;=i-1; 最大递增子串和。 F - Piggy-Bank HDU - 1114 给出存钱罐本身的重量和装钱后的重量，以及存钱罐中钱的面值和重量，求存钱罐装满时，钱的总和最小是多少 完全背包解题，每种钱币都可以装无限个，注意初始化的值 dp[ i ] = min ( dp[ i ], dp[ i - w[ j ] ] + v[ j ] ); G - 免费馅饼 HDU - 1176 0—10的点，不同时间在每个点上掉下来物品，只能到达左右两边距离为1和本身所在的位置，求最大物品数 dp[x][t] = max ( dp[x-1][t-1],dp[x][t-1],dp[x+1][t-1]) + v[x][t] H - Tickets HDU - 1260 单张或两张一起买，给出一个一个买票和两个两个买票的时间，求最少 dp[ i ] = min ( dp[ i - 1 ] + s[ i ], dp[ i - 2 ] + d[ i - 1 ] ); I - 最少拦截系统 HDU - 1257 求有多少个递减序列 反过来，转换成求整个系列有多少个LIS，则是所求的组数 J - FatMouse's Speed HDU - 1160 给n个老鼠的体重和速度，求找出一个最长的序列，此序列体重递增速度递减 按体重递增排序，再求最长递增(此递增表示体重递增速度递减)子序列。 dp[i] = max(dp[j]+1) 0&lt;=j&lt;=i-1 K - Jury Compromise POJ - 1015 必须满足辩方总分D和控方总分P的差的绝对值|D-P|最小。如果有多种选择方案的 |D-P| 值相同，那么选辩控双方总分之和D+P最大的方案即可。 dp(j, k)表示，取j 个候选人，使其辩控差为k 的所有方案中，辩控和最大的那个方案的辩控和。 综上：dp[j][k]=dp[j-1][k-V[i]]+S[i] 正向计算，输出的时候就用正向的输出了,不过每次都要查找下一个位置是否在之前用过了 L - Common Subsequence POJ - 1458 LCS LCS模板 M - Help Jimmy POJ - 1661 老鼠在时刻0从高于所有平台的某处开始下落.当Jimmy落到某个平台上时，游戏者选择让它向左还是向右跑.当Jimmy跑到平台的边缘时，开始继续下落。Jimmy每次下落的高度不能超过MAX dp[i][0] = min(dp[k][0]+l[i]-l[k], dp[k][1]+r[i]-l[k]) + h[i]-h[k]; (左左和左右取最小) dp[i][1] = min(dp[k][0]+r[i]-l[k], dp[k][1]+r[i]-r[k]) + h[i]-h[k];(右左和右右取最小) N - Longest Ordered Subsequence POJ - 2533 LIS LIS模板 O - Treats for the Cows POJ - 3186 n个数在一个双端队列中，每次从队首或队尾出。出的第n个数乘以n，最后加起来，求最大和。 dp[i][j] 代表从i取到j的最大总数 dp[i][j] = max(dp[i+1][j]+a[i] * (n+i-j) , dp[i][j-1]+a[j] * (n+i-j)); P - FatMouse and Cheese HDU - 1078 给定一幅图，每个点有一定权值，现在有一只老鼠在起始点（0,0），他能水平或者垂直移动1~k格之后，停在某点并获得权值，而且每次移动后所在的点，都要比刚离开的那个点的权值更大，求最多能获得多少权值。 DP / Memoized dp[ x ][ y ] = dp[ xx ][ yy ] + val[ x ][ y ] Q - Phalanx HDU - 2859 给了一个字符串矩阵，求以次对角线方向对称的最大对称矩阵。 每次只需求最外面一层对称个数sum，再和右上角对称矩阵大小加一取最小就行，就求出当前小矩阵的最大对称矩阵。最后取个所有对称矩阵大小的最大值就行。 dp[i][j] = min(sum,dp[i-1][j+1]+1); R - Milking Time POJ - 3616 奶牛Bessie在0~N时间段产奶。农夫约翰有M个时间段可以挤奶，时间段f,t内Bessie能挤到的牛奶量e。奶牛产奶后需要休息R小时才能继续下一次产奶，求Bessie最大的挤奶量。 dp[ i ] = max ( dp[ j ] + node[ i ].val, dp[ i ] ) ( node[ j ].ed &lt;= node[ i ].st ) S - Making the Grade POJ - 3666 农夫约翰想修一条尽量平缓的路，路的每一段海拔是A_i，修理后是B_i，花费|A_i – B_i|，求最小花费。 dp[i][j] = min(dp[i – 1][k]) + |A[i] – B[j]| 离散化]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[GSLA ch-6] Eigenvalues and Eigenvectors]]></title>
    <url>%2F2017%2F12%2F14%2FGSLA%2FEigenvalues%20and%20Eigenvectors%2F</url>
    <content type="text"><![CDATA[Eigenvalues and Eigenvectors 6.1 Introduction certain vectors x are in the same direction as Ax basic equation: ​ $ Ax = \lambda x $ how to compute: let $ det(A-\lambda x)= 0 $ ,find the roots == find eigenvalues, find eigenvectors in Null space $ A^nx = \lambda^n x $ span eigenspace Projection: $ \lambda = 1 $ or 0 Reflection: $ \lambda = 1 $ or -1 Rotation: complex eigenvalues only &lt;!-- more --&gt; &lt;br&gt; product of eigenvalues == determinant == product of pivots sum of eigenvalues == sum of diagonal entries ( not pivots ) == trace &lt;br&gt; 6.2 Diagonalizing eigenvectors in columns of S, eigenvalues in diagonal of Λ $ Λ = S^{-1}AS $ $ A = SΛS^{-1} $ &lt;br&gt; Independent x from different λ: $ c_1 λ_1 x_1 + c_2 λ_2 x_2 = 0 $ $ c_1 λ_2 x_2 + c_2 λ_2 x_2 = 0 $ subtract $ (λ_1 - λ_2)c_1x_1 = 0 $ &lt;br&gt; diagonalizability: enough eigenvectors (maybe same λ) so that S is invertible $u_k = A^k u_0 = S \Lambda^k S^{-1}u_0$ $ u_0 = c_1x_1 + c_2x_2 + .. +c_nx_n $ eigenvector basis multiply $\lambda_i ^ k$ add up A,B share same eigenvector matrix S if and only if AB = BA &lt;br&gt; ?Heisenberg uncertainty principle position matrix P, momentum matrix Q, $ QP-PQ = I $ knew P still could not know Q $ |Px||Qx| \ge \frac{1}{2}|x|^2 $ &lt;br&gt; 6.3 Applications to Differential Equations &lt;br&gt; 6.4 Symmetric Matrices real eigenvalues orthonormal eigenvectors Spectral Theorem (principle of axis theorem) $ A = Q\Lambda Q^T $ &lt;br&gt; Normal Matrices $ \bar A^TA = A \bar A^T $ symmetric, skewed-symmetric, orthogonal A has n orthonormal vectors ($ A = Q\Lambda \bar Q ^T $) if and only if A is normal Real Eigenvalues proof: $ Ax = \lambda x $ $ \bar x ^TA = \bar x ^T \bar\lambda $ ( $ A = A^T $ )( conjugate and transpose ) $ \bar x^T A x = \bar x^T \lambda x $ $ \bar x ^ T A x = \bar x ^ T \bar\lambda x $ left side the same therefor $ \lambda == \bar \lambda $ &lt;br&gt; Orthonormal proof: no eigenvalues repeated Allow repeated eigenvalues ( ? Schur's Theorem ) &lt;br&gt; sum of rank one projection matrices $ A = \lambda_1x_1x_1^T + \lambda_2x_2x_2^T+... $ $ = \lambda_1P_1 +\lambda_2P2+.... $ &lt;br&gt; number of positive pivots == number of positive eigenvalues &lt;br&gt; $ A = LDL^T $ &lt;br&gt; 6.5 Positive Definite Matrices All λ &gt; 0 &lt;br&gt; quick way to test All pivots positive n eigenvalues positive $ x^TAx $ is positive except x = 0 $ A == R^TR $ (symmetric) and R has independent columns ($ x^T R^TRX &gt;= 0 $) n upper left determinants R can be chosen: rectangular / $ (L\sqrt D)^T $ / $ Q \sqrt\Lambda Q^T $ $x^TAx $ (2*2) = $ ax^2 + 2bxy + cy^2 &gt; 0 $ ( ellipse $ z = x^2/a^2 + y^2/b^2 $ ) &lt;br&gt; Application tilted ellipse $ x^TAx $ lined - up ellipse $ X^T\Lambda X = 1 $ rotation matrix Q axes: eigenvectors half-length: $ 1/\sqrt\lambda $ 6.6 Similar Matrices DEF: A similar to B (A family) $ B = M^{-1}AM $ &lt;br/&gt; Property: A and B have same eigenvalues x a eigenvector of A and $ M^{-1}x $ eigenvector of B &lt;br/&gt; Jordan Form triple eigenvalues while one eigenvector J with λ in the diagonal and 1 above similar to every matrices with repeated eigenvalues λ and one eigenvector λ repeated only once than J == Λ Jordan Block make A as simple as possible while preserving essential properties 6.7 Singular Value Decomposition SUMMARY $ A = U \Sigma V^T $ $ \Sigma^2 = \Lambda $ (of $ A^TA $ and $ AA^T $ ) $ U = Q $ (of $ AA^T $ in $ R^m $) $ V = Q $ (of $ A^TA $ in $ R^n $) &lt;br/&gt; orthonormal basis of row space {$ v_1, v_2, ... v_r $} orthonormal basis of null space{ $ v_{r+1}, v_{r+2},...v_n $} orthonormal basis of column space{ $ u_1, u_2,...u_r $} orthonormal basis of left null space { $ u_{r+1}, u_{r+2}, ... u_m $} &lt;br/&gt; Rotation -- Stretch -- Rotation $ Av_1 = \sigma_1u_1 $ ... so $ AV = U\Sigma $ (m * n) * (n * n) = (m * m)* (m * n) $ V $ and $ U $ are orthogonal matrices $ \Sigma $ = ( old r*r $ \Sigma $ ) + (m-r zero rows) + ( n-r zero columns ) therefore ... &lt;br/&gt; when A positive definite symmetric $ A = U \Sigma V^T = Q\Lambda Q^{T} $ 7.3 Diagonalization and the Pseudoinverse change bases $ \Lambda { w -- w } = S^{-1}{std -- w} A_{std} S_{ w -- std } $ $ \Sigma { v -- u } = U^{-1}{std -- u} A{std}V{v -- std} $ Polar Decomposition orthogonal and semidefinite rotation and stretching $ A = U\Sigma V^T = ( UV^T ) (V \Sigma V^T) = QH $ Pseudoinverse $ A^+ = V\Sigma^+ U^T $]]></content>
      <tags>
        <tag>Linear Algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kuangbin专题四 最短路]]></title>
    <url>%2F2017%2F12%2F10%2FACM%2Fkuangbin-4%2F</url>
    <content type="text"><![CDATA[kuangbin专题四 最短路 ACM图论存图方式 http://jzqt.github.io/2015/07/21/ACM%E5%9B%BE%E8%AE%BA%E4%B9%8B%E5%AD%98%E5%9B%BE%E6%96%B9%E5%BC%8F/ 单源最短路 Dijkstra ​ $ O(V^2 + E) $ &lt;!-- more --&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 int edg[ N ][ N ]; // weight of each edge int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited // CLRS 24.3 // s:start point, n: total number of nodes void dijkstra ( int s, int n ) &#123; // INITIALIZE-SINGLE-SOURCE memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = edg[ s ][ i ]; dis[ s ] = 0; // Extract Min // u for the node, mi for dis[ u ] for ( int i = 1; i &lt;= n - 1; ++i ) &#123; int u = -1, mi = INF; for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] &amp;&amp; dis[ j ] &lt; mi ) mi = dis[ u = j ]; vis[ u ] = true; // Relax edges adjacent to u for ( int j = 1; j &lt;= n; ++j ) if ( !vis[ j ] ) dis[ j ] = min ( dis[ j ], dis[ u ] + edg[ u ][ j ] ); &#125; &#125; Dijkstra + STL priority_queue ​ $ O((V+E)lgV) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 int head[ N ]; //链式前向星建图 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct Node &#123; int u, d; // id, dis bool operator&lt; ( const Node &amp;rhs ) const &#123; return d &gt; rhs.d; &#125; &#125;; struct Edge &#123; int u, v, w, nex; &#125; edg[ N ]; // CLRS 24.3 // s:start point void dijkstra ( int s ) &#123; priority_queue&lt;Node&gt; Q; dis[ s ] = 0; Q.push ( ( Node )&#123;s, dis[ s ]&#125; ); while ( !Q.empty () ) &#123; // u = Extract_Min( Q ) Node x = Q.top (); Q.pop (); int u = x.u; if ( vis[ u ] ) continue; vis[ u ] = true; // for each vertex v in G.adj[ u ] for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v; int w = edg[ i ].w; // relax if ( dis[ v ] &gt; dis[ u ] + w ) &#123; dis[ v ] = dis[ u ] + w; Q.push ( ( Node )&#123;v, dis[ v ]&#125; ); &#125; &#125; &#125; &#125; SPFA (Shortest Path Faster Algorithm) ​ $ O(kE) $ ( k &lt; 2 ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited int cnt[ N ]; //入列次数超过n，有负环 int head[ N ]; //链式前向星 struct edg &#123; int u, v, w, nex; &#125; edg[ N ]; bool SPFA ( int s, int n ) &#123; // INIT memset ( vis, false, sizeof ( vis ) ); vis[ s ] = true; for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = INF; dis[ s ] = 0; memset ( cnt, 0, sizeof ( cnt ) ); cnt[ s ] = 1; // BFS方式的spfa queue&lt;int&gt; Q; Q.push ( s ); while ( !Q.empty () ) &#123; int u = Q.front (); Q.pop (); vis[ u ] = false; // 出队要取消标记 for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v; // Relax所有出边 if ( dis[ v ] &gt; dis[ u ] + edg[ i ].w ) &#123; dis[ v ] = dis[ u ] + edg[ i ].w; //没入队的标记并入队 if ( !vis[ v ] ) &#123; vis[ v ] = true; Q.push ( v ); // 判断负环 if ( ++cnt[ v ] &gt; n ) return false; &#125; &#125; &#125; &#125; return true; &#125; 贴一个dfs的，自己没写过，感觉dijkstra+heap最好 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 int spfa_dfs ( int u ) &#123; vis[ u ] = true; for ( int i = head[ u ]; i != -1; i = edg[ i ].nex ) &#123; int v = edg[ i ].v, w = edg[ i ].w; if ( dis[ u ] + w &lt; dis[ v ] ) &#123; dis[ v ] = dis[ u ] + w; if ( !vis[ v ] ) &#123; if ( spfa_dfs ( v ) ) return 1; &#125; else return 1; &#125; &#125; vis[ u ] = false; return 0; &#125; Bellman-Ford ​ $ O(VE) $ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 int dis[ N ]; // distance of each node from source bool vis[ N ]; // SET S represent for nodes already visited struct edg &#123; int u, v, w; &#125; edg[ N ]; //CLRS 24.1 //对每个点，relax所有的边 bool Bellman_Ford ( int n, int e, int s, double num ) &#123; // Initialize Single Sorce( G,s ) for ( int i = 1; i &lt;= n; ++i ) dis[ i ] = 0; dis[ s ] = num; // for i = 1 to |G.V|-1 for ( int i = 0; i &lt; n - 1; ++i ) &#123; // for each edg ( u,v ) in G.E for ( int j = 0; j &lt; e; ++j ) &#123; int u = edg[ j ].u; int v = edg[ j ].v; // Relax if ( dis[ v ] &gt; dis[ u ] + edg[ j ].w ) &#123; dis[ v ] = dis[ u ] + edg[ u ].w; &#125; &#125; &#125; // 存在负环 for ( int i = 0; i &lt; e; ++i ) if ( dis[ edg[ i ].v ] &gt; ( dis[ edg[ i ].u ] - edg[ i ].w ) ) return false; return true; &#125; 适用场景 如果是稠密图，Dijkstra+heap比SPFA快。稀疏图则SPFA更快。再就是SPFA可以判断负环 对于极端的链状图，SPFA无疑是最合适的了。每个结只进队一次，标准的O(E)。 朴素的dijkstra对于这样的图就力不从心了：每次循环都过一遍结点，在松弛，然后发现每次O(V)的时间只松弛了一个点。 ​ 多源最短路 Floyd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // CLRS 25.2 All-Pairs-Shortest-Path void Floyd () &#123; // Dp // 表示所有点只经过集合&#123; 1..k &#125;内的点时的最短路径 // 只经过1的时候relax一次，在添加上经过2后的relax，最后一直到n for ( int k = 1; k &lt;= n; ++k ) &#123; //选取每一个起点 for ( int i = 1; i &lt;= n; ++i ) //选取每一个终点 for ( int j = 1; j &lt;= n; ++j ) // d(ij)^k = min &#123; d(ik)^(k-1) + d(kj)^(k-1) &#125; dis[ i ][ j ] = dis[ i ][ j ] || ( dis[ i ][ k ] &amp;&amp; dis[ k ][ j ] ); &#125; &#125; 题目列表 A - Til the Cows Come Home POJ-2387 一个农场有n (1000)个点，有t (2000)条道路连接, 从n到1最短 dijkstra 模板 B - Frogger POJ-2253 无向图一条1~2的路径使得该路径上的最大边权最小. (max Route weight is the minimum among all routes) dijkstra变形 double minimax = max ( mi, v[ j ][ u ] ); C - Heavy Transportation POJ - 1797 ​ 有n个城市，n个城市之间有m条公路或桥梁，每个公路或桥都有一个最大载重量，问从城市1到城市n所能运送到货物到最大重量是多少 ( min Route weight is the maximum among all routes ) dijkstra变形 int maxmini = min ( mi, v[ j ][ u ] ); D - Silver Cow Party POJ - 3268 n个农场，m条单向路，n个牛分别在n个农场，第x农场为终点，问每个牛从所在农场前往x农场的往返路程最小值是多少，求出n个牛中最短路上往返路程的最大的那个 从n个点到1再从1回到n个点，通过调转边的方向两次dijkstra E - Currency Exchange POJ - 1860 有n种货币，你含有num面额的其中一种货币。求有没有可能，在多次兑换后你手中的货币大于num。 求最大路径，反向用Bellman-Ford F - Wormholes POJ - 3259 农场之间有很多条路，以及单向的虫洞，每条路走完会花费一定的时间，而虫洞可以回到之前的时间，问农场主是否能回到自己出发时间前的出发点 SPFA判断负环 G - MPI Maelstrom POJ - 1502 从第一个点出发，求到其他点最短路的最大值 dijkstra 注意下三角矩阵邻接表的建图 H - Cow Contest POJ - 3660 n个牛进行比赛，现已知m个关系， 牛u可以胜过牛v。问最后可以确定排名位数的有几个牛. Floyd判断两两牛之间的关系。如果一个牛可以胜过a个牛，b个牛可以胜过它，那么如果a＋b＝n－1，他的排名就可以确定 I - Arbitrage POJ - 2240 给定多种货币之间的兑换关系，问是否可以套利 Bellman-Ford判断正环（要返回自己所以松弛n次) floyd判断回来后是否&gt;1 J - Invitation Cards POJ - 1511 求源点到各点的往返最短路之和 邻接表逆置（建了两个邻接表） 数据多需要优化SPFA/dijkstra+heap K - Candies POJ - 3159 给n个人分糖果，m组数据a，b，c；意思是a比b少的糖果个数绝对不超过c个，也就是d(b)-d(a) &lt; c，求1比n少的糖果数的最大值。 差分约束，和最短路的松弛一样 数据多，dijkstra+heap L - Subway POJ - 2502 小明步行的速度是10km/h，地铁速度是40km/h，给定家和学校的坐标，再给定多条地铁线路站点的坐标，问小明从家到学校所需的最短时间 dijkstra，建图连接所有的点并赋值时间 M - 昂贵的聘礼 POJ - 1062 每个人可能有直接购买或者交换物品换取折扣这两种方式交易（交换物品要从别人手里买） 等级差之间超过m的不能交易 求用最少的钱买到非洲大酋长的承诺 等级限制采用枚举的方法，分别从lv[ 1 ] - m ~~ lv[ 1 ] + m，每次枚举的区间长度为m,一共m次最短路搜索 N - Tram POJ - 1847 电动巴士在每个十字路口有一个默认方向，走向别的方向需要改动扳手。 dijkstra 每个边初始化为INF,要切换的路 = 1,不用切换 = 0 O - Extended Traffic LightOJ - 1074 给定每条街的拥挤度p(x)，街a到街b的时间就是(p(b)-p(a))**3，求第一个点到第k个点的最短路 SPFA判断负环 dfs记录负环里的点 P - The Shortest Path in Nya Graph HDU - 4725 共n个点，n层(每个点单独一层)，相邻的两层之间权值为w 还有m条额外的边，权值为v，求1到n的最短路 建图 ！！给每个点两个辅助点，一个做出度，一个做入度 Q - Marriage Match IV HDU - 3416 网络流，不会qwq R - 0 or 1 HDU - 4370 X12+X13+...X1n=1,X1n+X2n+...Xn-1n=1,其余行列和相同，求ΣCij*Xij 神一样的建图！！节点1的出度为1.节点n的入度为1.节点2-n-1的入度和出度相等. 问题就相当于求一条最短路，从节点1出发，到节点N. 同时节点1的一个最小环+节点n的一个最小环也是可行解，两者取最小 S - Layout POJ - 3169 两点间可能&lt;= x 或者&gt;=x，求1--&gt;n最大 差分约束，和最短路一样，SPFA判断负环则代表不存在可行解]]></content>
      <categories>
        <category>ACM</category>
      </categories>
      <tags>
        <tag>ACM</tag>
      </tags>
  </entry>
</search>
